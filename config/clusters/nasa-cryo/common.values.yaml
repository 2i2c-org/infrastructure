basehub:
  nfs:
    enabled: true
    volumeReporter:
      enabled: false
    pv:
      # pv.serverIP is set in each individual staging and prod config file
      enabled: true
      # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
      mountOptions:
      - rsize=1048576
      - wsize=1048576
      - timeo=600
      - soft   # We pick soft over hard, so NFS lockups don't lead to hung processes
      - retrans=2
      - noresvport
      baseShareName: /
  dask-gateway:
    enabled: true
  jupyterhub:
    custom:
      daskhubSetup:
        enabled: true
      2i2c:
        add_staff_user_ids_to_admin_users: true
        add_staff_user_ids_of_type: github
      jupyterhubConfigurator:
        enabled: false
      homepage:
        templateVars:
          org:
            name: Cryo in the Cloud
            logo_url: https://raw.githubusercontent.com/CryoInTheCloud/CryoCloudWebsite/main/cryocloud.png
            url: https://github.com/CryoInTheCloud
          designed_by:
            name: 2i2c
            url: https://2i2c.org
          operated_by:
            name: 2i2c
            url: https://2i2c.org
          # Ideally, this community would like to list more than one funder
          # Issue tracking implementation of this feature:
          # https://github.com/2i2c-org/default-hub-homepage/issues/16
          funded_by:
            name: NASA ICESat-2 Science Team
            url: https://icesat-2.gsfc.nasa.gov/science_definition_team
    hub:
      allowNamedServers: true
      config:
        JupyterHub:
          authenticator_class: github
        GitHubOAuthenticator:
          # We are restricting profiles based on GitHub Team membership and
          # so need to populate the teams in the auth state
          allowed_organizations:
          - CryoInTheCloud:cryoclouduser
          - CryoInTheCloud:cryocloudadvanced
          - CryoInTheCloud:csdms-2025-workshop
        Authenticator:
          # We are restricting profiles based on GitHub Team membership and
          # so need to persist auth state
          admin_users:
          - tsnow03
          - JessicaS11
          - jdmillstein
          - dfelikson
          - fperez
          - scottyhq
          - jomey
          - rwegener2
          - itcarroll
      loadRoles:
        user:
          scopes:
          - self
          - shares!user
    singleuser:
      cloudMetadata:
        blockWithIptables: false
      defaultUrl: /lab
      storage:
        extraVolumes:
        - name: dev-shm
          emptyDir:
            medium: Memory
        extraVolumeMounts:
          # A shared folder readable & writeable by everyone
          # https://github.com/CryoInTheCloud/hub-image/issues/43#issuecomment-1466392517
        - name: home
          mountPath: /home/jovyan/shared-public
          subPath: _shared-public
          readOnly: false
        - name: home
          mountPath: /home/jovyan/shared
          subPath: _shared
          readOnly: true
        - name: dev-shm
          mountPath: /dev/shm
      initContainers:
      - name: volume-mount-ownership-fix
        image: busybox:1.36.1
        command:
        - sh
        - -c
        - id && chown 1000:1000 /home/jovyan /home/jovyan/shared /home/jovyan/shared-public && ls -lhd /home/jovyan
        securityContext:
          runAsUser: 0
        volumeMounts:
        - name: home
          mountPath: /home/jovyan
          subPath: '{escaped_username}'
            # Mounted without readonly attribute here,
            # so we can chown it appropriately
        - name: home
          mountPath: /home/jovyan/shared
          subPath: _shared
        - name: home
          mountPath: /home/jovyan/shared-public
          subPath: _shared-public
        - name: home
          mountPath: /home/rstudio
          subPath: '{escaped_username}'
        - name: home
          mountPath: /home/rstudio/shared
          subPath: _shared
          readOnly: true
      profileList:
      - display_name: CPU Only
        default: true
        profile_options:
          image:
            display_name: Environment
            unlisted_choice:
              enabled: true
              display_name: Custom image
              description: Specify your own docker image (must have python and jupyterhub installed in it)
              validation_regex: ^.+:.+$
              validation_message: Must be a publicly available docker image, of form <image-name>:<tag>
              kubespawner_override:
                image: '{value}'
            choices:
              01-python:
                display_name: CryoCloud Python Image
                description: Python image maintained by the CryoCloud team
                default: true
                kubespawner_override:
                  # Image repo: https://github.com/CryoInTheCloud/hub-image
                  image: quay.io/cryointhecloud/cryo-hub-image:d624b28e39c4
              02-r:
                display_name: CryoCloud R Image
                description: R environment with many geospatial libraries pre-installed
                kubespawner_override:
                  # Image repo: https://github.com/CryoInTheCloud/hub-Rstudio-image
                  image: quay.io/cryointhecloud/cryo-hub-r-image:c2ee1f933030
              03-matlab:
                display_name: Matlab
                description: Matlab Environment (bring your own license)
                kubespawner_override:
                  # From https://2i2c.freshdesk.com/a/tickets/1537
                  image: openscapes/matlab:latest
          resource_allocation:
            display_name: Resource Allocation
            choices:
              mem_2_gb:
                display_name: ~2 GB RAM, ~0.2 CPUs
                description: Up to ~4 CPUs when available
                allowed_groups: &regular_allowed_groups
                - CryoInTheCloud:CryoCloudAdvanced
                - CryoInTheCloud:CryoCloudUser
                - CryoInTheCloud:csdms-2025-workshop
                - 2i2c-org:hub-access-for-2i2c-staff
                kubespawner_override:
                  mem_guarantee: 1951419879
                  mem_limit: 1951419879
                  cpu_guarantee: 0.22815625
                  cpu_limit: 3.6505
                  node_selector:
                    node.kubernetes.io/instance-type: r5.xlarge
              mem_4_gb:
                display_name: ~4 GB RAM, ~0.5 CPUs
                description: Up to ~4 CPUs when available
                allowed_groups: *regular_allowed_groups
                kubespawner_override:
                  mem_guarantee: 3902839759
                  mem_limit: 3902839759
                  cpu_guarantee: 0.4563125
                  cpu_limit: 3.6505
                  node_selector:
                    node.kubernetes.io/instance-type: r5.xlarge
              mem_7_gb:
                display_name: ~7 GB RAM, ~0.9 CPUs
                allowed_groups: *regular_allowed_groups
                description: Up to ~4 CPUs when available
                kubespawner_override:
                  mem_guarantee: 7805679519
                  mem_limit: 7805679519
                  cpu_guarantee: 0.912625
                  cpu_limit: 3.6505
                  node_selector:
                    node.kubernetes.io/instance-type: r5.xlarge
              mem_15_gb:
                display_name: ~15 GB RAM, ~1.8 CPUs
                allowed_groups: *regular_allowed_groups
                description: Up to ~4 CPUs when available
                kubespawner_override:
                  mem_guarantee: 15611359038
                  mem_limit: 15611359038
                  cpu_guarantee: 1.82525
                  cpu_limit: 3.6505
                  node_selector:
                    node.kubernetes.io/instance-type: r5.xlarge
              mem_29_gb:
                display_name: ~29 GB RAM, ~4 CPUs
                allowed_groups: *regular_allowed_groups
                description: ~4 CPUs always available
                kubespawner_override:
                  mem_guarantee: 31222718077
                  mem_limit: 31222718077
                  cpu_guarantee: 3.6505
                  cpu_limit: 3.6505
                  node_selector:
                    node.kubernetes.io/instance-type: r5.xlarge
              mem_60_gb:
                display_name: ~60 GB RAM, ~8 CPUs
                description: Up to ~15 CPUs when available
                allowed_groups: &large_allowed_groups
                - 2i2c-org:hub-access-for-2i2c-staff
                - CryoInTheCloud:CryoCloudAdvanced
                - CryoInTheCloud:ml-in-glaciology
                kubespawner_override:
                  mem_guarantee: 64020707016
                  mem_limit: 64020707016
                  cpu_guarantee: 7.69055
                  cpu_limit: 15.3811
                  node_selector:
                    node.kubernetes.io/instance-type: r5.4xlarge
              mem_119_gb:
                display_name: ~119 GB RAM, ~15 CPUs
                allowed_groups: *large_allowed_groups
                description: ~15 CPUs always available
                kubespawner_override:
                  mem_guarantee: 128041414033
                  mem_limit: 128041414033
                  cpu_guarantee: 15.3811
                  cpu_limit: 15.3811
                  node_selector:
                    node.kubernetes.io/instance-type: r5.4xlarge
      - display_name: NVIDIA Tesla T4, ~16 GB, ~4 CPUs
        description: Start a container on a dedicated node with a GPU
        slug: gpu
        allowed_groups:
        - 2i2c-org:hub-access-for-2i2c-staff
        - CryoInTheCloud:ml-in-glaciology
        profile_options:
          image:
            display_name: Image
            unlisted_choice:
              enabled: true
              display_name: Custom image
              validation_regex: ^.+:.+$
              validation_message: Must be a publicly available docker image of form <image-name>:<tag>
              kubespawner_override:
                image: '{value}'
            choices:
              pytorch:
                display_name: Pangeo PyTorch ML Notebook
                default: true
                slug: pytorch
                kubespawner_override:
                  image: quay.io/pangeo/pytorch-notebook:2025.05.22
        kubespawner_override:
          environment:
            NVIDIA_DRIVER_CAPABILITIES: compute,utility
          mem_limit:
          mem_guarantee: 14G
          node_selector:
            node.kubernetes.io/instance-type: g4dn.xlarge
          extra_resource_limits:
            nvidia.com/gpu: '1'
    scheduling:
      userScheduler:
        enabled: true
  jupyterhub-home-nfs:
    enabled: true
    eks:
      enabled: true
    prometheusExporter:
      enabled: true

