basehub:
  nfs:
    pv:
      # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
      mountOptions:
        - rsize=1048576
        - wsize=1048576
        - timeo=600
        - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
        - retrans=2
        - noresvport
      serverIP: fs-01707b06.efs.us-west-2.amazonaws.com
      # This is different from rest of our hubs!
      baseShareName: /

  jupyterhub:
    custom:
      2i2c:
        add_staff_user_ids_to_admin_users: true
        add_staff_user_ids_of_type: "github"
      homepage:
        templateVars:
          org:
            name: Jupyter meets the Earth
            logo_url: https://pangeo-data.github.io/jupyter-earth/_static/jupyter-earth.png
            url: https://jupytearth.org
          designed_by:
            name: 2i2c
            url: https://2i2c.org
          operated_by:
            name: 2i2c
            url: https://2i2c.org
          funded_by:
            name: Jupyter meets the Earth
            url: https://jupytearth.org

    singleuser:
      extraFiles:
        jupyter_server_config.json:
          mountPath: /etc/jupyter/jupyter_notebook_config.json
          data:
            # Allow jupyterlab option to show hidden files in browser
            # https://github.com/berkeley-dsep-infra/datahub/issues/3160
            ContentsManager:
              allow_hidden: true
      initContainers:
        # Need to explicitly set this up and copy what's in basehub/values.yaml
        # as we have an extra 'shared-public' directory here.
        - name: volume-mount-ownership-fix
          image: busybox
          command:
            [
              "sh",
              "-c",
              "id && chown 1000:1000 /home/jovyan /home/jovyan/shared /home/jovyan/shared-public && ls -lhd /home/jovyan",
            ]
          securityContext:
            runAsUser: 0
          volumeMounts:
            - name: home
              mountPath: /home/jovyan
              subPath: "{username}"
            - name: home
              mountPath: /home/jovyan/shared
              subPath: _shared
            - name: home
              mountPath: /home/jovyan/shared-public
              subPath: _shared_public

      # /dev/shm is mounted as a filesystem path, where writing to it means to
      # write to memory.
      #
      # How to: https://stackoverflow.com/questions/46085748/define-size-for-dev-shm-on-container-engine/46434614#46434614
      # Request for this by Ellie: https://fperezgroup.slack.com/archives/C020XCEFPEH/p1658168872788389
      storage:
        extraVolumes:
          - name: dev-shm
            emptyDir:
              medium: Memory
        extraVolumeMounts:
          - name: dev-shm
            mountPath: /dev/shm
          # FIXME: we override the list extraVolumeMounts which is also set in
          #        the the basehub chart, due to that, we need to add this here
          #        as well. An option is to add hub.extraConfig entries that
          #        append the kubespawner configuration to include these extra
          #        volume mounts.
          #
          - name: home
            mountPath: /home/jovyan/shared
            subPath: _shared
            readOnly: true
          - name: home
            mountPath: /home/jovyan/shared-public
            subPath: _shared_public

      # Increased as we have experienced a too slow image pull at least
      # once. Our pods can take ~6-7 minutes to start on a new node it
      # seems, so this gives us some margin.
      startTimeout: 1200

      extraEnv:
        GH_SCOPED_CREDS_APP_URL: https://github.com/apps/hub-jupytearth-org-github-integ
        GH_SCOPED_CREDS_CLIENT_ID: Iv1.a073b1649637af12
      profileList:
        - display_name: "16th of Medium: 0.25-4 CPU, 1-16 GB"
          default: True
          description: "A shared machine, the recommended option until you experience a limitation."
          kubespawner_override:
            cpu_guarantee: 0.225
            mem_guarantee: 0.875G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: m5.xlarge
          profile_options: &profile_options
            image:
              display_name: Image
              choices:
                jmte:
                  display_name: JMTE all-in-one image (deprecated)
                  default: true
                  slug: "jmte"
                  kubespawner_override:
                    image: "286354552638.dkr.ecr.us-west-2.amazonaws.com/jmte/user-env:9baee2d"
                tensorflow:
                  display_name: Pangeo Tensorflow ML Notebook
                  slug: "tensorflow"
                  kubespawner_override:
                    image: "quay.io/pangeo/ml-notebook:2023.05.18"
                pytorch:
                  display_name: Pangeo PyTorch ML Notebook
                  slug: "pytorch"
                  kubespawner_override:
                    image: "quay.io/pangeo/pytorch-notebook:2023.05.18"
                datascience:
                  display_name: Jupyter DockerStacks Julia Notebook
                  slug: "julia"
                  kubespawner_override:
                    image: "jupyter/julia-notebook:2023-07-05"
        - display_name: "4th of Medium: 1-4 CPU, 4-16 GB"
          description: "A shared machine."
          profile_options: *profile_options
          kubespawner_override:
            cpu_guarantee: 0.875
            mem_guarantee: 3.5G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: m5.xlarge
        - display_name: "Medium: 4 CPU, 16 GB"
          description: "A dedicated machine for you."
          profile_options: *profile_options
          kubespawner_override:
            cpu_guarantee: 3.5
            mem_guarantee: 14G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: m5.xlarge
        - display_name: "Large: 16 CPU, 64 GB"
          description: "A dedicated machine for you."
          profile_options: *profile_options
          kubespawner_override:
            mem_guarantee: 56G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: m5.4xlarge
        - display_name: "Massive: 64 CPU, 256 GB"
          description: "A dedicated machine for you."
          profile_options: *profile_options
          kubespawner_override:
            mem_guarantee: 224G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: m5.16xlarge
        - display_name: "Massive high-memory: 64 CPU, 976 GB"
          description: "A dedicated machine for you."
          profile_options: *profile_options
          kubespawner_override:
            mem_guarantee: 900G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: x1.16xlarge
        - display_name: "Medium GPU: 4 CPU, 16 GB, 1 T4 Tensor Core GPU"
          description: "A dedicated machine for you with one GPU attached."
          profile_options: *profile_options
          kubespawner_override:
            cpu_guarantee: 3.5
            mem_guarantee: 14G
            mem_limit: null
            environment:
              NVIDIA_DRIVER_CAPABILITIES: compute,utility
            node_selector:
              node.kubernetes.io/instance-type: g4dn.xlarge
            extra_resource_limits:
              nvidia.com/gpu: "1"
        - display_name: "Large GPU: 16 CPU, 64 GB, 1 T4 Tensor Core GPU"
          description: "A dedicated machine for you with one GPU attached."
          profile_options: *profile_options
          kubespawner_override:
            mem_guarantee: 56G
            mem_limit: null
            environment:
              NVIDIA_DRIVER_CAPABILITIES: compute,utility
            node_selector:
              node.kubernetes.io/instance-type: g4dn.4xlarge
            extra_resource_limits:
              nvidia.com/gpu: "1"
        - display_name: "Massive GPU: 64 CPU, 256 GB, 1 T4 Tensor Core GPU"
          description: "A dedicated machine for you with one GPU attached."
          profile_options: *profile_options
          kubespawner_override:
            mem_guarantee: 200G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: g4dn.16xlarge
            environment:
              NVIDIA_DRIVER_CAPABILITIES: compute,utility
            extra_resource_limits:
              nvidia.com/gpu: "1"

    hub:
      config:
        JupyterHub:
          authenticator_class: cilogon
        CILogonOAuthenticator:
          scope:
            - "profile"
          username_claim: "preferred_username"
          # Only show the option to login with GitHub
          shown_idps:
            - http://github.com/login/oauth/authorize
        Authenticator:
          allowed_users: &users
            # This is just listing a few of the users/admins, a lot of
            # users has been added manually, see:
            # https://github.com/pangeo-data/jupyter-earth/issues/53
            - abbyazari # Abby Azari
            - andersy005 # Anderson Banihirwe
            - consideratio # Erik Sundell
            - choldgraf # Chris Holdgraf
            - elliesch # Ellie Abrahams
            - EMscience # Edom Moges
            - espg # Shane Grigsby
            - facusapienza21 # Facundo Sapienza
            - fperez # Fernando PÃ©rez
            - kmpaul # Kevin Paul
            - lrennels # Lisa Rennels
            - mrsiegfried # Matthew Siegfried
            - tsnow03 # Tasha Snow
            - whyjz # Whyjay Zheng
            - yuvipanda # Yuvi Panda
            - jonathan-taylor # Jonathan Taylor
          admin_users: *users
      allowNamedServers: true

dask-gateway:
  gateway:
    backend:
      scheduler:
        # IMPORTANT: We have experienced that the scheduler can fail with
        #            1GB memory limit. This was observed "stream closed"
        #            from the python client working against the
        #            Dask-Gateway created DaskCluster.
        #
        #            CommClosedError: in <TLS (closed) ConnectionPool.gather local=tls://192.168.40.210:54296 remote=gateway://traefik-prod-dask-gateway.prod:80/prod.b9600f678bb747c1a5f038b5bef3eb90>: Stream is closed
        #
        cores:
          request: 1
          limit: 64
        memory:
          request: 2G
          limit: 500G

    # Note that we are overriding options provided in 2i2c's helm chart that has
    # default values for these config entries.
    #
    extraConfig:
      # This configuration represents options that can be presented to users
      # that want to create a Dask cluster using dask-gateway. For more
      # details, see https://gateway.dask.org/cluster-options.html
      #
      # The goal is to provide a simple configuration that allow the user some
      # flexibility while also fitting well well on AWS nodes that are all
      # having 1:4 ratio between CPU and GB of memory. By providing the
      # username label, we help administrators to track user pods.
      option_handler: |
        from dask_gateway_server.options import Options, Select, String, Mapping
        def cluster_options(user):
            def option_handler(options):
                if ":" not in options.image:
                    raise ValueError("When specifying an image you must also provide a tag")
                extra_labels = {}
                scheduler_extra_pod_annotations = {
                    "prometheus.io/scrape": "true",
                    "prometheus.io/port": "8787",
                }
                chosen_worker_cpu = int(options.worker_specification.split("CPU")[0])
                chosen_worker_memory = 4 * chosen_worker_cpu
                # We multiply the requests by a fraction to ensure that the
                # worker fit well within a node that need some resources
                # reserved for system pods.
                return {
                    # A default image is suggested via DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE env variable
                    "image": options.image,
                    "scheduler_extra_pod_labels": extra_labels,
                    "scheduler_extra_pod_annotations": scheduler_extra_pod_annotations,
                    "worker_extra_pod_labels": extra_labels,
                    "worker_cores": 0.85 * chosen_worker_cpu,
                    "worker_cores_limit": chosen_worker_cpu,
                    "worker_memory": "%fG" % (0.85 * chosen_worker_memory),
                    "worker_memory_limit": "%fG" % chosen_worker_memory,
                    "environment": options.environment,
                }
            return Options(
                Select(
                    "worker_specification",
                    [
                        "1CPU, 4GB",
                        "2CPU, 8GB",
                        "4CPU, 16GB",
                        "8CPU, 32GB",
                        "16CPU, 64GB",
                        "32CPU, 128GB",
                        "64CPU, 256GB",
                    ],
                    default="1CPU, 4GB",
                    label="Worker specification",
                ),
                # The default image is set via DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE env variable
                String("image", label="Image"),
                Mapping("environment", {}, label="Environment variables"),
                handler=option_handler,
            )
        c.Backend.cluster_options = cluster_options
      idle: |
        # timeout after 30 minutes of inactivity
        c.KubeClusterConfig.idle_timeout = 1800
