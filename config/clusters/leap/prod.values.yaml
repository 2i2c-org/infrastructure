dask-gateway:
  gateway:
    backend:
      scheduler:
        cores:
          request: 1
          limit: 2
        memory:
          request: 2G
          limit: 4G
basehub:
  nfs:
    pv:
      serverIP: storage-quota-home-nfs.prod.svc.cluster.local
  userServiceAccount:
    annotations:
      iam.gke.io/gcp-service-account: leap-prod@leap-pangeo.iam.gserviceaccount.com
  jupyterhub:
    ingress:
      hosts: [leap.2i2c.cloud]
      tls:
      - hosts: [leap.2i2c.cloud]
        secretName: https-auto-tls
    proxy:
      chp:
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 1
            memory: 512Mi
    scheduling:
      userPlaceholder:
        # Keep 5 full nodes around
        replicas: 5
        resources:
          requests:
            # A=122203068Ki allocatable (memory available for pods)
            # A running node in-practice was using 687865856 (~688Mi) that we'll reserve as R
            # So given that no profile < 1GiB, let's round to R=1GiB to give some room
            # Memory of placeholder = A - R = (see below)
            memory: 124062199808

    hub:
      resources:
        requests:
          cpu: 100m
          memory: 512Mi
        limits:
          cpu: 1
          memory: 1Gi
      config:
        GitHubOAuthenticator:
          oauth_callback_url: https://leap.2i2c.cloud/hub/oauth_callback
    singleuser:
      extraEnv:
        SCRATCH_BUCKET: gs://leap-scratch/$(JUPYTERHUB_USER)
        PERSISTENT_BUCKET: gs://leap-persistent/$(JUPYTERHUB_USER)
        PANGEO_SCRATCH: gs://leap-scratch/$(JUPYTERHUB_USER)
  jupyterhub-home-nfs:
    gke:
      volumeId: projects/leap-pangeo/zones/us-central1-c/disks/hub-nfs-homedirs-prod
    quotaEnforcer:
      config:
        QuotaManager:
          hard_quota: 100 # in GiB
    resources:
      requests:
        cpu: 0.2
        memory: 1G
      limits:
        cpu: 0.4
        memory: 2G
