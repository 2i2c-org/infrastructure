userServiceAccount:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::790657130469:role/2i2c-aws-us-itcoocean
jupyterhub:
  ingress:
    hosts: [itcoocean.2i2c.cloud]
    tls:
      - hosts: [itcoocean.2i2c.cloud]
        secretName: https-auto-tls
  custom:
    2i2c:
      add_staff_user_ids_to_admin_users: true
      add_staff_user_ids_of_type: "github"
    jupyterhubConfigurator:
      enabled: false
    homepage:
      templateVars:
        org:
          name: ITCOocean
          url: https://incois.gov.in/ITCOocean/index.jsp
          logo_url: https://user-images.githubusercontent.com/2545978/253672085-ec5ca6fb-147b-4fcd-87f1-431108b62558.png
        designed_by:
          name: 2i2c
          url: https://2i2c.org
        operated_by:
          name: 2i2c
          url: https://2i2c.org
        funded_by:
          name: ESIP
          url: https://www.esipfed.org/
  hub:
    config:
      JupyterHub:
        authenticator_class: github
      GitHubOAuthenticator:
        oauth_callback_url: https://itcoocean.2i2c.cloud/hub/oauth_callback
        populate_teams_in_auth_state: true
        allowed_organizations:
          - Hackweek-ITCOocean:itcoocean-hackweek-2023
          - nmfs-opensci:2i2c-demo
        scope:
          - read:org
      Authenticator:
        enable_auth_state: true
        admin_users:
          - eeholmes # Eli Holmes, Community representative
  singleuser:
    nodeSelector:
      2i2c/hub-name: itcoocean
    # Requested in https://2i2c.freshdesk.com/a/tickets/1320
    defaultUrl: /lab
    # shared-public for collaboration
    # See https://github.com/2i2c-org/infrastructure/issues/2821#issuecomment-1665642853
    storage:
      extraVolumeMounts:
        - name: home
          mountPath: /home/jovyan/shared
          subPath: _shared
          readOnly: true
        - name: home
          mountPath: /home/jovyan/shared-public
          subPath: _shared-public
          readOnly: false
        - name: dev-shm
          mountPath: /dev/shm
    initContainers:
      - name: volume-mount-ownership-fix
        image: busybox:1.36.1
        command:
          - sh
          - -c
          - id && chown 1000:1000 /home/jovyan /home/jovyan/shared /home/jovyan/shared-public && ls -lhd /home/jovyan
        securityContext:
          runAsUser: 0
        volumeMounts:
          - name: home
            mountPath: /home/jovyan
            subPath: "{username}"
          # Mounted without readonly attribute here,
          # so we can chown it appropriately
          - name: home
            mountPath: /home/jovyan/shared
            subPath: _shared
          - name: home
            mountPath: /home/jovyan/shared-public
            subPath: _shared-public
    extraEnv:
      SCRATCH_BUCKET: s3://2i2c-aws-us-scratch-itcoocean/$(JUPYTERHUB_USER)
    profileList:
      # NOTE: About node sharing
      #
      #       CPU/Memory requests/limits are actively considered still. This
      #       profile list is setup to involve node sharing as considered in
      #       https://github.com/2i2c-org/infrastructure/issues/2121.
      #
      #       - Memory requests are different from the description, based on:
      #         what's found to remain allocate in k8s, subtracting 1GiB
      #         overhead for misc system pods, and transitioning from GB in
      #         description to GiB in mem_guarantee
      #         https://cloud.google.com/kubernetes-engine/docs/concepts/plan-node-sizes.
      #
      #       - CPU requests are lower than the description, with a factor
      #         that depends on the node's total CPU like: (node_cpu - 1)/node_cpu
      #
      #         The purpose of this is to ensure that we manage to schedule pods
      #         even if system pods have requested up to 1 CPU.
      #
      #         4  CPU node: 0.75
      #         16 CPU node: 0.9375
      #         64 CPU node: 0.984375
      #
      #       - Memory limits are setup conservatively to protect users from
      #         acquiring a false belief that what memory request they have made
      #         has been sufficient. This helps users tune their resource
      #         requests before events.
      #
      - display_name: "Small: up to 4 CPU / 32 GB RAM"
        description: &profile_list_description "Start a container with at least a chosen share of capacity on a node of this type"
        slug: small
        default: true
        profile_options:
          image: &profile_list_profile_options_image
            display_name: Image
            choices:
              geospatial-python-tensorflow:
                display_name: Geospatial Python with tensorflow
                kubespawner_override:
                  image: eeholmes/iopython-tf:20230901
              geospatial-python-openscapes:
                display_name: Openscapes Python
                kubespawner_override:
                  image: openscapes/python:f577786
              geospatial-python-normal:
                display_name: Geospatial Python
                kubespawner_override:
                  image: eeholmes/iopython:20230901
              geospatial-r-normal:
                display_name: Geospatial R
                kubespawner_override:
                  image: openscapes/rocker:a7596b5
              geospatial-r-sdm:
                default: true
                display_name: Geospatial R with SDM
                kubespawner_override:
                  image: eeholmes/iorocker:20230901
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_1:
                default: true
                display_name: ~1 GB, ~0.125 CPU
                kubespawner_override:
                  mem_guarantee: 0.836G
                  mem_limit: 1G
                  cpu_guarantee: 0.094
              mem_2:
                display_name: ~2 GB, ~0.25 CPU
                kubespawner_override:
                  mem_guarantee: 1.671G
                  mem_limit: 2G
                  cpu_guarantee: 0.188
              mem_4:
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.342G
                  mem_limit: 4G
                  cpu_guarantee: 0.375
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 6.684G
                  mem_limit: 8G
                  cpu_guarantee: 0.75
              mem_16:
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 13.369G
                  mem_limit: 16G
                  cpu_guarantee: 1.5
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 26.738G
                  mem_limit: 32G
                  cpu_guarantee: 3.0
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
          node_selector:
            node.kubernetes.io/instance-type: r5.xlarge
      - display_name: "Medium: up to 16 CPU / 128 GB RAM"
        description: *profile_list_description
        slug: medium
        profile_options:
          image: *profile_list_profile_options_image
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_1:
                display_name: ~1 GB, ~0.125 CPU
                kubespawner_override:
                  mem_guarantee: 0.903G
                  mem_limit: 1G
                  cpu_guarantee: 0.117
              mem_2:
                display_name: ~2 GB, ~0.25 CPU
                kubespawner_override:
                  mem_guarantee: 1.805G
                  mem_limit: 2G
                  cpu_guarantee: 0.234
              mem_4:
                default: true
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.611G
                  mem_limit: 4G
                  cpu_guarantee: 0.469
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 7.222G
                  mem_limit: 8G
                  cpu_guarantee: 0.938
              mem_16:
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 14.444G
                  mem_limit: 16G
                  cpu_guarantee: 1.875
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 28.887G
                  mem_limit: 32G
                  cpu_guarantee: 3.75
              mem_64:
                display_name: ~64 GB, ~8.0 CPU
                kubespawner_override:
                  mem_guarantee: 57.775G
                  mem_limit: 64G
                  cpu_guarantee: 7.5
              mem_128:
                display_name: ~128 GB, ~16.0 CPU
                kubespawner_override:
                  mem_guarantee: 115.549G
                  mem_limit: 128G
                  cpu_guarantee: 15.0
      # Requested in: https://2i2c.freshdesk.com/a/tickets/1320
      - display_name: "Bring your own image"
        description: Specify your own docker image (must have python and jupyterhub installed in it)
        slug: custom
        allowed_groups:
          - Hackweek-ITCOocean:itcoocean-hackweek-2023
          - nmfs-opensci:2i2c-demo
          - 2i2c-org:hub-access-for-2i2c-staff
        profile_options:
          image:
            display_name: Image
            unlisted_choice:
              enabled: True
              display_name: "Custom image"
              validation_regex: "^.+:.+$"
              validation_message: "Must be a publicly available docker image, of form <image-name>:<tag>"
              kubespawner_override:
                image: "{value}"
            choices: {}
          resource_allocation:
            display_name: Resource Allocation
            choices:
              mem_1_9:
                display_name: 1.9 GB RAM, upto 3.7 CPUs
                kubespawner_override:
                  mem_guarantee: 1991244775
                  mem_limit: 1991244775
                  cpu_guarantee: 0.2328125
                  cpu_limit: 3.725
                  node_selector:
                    node.kubernetes.io/instance-type: r5.xlarge
                default: true
              mem_3_7:
                display_name: 3.7 GB RAM, upto 3.7 CPUs
                kubespawner_override:
                  mem_guarantee: 3982489550
                  mem_limit: 3982489550
                  cpu_guarantee: 0.465625
                  cpu_limit: 3.725
                  node_selector:
                    node.kubernetes.io/instance-type: r5.xlarge
              mem_7_4:
                display_name: 7.4 GB RAM, upto 3.7 CPUs
                kubespawner_override:
                  mem_guarantee: 7964979101
                  mem_limit: 7964979101
                  cpu_guarantee: 0.93125
                  cpu_limit: 3.725
                  node_selector:
                    node.kubernetes.io/instance-type: r5.xlarge
              mem_14_8:
                display_name: 14.8 GB RAM, upto 3.7 CPUs
                kubespawner_override:
                  mem_guarantee: 15929958203
                  mem_limit: 15929958203
                  cpu_guarantee: 1.8625
                  cpu_limit: 3.725
                  node_selector:
                    node.kubernetes.io/instance-type: r5.xlarge
              mem_29_7:
                display_name: 29.7 GB RAM, upto 3.7 CPUs
                kubespawner_override:
                  mem_guarantee: 31859916406
                  mem_limit: 31859916406
                  cpu_guarantee: 3.725
                  cpu_limit: 3.725
                  node_selector:
                    node.kubernetes.io/instance-type: r5.xlarge
              mem_60_6:
                display_name: 60.6 GB RAM, upto 15.6 CPUs
                kubespawner_override:
                  mem_guarantee: 65094448840
                  mem_limit: 65094448840
                  cpu_guarantee: 7.8475
                  cpu_limit: 15.695
                  node_selector:
                    node.kubernetes.io/instance-type: r5.4xlarge
              mem_121_2:
                display_name: 121.2 GB RAM, upto 15.6 CPUs
                kubespawner_override:
                  mem_guarantee: 130188897681
                  mem_limit: 130188897681
                  cpu_guarantee: 15.695
                  cpu_limit: 15.695
                  node_selector:
                    node.kubernetes.io/instance-type: r5.4xlarge
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
          node_selector:
            node.kubernetes.io/instance-type: r5.4xlarge
