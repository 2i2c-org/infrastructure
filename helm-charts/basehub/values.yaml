azureFile:
  enabled: false
  pv:
    secretNamespace: azure-file
    secretName: access-credentials
    shareName: homes
    mountOptions:
      - uid=1000
      - forceuid
      - gid=1000
      - forcegid
      - nobrl

nfs:
  enabled: true
  shareCreator:
    enabled: true
    tolerations: []
  pv:
    mountOptions:
      - soft
      - noatime
      - vers=4.2
    serverIP: nfs-server-01
    # MUST HAVE TRAILING SLASH
    baseShareName: /export/home-01/homes/

# Use NFS provided by an in cluster server with the nfs-external-provisioner chart
inClusterNFS:
  enabled: false
  size: 100Gi

# A placeholder as global values that can be referenced from the same location
# of any chart should be possible to provide, but aren't necessarily provided or
# used.
global: {}

jupyterhub:
  custom:
    singleuserAdmin:
      extraVolumeMounts:
        - name: home
          mountPath: /home/jovyan/shared-readwrite
          subPath: _shared
    cloudResources:
      provider: ""
      gcp:
        projectId: ""
      scratchBucket:
        enabled: false
    docs_service:
      enabled: false
      repo: ""
      branch: ""
    2i2c:
      # Should 2i2c engineering staff user IDs be injected to the admin_users
      # configuration of the JupyterHub's authenticator by our custom
      # jupyterhub_config.py snippet as declared in hub.extraConfig?
      add_staff_user_ids_to_admin_users: false
      add_staff_user_ids_of_type: ""
      staff_github_ids:
        - choldgraf
        - consideRatio
        - damianavila
        - GeorgianaElena
        - sgibson91
        - yuvipanda
      staff_google_ids:
        - choldgraf@2i2c.org
        - erik@2i2c.org
        - damianavila@2i2c.org
        - georgianaelena@2i2c.org
        - sgibson@2i2c.org
        - yuvipanda@2i2c.org
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 256m
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: letsencrypt-prod
  scheduling:
    userPlaceholder:
      enabled: true
      replicas: 0
    podPriority:
      enabled: true
      globalDefault: false
      defaultPriority: 0
      userPlaceholderPriority: -10
    userScheduler:
      enabled: true
      nodeSelector:
        hub.jupyter.org/node-purpose: core
      resources:
        requests:
          # FIXME: Just unset this?
          cpu: 0.01
          memory: 64Mi
        limits:
          memory: 1G
  prePuller:
    continuous:
      enabled: false
    hook:
      enabled: false
  proxy:
    service:
      type: ClusterIP
    chp:
      nodeSelector:
        hub.jupyter.org/node-purpose: core
      resources:
        requests:
          # FIXME: We want no guarantees here!!!
          # This is lowest possible value
          cpu: 0.01
          memory: 64Mi
        limits:
          memory: 1Gi
    traefik:
      image:
        tag: v2.4.8
      nodeSelector:
        hub.jupyter.org/node-purpose: core
      resources:
        requests:
          memory: 64Mi
        limits:
          memory: 1Gi
    https:
      enabled: false
      letsencrypt:
        contactEmail: yuvipanda@gmail.com
  singleuser:
    extraEnv:
      # notebook writes secure files that don't need to survive a
      # restart here. Writing 'secure' files on some file systems (like
      # Azure Files with SMB) seems buggy, so we just put runtime dir on
      # /tmp. This is ok in our case, since no two users are on the same
      # container.
      JUPYTER_RUNTIME_DIR: /tmp/.jupyter-runtime
      # By default, /bin/sh is used as shell for terminals, not /bin/bash
      # Most people do not expect this, so let's match expectation
      SHELL: /bin/bash
    extraFiles:
      jupyter_notebook_config.json:
        mountPath: /usr/local/etc/jupyter/jupyter_notebook_config.json
        # if a user leaves a notebook with a running kernel,
        # the effective idle timeout will typically be cull idle timeout
        # of the server + the cull idle timeout of the kernel,
        # as culling the kernel will register activity,
        # resetting the no_activity timer for the server as a whole
        data:
          MappingKernelManager:
            # shutdown kernels after no activity
            cull_idle_timeout: 3600
            # check for idle kernels this often
            cull_interval: 300
            # a kernel with open connections but no activity still counts as idle
            # this is what allows us to shutdown servers
            # when people leave a notebook open and wander off
            cull_connected: true
    startTimeout: 600 # 10 mins, because sometimes we have too many new nodes coming up together
    defaultUrl: /tree
    nodeSelector:
      hub.jupyter.org/node-purpose: user
    image:
      name: quay.io/2i2c/2i2c-hubs-image
      tag: 532c5eab47a1
    storage:
      type: static
      static:
        pvcName: home-nfs
        subPath: "{username}"
      extraVolumeMounts:
        - name: home
          mountPath: /home/jovyan/shared
          subPath: _shared
          readOnly: true
    memory:
      guarantee: 256M
      limit: 1G
    cpu:
      # If no CPU limit is set, it is possible for a single user or group of users to
      # starve everyone else of CPU time on a node, even causing new user pods to completely
      # fail as the notebook server process gets no CPU to complete auth handshake with
      # the server, and even trivial cells like `print("hello world")` may not run.
      # Unlike memory guarantees, CPU guarantees are actually enforced by the Linux Kernel
      # (see https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b)
      # By giving each user a 5% CPU guarantee (represented by 0.05), we ensure that:
      # 1. Simple cells will always execute
      # 2. Notebook server processes will always start - so users won't have server spawn failure
      # 3. We don't accidentally set just a high limit for a particular hub and not set a
      #    guarantee, at which point kubernetes treats the limit as the guarantee! This causes
      #    far more nodes to be scaled up than needed, making everything super slow (like in
      #    https://github.com/2i2c-org/infrastructure/issues/790)
      # 4. Most of our workloads are still memory bound, and we want scaling to happen only
      #    when a node is full on its memory guarantees. But a 0.05 guarantee means a n1-highmem-8
      #    node can fit 160 user pods, and since kubernetes already caps us at 100 pods a node,
      #    this guarantee doesn't actually change our scheduling.
      guarantee: 0.05
    networkPolicy:
      # Allow unrestricted access to the internet but not local cluster network
      enabled: true
      egress:
        - to:
            - ipBlock:
                cidr: 0.0.0.0/0
                except:
                  # Don't allow network access to private IP ranges
                  # Listed in https://datatracker.ietf.org/doc/html/rfc1918
                  - 10.0.0.0/8
                  - 172.16.0.0/12
                  - 192.168.0.0/16
                  # Don't allow network access to the metadata IP
                  - 169.254.169.254/32
        # Allow code in hubs to talk to ingress provider, so they can talk to
        # the hub via its public URL
        - to:
            - namespaceSelector:
                matchLabels:
                  name: support
              podSelector:
                matchLabels:
                  app.kubernetes.io/name: ingress-nginx
        # If a hub is using autohttps instead of ingress-nginx, allow traffic
        # to the autohttps pod as well
        - to:
            - podSelector:
                matchLabels:
                  app: jupyterhub
                  component: autohttps
        # Allow traffic to the proxy pod from user pods
        # This is particularly important for daskhubs that utilise the proxy
        # in order to create clusters (schedulers and workers)
        - to:
            - podSelector:
                matchLabels:
                  app: jupyterhub
                  component: proxy
        # Allow traffic to the traefik pod from user pods. Needed for daskhubs.
        - to:
            - podSelector:
                matchLabels:
                  app.kubernetes.io/component: traefik
  hub:
    extraFiles:
      configurator-schema-default:
        mountPath: /usr/local/etc/jupyterhub-configurator/00-default.schema.json
        data:
          type: object
          name: config
          properties:
            KubeSpawner.image:
              type: string
              title: User docker image
              description: Determines languages, libraries and interfaces available
              help: Leave this blank to use the default
            Spawner.default_url:
              type: string
              title: Default User Interface
              enum:
                - "/tree"
                - "/lab"
                - "/rstudio"
              default: "/tree"
              enumMetadata:
                interfaces:
                  - value: "/tree"
                    title: Classic Notebook
                    description:
                      The original single-document interface for creating
                      Jupyter Notebooks.
                  - value: "/lab"
                    title: JupyterLab
                    description: A Powerful next generation notebook interface
                  - value: "/rstudio"
                    title: RStudio
                    description: An IDE For R, created by the RStudio company

    services:
      configurator:
        url: http://configurator:10101
        command:
          - python3
          - -m
          - jupyterhub_configurator.app
          - --Configurator.config_file=/usr/local/etc/jupyterhub-configurator/jupyterhub_configurator_config.py
    image:
      name: quay.io/2i2c/pilot-hub
      tag: "0.0.1-n1159.h5b045cd"
    nodeSelector:
      hub.jupyter.org/node-purpose: core
    networkPolicy:
      enabled: true
      ingress:
        - from:
            - podSelector:
                matchLabels:
                  app: jupyterhub
                  component: hub
          ports:
            - port: 8081
              protocol: TCP
        - from:
            - podSelector:
                matchLabels:
                  app: jupyterhub
                  component: proxy
            - podSelector:
                matchLabels:
                  app: jupyterhub
                  component: hub
          ports:
            - port: 10101
              protocol: TCP
        - from:
            - namespaceSelector:
                matchLabels:
                  name: support
              podSelector:
                matchLabels:
                  app: prometheus
                  component: server
          ports:
            - port: http
              protocol: TCP
    resources:
      requests:
        # Very small unit, since we don't want any CPU guarantees
        cpu: 0.01
        memory: 128Mi
      limits:
        memory: 2Gi
    extraConfig:
      01-working-dir: |
        # Make sure working directory is ${HOME}
        # hubploy has a bug where it unconditionally puts workingdir to be /srv/repo
        c.KubeSpawner.working_dir = '/home/jovyan'
      02-prometheus: |
        # Allow unauthenticated prometheus requests
        # Otherwise our prometheus server can't get to these
        c.JupyterHub.authenticate_prometheus = False
      03-no-setuid: |
        c.KubeSpawner.extra_container_config = {
          'securityContext': {
            # Explicitly disallow setuid binaries from working inside the container
            'allowPrivilegeEscalation': False
          }
        }
      04-custom-theme: |
        from z2jh import get_config
        c.JupyterHub.template_paths = ['/usr/local/share/jupyterhub/custom_templates/']

        c.JupyterHub.template_vars = {
          'custom': get_config('custom.homepage.templateVars')
        }
      05-custom-admin: |
        from z2jh import get_config
        from kubespawner import KubeSpawner
        from jupyterhub_configurator.mixins import ConfiguratorSpawnerMixin

        class CustomSpawner(ConfiguratorSpawnerMixin, KubeSpawner):
          def start(self, *args, **kwargs):
            custom_admin = get_config('custom.singleuserAdmin', {})
            if custom_admin and self.user.admin:
                extra_init_containers = custom_admin.get('initContainers', [])
                extra_volume_mounts = custom_admin.get('extraVolumeMounts', [])

                self.init_containers += [container for container in extra_init_containers if container not in self.init_containers]
                self.volume_mounts += [volume for volume in extra_volume_mounts if volume not in self.volume_mounts]

            return super().start(*args, **kwargs)


        c.JupyterHub.spawner_class = CustomSpawner
      06-cloud-storage-bucket: |
        from z2jh import get_config
        cloud_resources = get_config('custom.cloudResources')
        scratch_bucket = cloud_resources['scratchBucket']
        import os

        if scratch_bucket['enabled']:
          # FIXME: Support other providers too
          assert cloud_resources['provider'] == 'gcp'
          project_id = cloud_resources['gcp']['projectId']

          release = os.environ['HELM_RELEASE_NAME']
          bucket_protocol = 'gcs'
          bucket_name = f'{project_id}-{release}-scratch-bucket'
          env = {
            'SCRATCH_BUCKET_PROTOCOL': bucket_protocol,
            # Matches "daskhub.scratchBUcket.name" helm template
            'SCRATCH_BUCKET_NAME': bucket_name,
            # Use k8s syntax of $(ENV_VAR) to substitute env vars dynamically in other env vars
            'SCRATCH_BUCKET': f'{bucket_protocol}://{bucket_name}/$(JUPYTERHUB_USER)',
            'PANGEO_SCRATCH': f'{bucket_protocol}://{bucket_name}/$(JUPYTERHUB_USER)',
          }

          c.KubeSpawner.environment.update(env)
      07-2i2c-add-staff-user-ids-to-admin-users: |
        from z2jh import get_config
        add_staff_user_ids_to_admin_users = get_config("custom.2i2c.add_staff_user_ids_to_admin_users", False)

        if add_staff_user_ids_to_admin_users:
            user_id_type = get_config("custom.2i2c.add_staff_user_ids_of_type")
            staff_user_ids = get_config(f"custom.2i2c.staff_{user_id_type}_ids", [])
            c.Authenticator.admin_users.extend(staff_user_ids)

            # Check what authenticator class is set. If it's "github", we assume
            # GitHub Orgs/Teams is being used for auth and unset allowed_users
            # so valid members are not refused access.
            # FIXME: This should be handled in basehub's schema validation file
            # so that we get useful feedback about config. But at time of writing,
            # it doesn't have one! Issue to track the creation of such files is:
            # https://github.com/2i2c-org/infrastructure/issues/937
            authenticator_class = get_config("hub.config.JupyterHub.authenticator_class")
            if authenticator_class == "github" and c.Authenticator.allowed_users:
                print("WARNING: hub.config.JupyterHub.authenticator_class was set to github and c.Authenticator.allowed_users was set, custom 2i2c jupyterhub config is now resetting allowed_users to an empty set.")
                c.Authenticator.allowed_users = set()
      08-add-docs-service-if-enabled: |
        from z2jh import get_config

        if get_config("custom.docs_service.enabled"):
          c.JupyterHub.services.append({"name": "docs", "url": "http://docs-service"})
