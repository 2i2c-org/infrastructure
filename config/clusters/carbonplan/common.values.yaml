basehub:
  nfs:
    pv:
      # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
      mountOptions:
        - rsize=1048576
        - wsize=1048576
        - timeo=600
        - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
        - retrans=2
        - noresvport
      serverIP: fs-8a4e4f8d.efs.us-west-2.amazonaws.com
      baseShareName: /
  jupyterhub:
    custom:
      2i2c:
        add_staff_user_ids_to_admin_users: true
        add_staff_user_ids_of_type: "github"
      homepage:
        templateVars:
          org:
            name: Carbon Plan
            logo_url: https://pbs.twimg.com/profile_images/1262387945971101697/5q_X3Ruk_400x400.jpg
            url: https://carbonplan.org
          designed_by:
            name: 2i2c
            url: https://2i2c.org
          operated_by:
            name: 2i2c
            url: https://2i2c.org
          funded_by:
            name: Carbon Plan
            url: https://carbonplan.org
    singleuser:
      serviceAccountName: cloud-user-sa
      image:
        name: carbonplan/trace-python-notebook
        # pullPolicy set to "Always" because we use the changing over time tag
        # "latest".
        pullPolicy: Always
        tag: "latest"
      profileList:
        # The mem-guarantees are here so k8s doesn't schedule other pods
        # on these nodes.
        - display_name: "Small: r5.large"
          description: "~2 CPU, ~15G RAM"
          slug: "small"
          default: true
          profile_options: &profile_options
            image:
              display_name: Image
              choices:
                python:
                  display_name: Trace Python Notebook
                  default: true
                  slug: "python"
                  kubespawner_override:
                    image: carbonplan/trace-python-notebook:latest
                main:
                  display_name: Main Single User
                  slug: "main"
                  kubespawner_override:
                    image: carbonplan/main-single-user:latest
                cmip6:
                  display_name: CMIP6 Downscaling
                  slug: "cmip6"
                  kubespawner_override:
                    image: carbonplan/cmip6-downscaling-single-user:latest
          kubespawner_override:
            # Expllicitly unset mem_limit, so it overrides the default memory limit we set in
            # basehub/values.yaml
            mem_limit: null
            mem_guarantee: 12G
            node_selector:
              node.kubernetes.io/instance-type: r5.large
        - display_name: "Medium: r5.xlarge"
          description: "~4 CPU, ~30G RAM"
          profile_options: *profile_options
          kubespawner_override:
            mem_limit: null
            mem_guarantee: 29G
            node_selector:
              node.kubernetes.io/instance-type: r5.xlarge
        - display_name: "Large: r5.2xlarge"
          description: "~8 CPU, ~60G RAM"
          profile_options: *profile_options
          kubespawner_override:
            mem_limit: null
            mem_guarantee: 60G
            node_selector:
              node.kubernetes.io/instance-type: r5.2xlarge
        - display_name: "Huge: r5.8xlarge"
          description: "~32 CPU, ~256G RAM"
          profile_options: *profile_options
          kubespawner_override:
            mem_limit: null
            mem_guarantee: 240G
            node_selector:
              node.kubernetes.io/instance-type: r5.8xlarge
        - display_name: "Very Huge: x1.16xlarge"
          description: "~64 CPU, ~976G RAM"
          profile_options: *profile_options
          kubespawner_override:
            mem_limit: null
            mem_guarantee: 940G
            node_selector:
              node.kubernetes.io/instance-type: x1.16xlarge
        - display_name: "Very Very Huge: x1.32xlarge"
          description: "~128 CPU, ~1952G RAM"
          profile_options: *profile_options
          kubespawner_override:
            mem_limit: null
            mem_guarantee: 1900G
            node_selector:
              node.kubernetes.io/instance-type: x1.32xlarge
    scheduling:
      userScheduler:
        enabled: true
    proxy:
      chp:
        resources:
          requests:
            cpu: 0.5
            memory: 256Mi
          limits:
            cpu: 1
            memory: 4Gi
        nodeSelector: {}
    hub:
      resources:
        requests:
          cpu: 0.5
          memory: 256Mi
        limits:
          cpu: 1
          memory: 4Gi
      allowNamedServers: true
      networkPolicy:
        # FIXME: For dask gateway
        enabled: false
      readinessProbe:
        enabled: false
      nodeSelector: {}
      config:
        Authenticator:
          allowed_users: &users
            - jhamman
          admin_users: *users

dask-gateway:
  traefik:
    resources:
      requests:
        cpu: 0.5
        memory: 512Mi
      limits:
        cpu: 2
        memory: 4Gi
  controller:
    resources:
      requests:
        cpu: 0.5
        memory: 512Mi
      limits:
        cpu: 2
        memory: 4Gi
  gateway:
    backend:
      scheduler:
        extraPodConfig:
          serviceAccountName: cloud-user-sa
      worker:
        extraPodConfig:
          serviceAccountName: cloud-user-sa
    resources:
      requests:
        cpu: 0.5
        memory: 512Mi
      limits:
        cpu: 2
        memory: 4Gi
      # TODO: figure out a replacement for userLimits.
    extraConfig:
      idle: |
        # timeout after 30 minutes of inactivity
        c.KubeClusterConfig.idle_timeout = 1800
