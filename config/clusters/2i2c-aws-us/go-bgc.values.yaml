userServiceAccount:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::790657130469:role/2i2c-aws-us-go-bgc
nfs:
  pv:
    # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
    mountOptions:
      - rsize=1048576
      - wsize=1048576
      - timeo=600
      - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
      - retrans=2
      - noresvport
    serverIP: fs-0b70db2b65209a77d.efs.us-west-2.amazonaws.com
    baseShareName: /
jupyterhub:
  ingress:
    hosts: [go-bgc.2i2c.cloud]
    tls:
      - hosts: [go-bgc.2i2c.cloud]
        secretName: https-auto-tls
  custom:
    2i2c:
      add_staff_user_ids_to_admin_users: true
      add_staff_user_ids_of_type: "github"
    homepage:
      templateVars:
        org:
          name: GO-BGC
          url: https://www.go-bgc.org/news/2023-float-data-workshop
          logo_url: https://www.go-bgc.org/wp-content/uploads/2023/04/GO-BGCBGC-Argo-Float-Data-Workshop-Flyer-2023-2-980x656.jpeg
        designed_by:
          name: 2i2c
          url: https://2i2c.org
        operated_by:
          name: 2i2c
          url: https://2i2c.org
        funded_by:
          name: GO-BGC
          url: https://2i2c.org
  hub:
    config:
      JupyterHub:
        authenticator_class: github
      GitHubOAuthenticator:
        oauth_callback_url: https://go-bgc.2i2c.cloud/hub/oauth_callback
        allowed_organizations:
          - 2i2c-org:hub-access-for-2i2c-staff
          - go-bgc
        scope:
          - read:org
      Authenticator:
        admin_users:
          - AlisonGray # Alison Grey, Community representative
          - dnicholson # David Nicholson, Community representative
  singleuser:
    image:
      # image choice preliminary and is expected to be setup via
      # https://go-bgc.2i2c.cloud/services/configurator/ by the community
      #
      # pangeo/pangeo-notebook is maintained at: https://github.com/pangeo-data/pangeo-docker-images
      name: pangeo/pangeo-notebook
      tag: "2023.07.05"
    extraEnv:
      SCRATCH_BUCKET: s3://scratch-go-bgc/$(JUPYTERHUB_USER)
      PANGEO_SCRATCH: s3://scratch-go-bgc/$(JUPYTERHUB_USER)
    profileList:
      # NOTE: About node sharing
      #
      #       CPU/Memory requests/limits are actively considered still. This
      #       profile list is setup to involve node sharing as considered in
      #       https://github.com/2i2c-org/infrastructure/issues/2121.
      #
      #       - Memory requests are different from the description, based on:
      #         whats found to remain allocate in k8s, subtracting 1GiB
      #         overhead for misc system pods, and transitioning from GB in
      #         description to GiB in mem_guarantee
      #         https://cloud.google.com/kubernetes-engine/docs/concepts/plan-node-sizes.
      #
      #       - CPU requests are lower than the description, with a factor
      #         that depends on the node's total CPU like: (node_cpu - 1)/node_cpu
      #
      #         The purpose of this is to ensure that we manage to schedule pods
      #         even if system pods have requested up to 1 CPU.
      #
      #         4  CPU node: 0.75
      #         16 CPU node: 0.9375
      #         64 CPU node: 0.984375
      #
      - display_name: "Medium: up to 16 CPU / 128 GB RAM"
        description: "Start a container with at least a chosen share of capacity on a node of this type"
        slug: medium
        default: true
        profile_options:
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_1:
                display_name: ~1 GB, ~0.125 CPU
                kubespawner_override:
                  mem_guarantee: 0.903G
                  cpu_guarantee: 0.117
              mem_2:
                display_name: ~2 GB, ~0.25 CPU
                kubespawner_override:
                  mem_guarantee: 1.805G
                  cpu_guarantee: 0.234
              mem_4:
                default: true
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.611G
                  cpu_guarantee: 0.469
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 7.222G
                  cpu_guarantee: 0.938
              mem_16:
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 14.444G
                  cpu_guarantee: 1.875
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 28.887G
                  cpu_guarantee: 3.75
              mem_64:
                display_name: ~64 GB, ~8.0 CPU
                kubespawner_override:
                  mem_guarantee: 57.775G
                  cpu_guarantee: 7.5
              mem_128:
                display_name: ~128 GB, ~16.0 CPU
                kubespawner_override:
                  mem_guarantee: 115.549G
                  cpu_guarantee: 15.0
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
          node_selector:
            node.kubernetes.io/instance-type: r5.4xlarge
