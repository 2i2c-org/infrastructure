userServiceAccount:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::790657130469:role/2i2c-aws-us-itcoocean
nfs:
  pv:
    # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
    mountOptions:
      - rsize=1048576
      - wsize=1048576
      - timeo=600
      - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
      - retrans=2
      - noresvport
    serverIP: fs-0b70db2b65209a77d.efs.us-west-2.amazonaws.com
    baseShareName: /
jupyterhub:
  ingress:
    hosts: [itcoocean.2i2c.cloud]
    tls:
      - hosts: [itcoocean.2i2c.cloud]
        secretName: https-auto-tls
  custom:
    2i2c:
      add_staff_user_ids_to_admin_users: true
      add_staff_user_ids_of_type: "github"
    homepage:
      templateVars:
        org:
          name: ITCOocean
          url: https://incois.gov.in/ITCOocean/index.jsp
          logo_url: https://user-images.githubusercontent.com/2545978/253672085-ec5ca6fb-147b-4fcd-87f1-431108b62558.png
        designed_by:
          name: 2i2c
          url: https://2i2c.org
        operated_by:
          name: 2i2c
          url: https://2i2c.org
        funded_by:
          name: ESIP
          url: https://www.esipfed.org/
  hub:
    config:
      JupyterHub:
        authenticator_class: github
      GitHubOAuthenticator:
        oauth_callback_url: https://itcoocean.2i2c.cloud/hub/oauth_callback
        allowed_organizations:
          - 2i2c-org:hub-access-for-2i2c-staff
          - Hackweek-ITCOocean:itcoocean-hackweek-2023
        scope:
          - read:org
      Authenticator:
        admin_users:
          - eeholmes # Eli Holmes, Community representative
  singleuser:
    # shared-public for collaboration
    # See https://2i2c.freshdesk.com/a/tickets/814
    storage:
      extraVolumeMounts:
        - name: home
          mountPath: /home/jovyan/shared-public
          subPath: _shared-public
          readOnly: false
        - name: home
          mountPath: /home/jovyan/shared
          subPath: _shared
          readOnly: true
    initContainers:
      - name: volume-mount-ownership-fix
        image: busybox
        command:
          [
            "sh",
            "-c",
            "id && chown 1000:1000 /home/jovyan && chown 1000:1000 /home/jovyan/shared-readwrite && chown 1000:1000 /home/jovyan/shared-public && ls -lhd /home/jovyan ",
          ]
        securityContext:
          runAsUser: 0
        volumeMounts:
          - name: home
            mountPath: /home/jovyan
            subPath: "{username}"
          # Mounted without readonly attribute here,
          # so we can chown it appropriately
          - name: home
            mountPath: /home/jovyan/shared-readwrite
            subPath: _shared
          - name: home
            mountPath: /home/jovyan/shared-public
            subPath: _shared-public
    extraEnv:
      SCRATCH_BUCKET: s3://scratch-itcoocean/$(JUPYTERHUB_USER)
      PANGEO_SCRATCH: s3://scratch-itcoocean/$(JUPYTERHUB_USER)
    profileList:
      # NOTE: About node sharing
      #
      #       CPU/Memory requests/limits are actively considered still. This
      #       profile list is setup to involve node sharing as considered in
      #       https://github.com/2i2c-org/infrastructure/issues/2121.
      #
      #       - Memory requests are different from the description, based on:
      #         whats found to remain allocate in k8s, subtracting 1GiB
      #         overhead for misc system pods, and transitioning from GB in
      #         description to GiB in mem_guarantee
      #         https://cloud.google.com/kubernetes-engine/docs/concepts/plan-node-sizes.
      #
      #       - CPU requests are lower than the description, with a factor
      #         that depends on the node's total CPU like: (node_cpu - 1)/node_cpu
      #
      #         The purpose of this is to ensure that we manage to schedule pods
      #         even if system pods have requested up to 1 CPU.
      #
      #         4  CPU node: 0.75
      #         16 CPU node: 0.9375
      #         64 CPU node: 0.984375
      #
      - display_name: "Small: up to 4 CPU / 32 GB RAM"
        description: &profile_list_description "Start a container with at least a chosen share of capacity on a node of this type"
        slug: small
        default: true
        profile_options:
          image:
            display_name: Image
            choices:
              geospatial-python-tensorflow:
                display_name: Geospatial Python with tensorflow
                kubespawner_override:
                  image: eeholmes/iopython:20230714
              geospatial-python-normal:
                display_name: Geospatial Python
                kubespawner_override:
                  image: openscapes/python:f577786
              geospatial-r-normal:
                display_name: Geospatial R
                kubespawner_override:
                  image: openscapes/rocker:a7596b5
              geospatial-r-sdm:
                default: true
                display_name: Geospatial R with SDM
                kubespawner_override:
                  image: eeholmes/iorocker:20230714
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_1:
                default: true
                display_name: ~1 GB, ~0.125 CPU
                kubespawner_override:
                  mem_guarantee: 0.836G
                  cpu_guarantee: 0.094
              mem_2:
                display_name: ~2 GB, ~0.25 CPU
                kubespawner_override:
                  mem_guarantee: 1.671G
                  cpu_guarantee: 0.188
              mem_4:
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.342G
                  cpu_guarantee: 0.375
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 6.684G
                  cpu_guarantee: 0.75
              mem_16:
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 13.369G
                  cpu_guarantee: 1.5
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 26.738G
                  cpu_guarantee: 3.0
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
          node_selector:
            node.kubernetes.io/instance-type: r5.xlarge
