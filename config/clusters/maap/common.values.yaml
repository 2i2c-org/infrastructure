nfs:
  enabled: true
  pv:
    enabled: true
    # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
    mountOptions:
    - rsize=1048576
    - wsize=1048576
    - timeo=600
    - soft   # We pick soft over hard, so NFS lockups don't lead to hung processes
    - retrans=2
    - noresvport

dask-gateway:
  enabled: true
jupyterhub:
  custom:
    daskhubSetup:
      enabled: true
    2i2c:
      add_staff_user_ids_to_admin_users: true
      add_staff_user_ids_of_type: github
    homepage:
      templateVars:
        org:
          name: The Multi-Mission Algorithm and Analysis Platform (MAAP)Project
          logo_url: https://maap-project.org/wp-content/uploads/2021/10/nasamaaplogo3.png
          url: https://maap-project.org/
        designed_by:
          name: 2i2c
          url: https://2i2c.org
        operated_by:
          name: 2i2c
          url: https://2i2c.org
        funded_by:
          name: NASA
          url: https://www.earthdata.nasa.gov/esds
  hub:
    allowNamedServers: true
    config:
      JupyterHub:
        authenticator_class: generic-oauth
      Authenticator:
        admin_users:
        - freitagb
        - wildintellect
        authenticator_class: generic-oauth
        enable_auth_state: true
        # Disable for now the authentication token refreshing
        # https://oauthenticator.readthedocs.io/en/latest/reference/api/gen/oauthenticator.oauth2.html#oauthenticator.oauth2.OAuthenticator.auth_refresh_age
        auth_refresh_age: 0
      GenericOAuthenticator:
        # We want to get user data from the JWT that's the id token,
        # not from a separate userdata API call
        userdata_from_id_token: true
        scope:
        - basic
        - profile
        - openid
        username_claim: preferred_username
        manage_groups: true
        auth_state_groups_key: oauth_user.roles
        admin_groups:
        - Admin
        # Being granted *any* jupyterhub related role should allow you
        # to login
        allowed_groups:
        - Admin
        - CPU:XS
        - CPU:S
        - CPU:M
        - CPU:L
        - CPU:XL
        - CPU:XXL
        - CPU:XXXL
        - GPU:T4
    extraConfig:
      001-username-claim: |
        def populate_token(spawner, auth_state):
          # For our deployment-service-check health check user, there is no auth_state.
          # So these env variables need not be set.
          if auth_state:

            spawner.environment.update({
              "MAAP_PGT": f"jwt:{auth_state.get("id_token", "")}",
              "KC_ACCESS_TOKEN": auth_state.get("access_token", ""),
              "KC_ID_TOKEN": auth_state.get("id_token", ""),
              "KC_REFRESH_TOKEN": auth_state.get("refresh_token", "")
            })

        c.Spawner.auth_state_hook = populate_token
  singleuser:
    cloudMetadata:
      blockWithIptables: false
    defaultUrl: /lab
    storage:
      extraVolumes:
        01-s3f3-volume:
          name: s3fs-volume
          emptyDir: {}
      extraVolumeMounts:
        01-shared-public-volumemount:
          name: home
          mountPath: /home/jovyan/shared-public
          subPath: _shared-public
          readOnly: false
        02-shared-public-rstudio-volumemount:
          name: home
          mountPath: /home/rstudio/shared-public
          subPath: _shared-public
          readOnly: false
        03-s3f3-private-bucket-volumemount:
          name: s3fs-volume
          mountPath: /home/jovyan/my-private-bucket
          subPath: my-private-bucket
          mountPropagation: HostToContainer
          readOnly: false
        04-s3f3-public-bucket-volumemount:
          name: s3fs-volume
          mountPath: /home/jovyan/my-public-bucket
          subPath: my-public-bucket
          mountPropagation: HostToContainer
          readOnly: false
        05-s3f3-shared-bucket-volumemount:
          name: s3fs-volume
          mountPath: /home/jovyan/shared-buckets
          subPath: shared-buckets
          mountPropagation: HostToContainer
          readOnly: true
        06-s3f3-private-triaged-jobs-volumemount:
          name: s3fs-volume
          mountPath: /home/jovyan/triaged-jobs
          subPath: triaged-jobs
          mountPropagation: HostToContainer
          readOnly: true
    extraContainers:
    - name: s3fs
      image: mas.dit.maap-project.org/root/che-sidecar-s3fs:2i2c
      image_pull_policy: Always
      securityContext:
        privileged: true
      resources:
        limits:
          memory: 512Mi
          cpu: 1.0
        requests:
          # If we don't set requests, k8s sets requests == limits!
          # So we set something tiny
          memory: 64Mi
          cpu: 0.01
      volumeMounts:
      - name: s3fs-volume
        mountPath: /my-public-bucket
        subPath: my-public-bucket
        mountPropagation: Bidirectional
      - name: s3fs-volume
        mountPath: /my-private-bucket
        subPath: my-private-bucket
        mountPropagation: Bidirectional
      - name: s3fs-volume
        mountPath: /shared-buckets
        subPath: shared-buckets
        mountPropagation: Bidirectional
      - name: s3fs-volume
        mountPath: /triaged-jobs
        subPath: triaged-jobs
        mountPropagation: Bidirectional
    profileList:
    - display_name: Choose your environment and resources
      default: true
      profile_options:
        image:
          display_name: Environment
          dynamic_image_building:
            enabled: true
          unlisted_choice:
            enabled: true
            display_name: Custom image
            validation_regex: ^.+:.+$
            validation_message: Must be a publicly available docker image, of form <image-name>:<tag>
            kubespawner_override:
              image: '{value}'
          choices:
            01-modify-pangeo:
              display_name: Modified Pangeo Notebook
              description: Pangeo based notebook with a Python environment
              kubespawner_override:
                image: public.ecr.aws/nasa-veda/pangeo-notebook-veda-image:2025.12.30-v1
                init_containers:
                # this container uses nbgitpuller to mount https://github.com/NASA-IMPACT/veda-docs/ for user pods
                # image source: https://github.com/NASA-IMPACT/jupyterhub-gitpuller-init
                - name: jupyterhub-gitpuller-init
                  image: public.ecr.aws/nasa-veda/jupyterhub-gitpuller-init:97eb45f9d23b128aff810e45911857d5cffd05c2
                  env:
                  - name: TARGET_PATH
                    value: veda-docs
                  - name: SOURCE_REPO
                    value: https://github.com/NASA-IMPACT/veda-docs
                  volumeMounts:
                  - name: home
                    mountPath: /home/jovyan
                    subPath: '{escaped_username}'
                  securityContext:
                    runAsUser: 1000
                    runAsGroup: 1000
            02-rocker:
              display_name: Rocker Geospatial with RStudio
              description: R environment with many geospatial libraries pre-installed
              kubespawner_override:
                image: rocker/binder:4.4
                image_pull_policy: Always
                  # Launch RStudio after the user logs in
                default_url: /rstudio
                  # Ensures container working dir is homedir
                  # https://github.com/2i2c-org/infrastructure/issues/2559
                working_dir: /home/rstudio
            03-qgis:
              display_name: QGIS on Linux Desktop
              description: Linux desktop in the browser, with qgis installed
              kubespawner_override:
                  # Launch people directly into the Linux desktop when they start
                default_url: /desktop
                  # Built from https://github.com/2i2c-org/nasa-qgis-image
                image: quay.io/2i2c/nasa-qgis-image:d76118ea0c15
        resource_allocation:
          display_name: Resource Allocation
          choices:
            # [[[cog
            # from deployer.dev_commands.generate.resource_allocation.generate_choices import choices
            # choices(["r5.xlarge:5", "r5.4xlarge:2"], "proportional-memory-strategy", default=True)
            # ]]]
            mem_2_gb:
              display_name: ~2 GB RAM, ~0.2 CPUs
              description: Up to ~4 CPUs when available
              kubespawner_override:
                mem_guarantee: 1951419879
                mem_limit: 1951419879
                cpu_guarantee: 0.22815625
                cpu_limit: 3.6505
                node_selector:
                  node.kubernetes.io/instance-type: r5.xlarge
              default: true
            mem_4_gb:
              display_name: ~4 GB RAM, ~0.5 CPUs
              description: Up to ~4 CPUs when available
              kubespawner_override:
                mem_guarantee: 3902839759
                mem_limit: 3902839759
                cpu_guarantee: 0.4563125
                cpu_limit: 3.6505
                node_selector:
                  node.kubernetes.io/instance-type: r5.xlarge
            mem_7_gb:
              display_name: ~7 GB RAM, ~0.9 CPUs
              description: Up to ~4 CPUs when available
              kubespawner_override:
                mem_guarantee: 7805679519
                mem_limit: 7805679519
                cpu_guarantee: 0.912625
                cpu_limit: 3.6505
                node_selector:
                  node.kubernetes.io/instance-type: r5.xlarge
            mem_15_gb:
              display_name: ~15 GB RAM, ~1.8 CPUs
              description: Up to ~4 CPUs when available
              kubespawner_override:
                mem_guarantee: 15611359038
                mem_limit: 15611359038
                cpu_guarantee: 1.82525
                cpu_limit: 3.6505
                node_selector:
                  node.kubernetes.io/instance-type: r5.xlarge
            mem_29_gb:
              display_name: ~29 GB RAM, ~4 CPUs
              description: ~4 CPUs always available
              kubespawner_override:
                mem_guarantee: 31222718077
                mem_limit: 31222718077
                cpu_guarantee: 3.6505
                cpu_limit: 3.6505
                node_selector:
                  node.kubernetes.io/instance-type: r5.xlarge
            mem_60_gb:
              display_name: ~60 GB RAM, ~8 CPUs
              description: Up to ~15 CPUs when available
              kubespawner_override:
                mem_guarantee: 64020707016
                mem_limit: 64020707016
                cpu_guarantee: 7.69055
                cpu_limit: 15.3811
                node_selector:
                  node.kubernetes.io/instance-type: r5.4xlarge
            mem_119_gb:
              display_name: ~119 GB RAM, ~15 CPUs
              description: ~15 CPUs always available
              kubespawner_override:
                mem_guarantee: 128041414033
                mem_limit: 128041414033
                cpu_guarantee: 15.3811
                cpu_limit: 15.3811
                node_selector:
                  node.kubernetes.io/instance-type: r5.4xlarge
            # [[[end]]]
    - display_name: NVIDIA Tesla T4, ~16 GB, ~4 CPUs
      description: Start a container on a dedicated node with a GPU
      slug: gpu
      allowed_groups:
      - 2i2c-org:hub-access-for-2i2c-staff
      - MAAP-Project:gpu
      profile_options:
        image:
          display_name: Environment
          dynamic_image_building:
            enabled: true
          unlisted_choice:
            enabled: true
            display_name: Custom image
            validation_regex: ^.+:.+$
            validation_message: Must be a publicly available docker image of form <image-name>:<tag>
            kubespawner_override:
              image: '{value}'
          choices:
            pytorch:
              display_name: Pangeo PyTorch ML Notebook
              default: false
              slug: pytorch
              kubespawner_override:
                image: quay.io/pangeo/pytorch-notebook:2024.11.11
            tensorflow2:
              display_name: Pangeo Tensorflow2 ML Notebook
              default: true
              slug: tensorflow2
              kubespawner_override:
                image: quay.io/pangeo/ml-notebook:2024.11.11
      kubespawner_override:
        environment:
          NVIDIA_DRIVER_CAPABILITIES: compute,utility
        mem_limit:
        mem_guarantee: 14G
        node_selector:
          node.kubernetes.io/instance-type: g4dn.xlarge
        extra_resource_limits:
          nvidia.com/gpu: '1'

  scheduling:
    userScheduler:
      enabled: true

binderhub-service:
  enabled: true
  networkPolicy:
    enabled: true
  # Explicitly specify what nodes we want for our builds
  # Otherwise we may scale up a larger node than needed
  dockerApi:
    nodeSelector:
      node.kubernetes.io/instance-type: r5.xlarge
  config:
    KubernetesBuildExecutor:
      node_selector:
        node.kubernetes.io/instance-type: r5.xlarge
    DockerRegistry:
      url: &url https://quay.io
      username: &username imagebuilding-non-gcp-hubs+image_builder
  buildPodsRegistryCredentials:
    server: *url
    username: *username
