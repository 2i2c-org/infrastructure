basehub:
  nfs:
    pv:
      # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
      mountOptions:
        - rsize=1048576
        - wsize=1048576
        - timeo=600
        - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
        - retrans=2
        - noresvport
      serverIP: fs-029a8973da2b1ef5f.efs.us-west-2.amazonaws.com
      baseShareName: /
  jupyterhub:
    custom:
      2i2c:
        add_staff_user_ids_to_admin_users: true
        add_staff_user_ids_of_type: "github"
      homepage:
        templateVars:
          org:
            name: "The Visualization, Exploration, and Data Analysis (VEDA) Project"
            logo_url: https://visex.netlify.app/graphics/nasa-veda-logo-pos.svg
            url: https://www.earthdata.nasa.gov/esds/veda
          designed_by:
            name: "2i2c"
            url: https://2i2c.org
          operated_by:
            name: "2i2c"
            url: https://2i2c.org
          funded_by:
            name: "NASA"
            url: https://www.earthdata.nasa.gov/esds
    hub:
      allowNamedServers: true
      config:
        Authenticator:
          admin_users:
            - abarciauskas-bgse
            - freitagb
            - j08lue
            - rezuma
            - ranchodeluxe
            - jsignell
        JupyterHub:
          authenticator_class: github
        GitHubOAuthenticator:
          allowed_organizations:
            - veda-analytics-access:all-users
            - 2i2c-org:hub-access-for-2i2c-staff
            - CYGNSS-VEDA:cygnss-iwg
          scope:
            - read:org
    prePuller:
      # NOTE: To have prePuller enabled makes sense as long as we stick with a
      #       single image option configured. Disable prePuller if the
      #       configurator is used or a list of images is chosen from in the
      #       profileList. Otherwise its likely to delay delay startup of a user
      #       pod wanting to getting to wait for the pulling of another image.
      #
      continuous:
        enabled: true
      hook:
        enabled: true
    singleuser:
      defaultUrl: /lab
      image:
        name: public.ecr.aws/nasa-veda/nasa-veda-singleuser
        # Based off pangeo/pangeo-notebook:2023.07.05 which uses JupyterLab <4, so jupyterlab-git and dask-dashboard work
        tag: "b807c7efa97c8df9ca38779f7e59d09f889fde9299b0d19de80389cf6b064f90"
      profileList:
        # NOTE: About node sharing
        #
        #       CPU/Memory requests/limits are actively considered still. This
        #       profile list is setup to involve node sharing as considered in
        #       https://github.com/2i2c-org/infrastructure/issues/2121.
        #
        #       - Memory requests are different from the description, based on:
        #         whats found to remain allocate in k8s, subtracting 1GiB
        #         overhead for misc system pods, and transitioning from GB in
        #         description to GiB in mem_guarantee.
        #       - CPU requests are lower than the description, with a factor of
        #         10%.
        #
        - display_name: "Small: up to 4 CPU / 32 GB RAM"
          description: &profile_list_description "Start a container with at least a chosen share of capacity on a node of this type"
          slug: small
          default: true
          profile_options:
            requests:
              # NOTE: Node share choices are in active development, see comment
              #       next to profileList: above.
              display_name: Node share
              choices:
                mem_1:
                  default: true
                  display_name: ~1 GB, ~0.125 CPU
                  kubespawner_override:
                    mem_guarantee: 0.904G
                    cpu_guarantee: 0.013
                mem_2:
                  display_name: ~2 GB, ~0.25 CPU
                  kubespawner_override:
                    mem_guarantee: 1.809G
                    cpu_guarantee: 0.025
                mem_4:
                  display_name: ~4 GB, ~0.5 CPU
                  kubespawner_override:
                    mem_guarantee: 3.617G
                    cpu_guarantee: 0.05
                mem_8:
                  display_name: ~8 GB, ~1.0 CPU
                  kubespawner_override:
                    mem_guarantee: 7.234G
                    cpu_guarantee: 0.1
                mem_16:
                  display_name: ~16 GB, ~2.0 CPU
                  kubespawner_override:
                    mem_guarantee: 14.469G
                    cpu_guarantee: 0.2
                mem_32:
                  display_name: ~32 GB, ~4.0 CPU
                  kubespawner_override:
                    mem_guarantee: 28.937G
                    cpu_guarantee: 0.4
          kubespawner_override:
            cpu_limit: null
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: r5.xlarge
        - display_name: "Medium: up to 16 CPU / 128 GB RAM"
          description: *profile_list_description
          slug: medium
          profile_options:
            requests:
              # NOTE: Node share choices are in active development, see comment
              #       next to profileList: above.
              display_name: Node share
              choices:
                mem_1:
                  display_name: ~1 GB, ~0.125 CPU
                  kubespawner_override:
                    mem_guarantee: 0.942G
                    cpu_guarantee: 0.013
                mem_2:
                  display_name: ~2 GB, ~0.25 CPU
                  kubespawner_override:
                    mem_guarantee: 1.883G
                    cpu_guarantee: 0.025
                mem_4:
                  default: true
                  display_name: ~4 GB, ~0.5 CPU
                  kubespawner_override:
                    mem_guarantee: 3.766G
                    cpu_guarantee: 0.05
                mem_8:
                  display_name: ~8 GB, ~1.0 CPU
                  kubespawner_override:
                    mem_guarantee: 7.532G
                    cpu_guarantee: 0.1
                mem_16:
                  display_name: ~16 GB, ~2.0 CPU
                  kubespawner_override:
                    mem_guarantee: 15.064G
                    cpu_guarantee: 0.2
                mem_32:
                  display_name: ~32 GB, ~4.0 CPU
                  kubespawner_override:
                    mem_guarantee: 30.128G
                    cpu_guarantee: 0.4
                mem_64:
                  display_name: ~64 GB, ~8.0 CPU
                  kubespawner_override:
                    mem_guarantee: 60.257G
                    cpu_guarantee: 0.8
                mem_128:
                  display_name: ~128 GB, ~16.0 CPU
                  kubespawner_override:
                    mem_guarantee: 120.513G
                    cpu_guarantee: 1.6
          kubespawner_override:
            cpu_limit: null
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: r5.4xlarge
        - display_name: "Large: up to 64 CPU / 512 GB RAM"
          description: *profile_list_description
          slug: large
          profile_options:
            requests:
              # NOTE: Node share choices are in active development, see comment
              #       next to profileList: above.
              display_name: Node share
              choices:
                mem_4:
                  display_name: ~4 GB, ~0.5 CPU
                  kubespawner_override:
                    mem_guarantee: 3.821G
                    cpu_guarantee: 0.05
                mem_8:
                  display_name: ~8 GB, ~1.0 CPU
                  kubespawner_override:
                    mem_guarantee: 7.643G
                    cpu_guarantee: 0.1
                mem_16:
                  default: true
                  display_name: ~16 GB, ~2.0 CPU
                  kubespawner_override:
                    mem_guarantee: 15.285G
                    cpu_guarantee: 0.2
                mem_32:
                  display_name: ~32 GB, ~4.0 CPU
                  kubespawner_override:
                    mem_guarantee: 30.571G
                    cpu_guarantee: 0.4
                mem_64:
                  display_name: ~64 GB, ~8.0 CPU
                  kubespawner_override:
                    mem_guarantee: 61.141G
                    cpu_guarantee: 0.8
                mem_128:
                  display_name: ~128 GB, ~16.0 CPU
                  kubespawner_override:
                    mem_guarantee: 122.282G
                    cpu_guarantee: 1.6
                mem_256:
                  display_name: ~256 GB, ~32.0 CPU
                  kubespawner_override:
                    mem_guarantee: 244.565G
                    cpu_guarantee: 3.2
                mem_512:
                  display_name: ~512 GB, ~64.0 CPU
                  kubespawner_override:
                    mem_guarantee: 489.13G
                    cpu_guarantee: 6.4
          kubespawner_override:
            cpu_limit: null
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: r5.16xlarge
    scheduling:
      userScheduler:
        enabled: true
