# This eksctl configuration file represents the cluster and node groups for use
# by the cluster.
# ref: https://eksctl.io/usage/schema/
#
# Cluster operations:
# ref: https://eksctl.io/usage/cluster-upgrade/
#
#   create:   eksctl create cluster --config-file=eksctl-cluster-config.yaml --set-kubeconfig-context
#   upgrade:  eksctl upgrade cluster --config-file=eksctl-cluster-config.yaml
#   delete:   eksctl delete cluster --config-file=eksctl-cluster-config.yaml
#
# Node group operations:
# ref: https://eksctl.io/usage/managing-nodegroups/
#
#   eksctl get nodegroups --cluster jmte
#
#   eksctl delete nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*" --approve
#   eksctl create nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*"
#   eksctl delete nodegroup --cluster jmte --name core-a
#   eksctl create nodegroup --cluster jmte --name core-a
#
#   eksctl delete nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*" --approve && eksctl create nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*"
#
# Attribution: this was based on @yuvipanda's work in 2i2c! <3
# ref: https://github.com/2i2c-org/pangeo-hubs/blob/8e552bc198d8339efe8c003cb847849255e8f8ed/aws/eksctl-config.yaml
#



apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: jmte
  # region:
  #   The region was chosen to to us-west-2 (Oregon) to be close to a CMIP-6
  #   dataset.
  #
  region: us-west-2
  version: "1.19"
  tags:
    2i2c.org/project: jmte

# availabilityZones:
#   For the EKS control plane, arbitrary chosen but made explicit to ensure we
#   can locate the node pool on an AZ where the EKS control plane exist as
#   required.
#
availabilityZones: [us-west-2d, us-west-2b, us-west-2a]



# This section will create additional k8s ServiceAccount's that are coupled with
# AWS Role's. By declaring pods to use them, you can grant these pods the
# associated permissions. For this deployment, we create a k8s ServiceAccount
# with Full S3 credentials which we then also declare user pods and dask worker
# pods will make use of.
#
iam:
  withOIDC: true        # https://eksctl.io/usage/security/#withoidc
  # serviceAccounts like nodeGroups etc can be managed directly with eksctl, for
  # more information, see: https://eksctl.io/usage/iamserviceaccounts/
  #
  #   eksctl create iamserviceaccount --config-file=eksctl-cluster-config.yaml
  #
  serviceAccounts:
    - metadata:
        name: s3-full-access
        namespace: prod
        labels:
          aws-usage: application
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
    - metadata:
        name: s3-full-access
        namespace: staging
        labels:
          aws-usage: application
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonS3FullAccess



# Choose the type of node group?
# - nodeGroups cannot be updated but must be recreated on changes:
#   https://eksctl.io/usage/managing-nodegroups/#nodegroup-immutability
# - managedNodeGroups cannot scale to zero:
#   https://github.com/aws/containers-roadmap/issues/724
#
# Choosing instance type?
# - Maximum pods: https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt
# - Node specs:   https://aws.amazon.com/ec2/instance-types/
# - Cost:         https://ec2pricing.net/
#
# Management advice:
# - Always use a suffix for node group names that you can replace with something
#   to create a new node group and delete the old. You will run into issues if
#   you name it "core" and "core-a" instead of "core-a" and "core-b", such as
#   when deleting "core" you end up draining both node groups.
#
nodeGroups:
  - name: core-a
    availabilityZones: [us-west-2d]   # aws ec2 describe-availability-zones --region <region-name>
    instanceType: m5.large   # 28 pods, 2 cpu, 8 GB
    minSize: 0
    maxSize: 2
    desiredCapacity: 1
    volumeSize: 80
    labels:
      hub.jupyter.org/node-purpose: core
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: core
    iam:
      withAddonPolicies:
        autoScaler: true
        efs: true

  # 57 pods, 4 cpu, 16 GB (Intel, 10 GBits network)
  - name: user-a-4
    availabilityZones: &user-availabilityZones [us-west-2d]
    instanceType: &user-instanceType m5.xlarge
    minSize: &user-minSize 0
    maxSize: &user-maxSize 4
    desiredCapacity: &user-desiredCapacity 0
    volumeSize: &user-volumeSize 80
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "4"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "4"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: &user-iam
      withAddonPolicies:
        autoScaler: true
        efs: true

  # 233 pods, 16 cpu, 64 GB (Intel, 10 GBits network)
  - name: user-a-16
    availabilityZones: *user-availabilityZones
    instanceType: m5.4xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "16"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "16"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: *user-iam

  # 736 pods, 64 cpu, 256 GB (Intel, 20 GBits network)
  - name: user-a-64
    availabilityZones: *user-availabilityZones
    instanceType: m5.16xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "64"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "64"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: *user-iam



  # Worker node pools using cheaper spot instances that are temporary.
  #
  #   References:
  #   - About spotAllocationStrategy: https://aws.amazon.com/blogs/compute/introducing-the-capacity-optimized-allocation-strategy-for-amazon-ec2-spot-instances/
  #   - About instancesDistribution:  https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-autoscaling-autoscalinggroup-instancesdistribution.html
  #
  # Note: instance types with different capacity (CPU/Memory) must have
  #       different node pools for the cluster autoscaler to work properly.
  #
  #   "Due to the Cluster Autoscaler’s limitations (more on that in the next
  #   section) on which Instance type to expand, it’s important to choose
  #   instances of the same size (vCPU and memory) for each InstanceGroup."
  #
  #   ref: https://medium.com/riskified-technology/run-kubernetes-on-aws-ec2-spot-instances-with-zero-downtime-f7327a95dea
  #
  # Note: use of YAML merge below (<<) would be great, but it is not supported
  #       and was just part of YAML 1.1 but not 1.0 or 1.2.
  #
  - name: worker-a-4
    availabilityZones: &worker-availabilityZones [us-west-2d, us-west-2b, us-west-2a]
    minSize: &worker-minSize 0
    maxSize: &worker-maxSize 8
    desiredCapacity: &worker-desiredCapacity 0
    volumeSize: &worker-volumeSize 80
    labels:
      k8s.dask.org/node-purpose: worker
      2i2c.org/node-cpu: "4"
    taints:
      k8s.dask.org_dedicated: worker:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.dask.org/node-purpose: worker
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "4"
      k8s.io/cluster-autoscaler/node-template/taint/k8s.dask.org_dedicated: worker:NoSchedule
    iam: &worker-iam
      withAddonPolicies:
        autoScaler: true
        efs: true
    # Spot instance specific configuration
    instancesDistribution:
      instanceTypes:
        - m5a.xlarge      # 57 pods, 4 cpu, 16 GB (AMD,   10 GBits network,  100% cost)
        - m5.xlarge       # 57 pods, 4 cpu, 16 GB (Intel, 10 GBits network, ~112% cost)
        # - m5n.xlarge    # 57 pods, 4 cpu, 16 GB (Intel, 25 GBits network, ~139% cost)
      onDemandBaseCapacity: &worker-onDemandBaseCapacity 0
      onDemandPercentageAboveBaseCapacity: &worker-onDemandPercentageAboveBaseCapacity 0
      spotAllocationStrategy: &worker-spotAllocationStrategy capacity-optimized

  - name: worker-a-16
    availabilityZones: *worker-availabilityZones
    minSize: *worker-minSize
    maxSize: *worker-maxSize
    desiredCapacity: *worker-desiredCapacity
    volumeSize: *worker-volumeSize
    labels:
      k8s.dask.org/node-purpose: worker
      2i2c.org/node-cpu: "16"
    taints:
      k8s.dask.org_dedicated: worker:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.dask.org/node-purpose: worker
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "16"
      k8s.io/cluster-autoscaler/node-template/taint/k8s.dask.org_dedicated: worker:NoSchedule
    iam: *worker-iam
    instancesDistribution:
      instanceTypes:
        - m5a.4xlarge     # 233 pods, 16 cpu, 64 GB (AMD,   10 GBits network,  100% cost)
        - m5.4xlarge      # 233 pods, 16 cpu, 64 GB (Intel, 10 GBits network, ~112% cost)
        # - m5n.4xlarge   # 233 pods, 16 cpu, 64 GB (Intel, 25 GBits network, ~139% cost)
      onDemandBaseCapacity: *worker-onDemandBaseCapacity
      onDemandPercentageAboveBaseCapacity: *worker-onDemandPercentageAboveBaseCapacity
      spotAllocationStrategy: *worker-spotAllocationStrategy

  - name: worker-a-64
    availabilityZones: *worker-availabilityZones
    minSize: *worker-minSize
    maxSize: *worker-maxSize
    desiredCapacity: *worker-desiredCapacity
    volumeSize: *worker-volumeSize
    labels:
      k8s.dask.org/node-purpose: worker
      2i2c.org/node-cpu: "64"
    taints:
      k8s.dask.org_dedicated: worker:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.dask.org/node-purpose: worker
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "64"
      k8s.io/cluster-autoscaler/node-template/taint/k8s.dask.org_dedicated: worker:NoSchedule
    iam: *worker-iam
    instancesDistribution:
      instanceTypes:
        - m5a.16xlarge     # 736 pods, 64 cpu, 256 GB (AMD,   12 GBits network,  100% cost)
        - m5.16xlarge      # 736 pods, 64 cpu, 256 GB (Intel, 20 GBits network, ~112% cost)
        # - m5n.16xlarge   # 736 pods, 64 cpu, 256 GB (Intel, 75 GBits network, ~139% cost)
      onDemandBaseCapacity: *worker-onDemandBaseCapacity
      onDemandPercentageAboveBaseCapacity: *worker-onDemandPercentageAboveBaseCapacity
      spotAllocationStrategy: *worker-spotAllocationStrategy
