# This eksctl configuration file represents the cluster and node groups for use
# by the cluster.
# ref: https://eksctl.io/usage/schema/
#
# Cluster operations:
# ref: https://eksctl.io/usage/cluster-upgrade/
#
#   create:   eksctl create cluster --config-file=eksctl-cluster-config.yaml --set-kubeconfig-context
#   upgrade:  eksctl upgrade cluster --config-file=eksctl-cluster-config.yaml
#   delete:   eksctl delete cluster --config-file=eksctl-cluster-config.yaml
#
# Node group operations:
# ref: https://eksctl.io/usage/managing-nodegroups/
#
#   eksctl get nodegroups --cluster jmte
#
#   eksctl delete nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*" --approve
#   eksctl create nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*"
#   eksctl delete nodegroup --cluster jmte --name core-a
#   eksctl create nodegroup --cluster jmte --name core-a
#
#   eksctl delete nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*" --approve && eksctl create nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*"
#
# Attribution: this was based on @yuvipanda's work in 2i2c! <3
# ref: https://github.com/2i2c-org/pangeo-hubs/blob/8e552bc198d8339efe8c003cb847849255e8f8ed/aws/eksctl-config.yaml
#



apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: jmte
  # region:
  #   The region was chosen to to us-west-2 (Oregon) to be close to a CMIP-6
  #   dataset.
  #
  region: us-west-2
  version: "1.19"
  tags:
    2i2c.org/project: jmte

# availabilityZones:
#   For the EKS control plane, arbitrary chosen but made explicit to ensure we
#   can locate the node pool on an AZ where the EKS control plane exist as
#   required.
#
availabilityZones: [us-west-2d, us-west-2b, us-west-2a]



# This section will create additional k8s ServiceAccount's that are coupled with
# AWS Role's. By declaring pods to use them, you can grant these pods the
# associated permissions. For this deployment, we create a k8s ServiceAccount
# with Full S3 credentials which we then also declare user pods and dask worker
# pods will make use of.
#
iam:
  withOIDC: true        # https://eksctl.io/usage/security/#withoidc
  # serviceAccounts like nodeGroups etc can be managed directly with eksctl, for
  # more information, see: https://eksctl.io/usage/iamserviceaccounts/
  #
  #   eksctl create iamserviceaccount --config-file=eksctl-cluster-config.yaml
  #
  serviceAccounts:
    - metadata:
        name: s3-full-access
        namespace: prod
        labels:
          aws-usage: application
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
    - metadata:
        name: s3-full-access
        namespace: staging
        labels:
          aws-usage: application
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonS3FullAccess



# Choose the type of node group?
# - nodeGroups cannot be updated but must be recreated on changes:
#   https://eksctl.io/usage/managing-nodegroups/#nodegroup-immutability
# - managedNodeGroups cannot scale to zero:
#   https://github.com/aws/containers-roadmap/issues/724
#
# Choosing instance type?
# - Maximum pods: https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt
# - Node specs:   https://aws.amazon.com/ec2/instance-types/
# - Cost:         https://ec2pricing.net/
# - Instance availability in zone:
#   - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-discovery.html
#   - aws ec2 describe-instance-type-offerings --location-type "availability-zone" --filters Name=location,Values=us-west-2d --region us-west-2 | grep g4dn
#
# Management advice:
# - Always use a suffix for node group names that you can replace with something
#   to create a new node group and delete the old. You will run into issues if
#   you name it "core" and "core-a" instead of "core-a" and "core-b", such as
#   when deleting "core" you end up draining both node groups.
#
# Common gotcha:
# - AWS quotas may stop you from scaling up. The symptoms for this will be that
#   you observe that a scale up request has been made by the cluster-autoscaler
#   but no new node ever comes online. If that happens, you should visit
#   https://<your-region-here>.console.aws.amazon.com/ec2autoscaling/home, click
#   on the auto scaling group (ASG), then go to the activity tab and verify that
#   you have run into a quota issue. Following that, you make a request to AWS using provided link: https://aws.amazon.com/contact-us/ec2-request
#
nodeGroups:
  - name: core-a
    availabilityZones: [us-west-2d]   # aws ec2 describe-availability-zones --region <region-name>
    instanceType: m5.large   # 28 pods, 2 cpu, 8 GB
    minSize: 0
    maxSize: 2
    desiredCapacity: 1
    volumeSize: 250
    labels:
      hub.jupyter.org/node-purpose: core
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: core
    iam:
      withAddonPolicies:
        autoScaler: true
        efs: true

  # 57 pods, 4 cpu, 16 GB (Intel, 10 GBits network)
  - name: user-a-4
    availabilityZones: &user-availabilityZones [us-west-2d]
    instanceType: &user-instanceType m5.xlarge
    minSize: &user-minSize 0
    maxSize: &user-maxSize 4
    desiredCapacity: &user-desiredCapacity 0
    volumeSize: &user-volumeSize 500
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "4"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "4"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: &user-iam
      withAddonPolicies:
        autoScaler: true
        efs: true

  # 233 pods, 16 cpu, 64 GB (Intel, 10 GBits network)
  - name: user-a-16
    availabilityZones: *user-availabilityZones
    instanceType: m5.4xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "16"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "16"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: *user-iam

  # 736 pods, 64 cpu, 256 GB (Intel, 20 GBits network)
  - name: user-a-64
    availabilityZones: *user-availabilityZones
    instanceType: m5.16xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "64"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "64"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: *user-iam

  # High memory nodes.
  #
  # The local SSD storage available on these high memory nodes is not exposed by
  # default in some easy way but is rather quite tricky to make use of in k8s.
  # To make that happen, one needs to have a daemonset installed to prepare the
  # nodes that has local storage to make it exposed.
  #
  # A discussion on how this is done is made in
  # https://github.com/pangeo-data/jupyter-earth/issues/88.
  #
  # To figure out what availability zones we could use, I used the command below
  # and took the union of that output with the zones of the EKS control plane
  # configured in the root level of this config. I'm not sure if I could use
  # nodes in other availability zones.
  #
  #   aws ec2 describe-instance-type-offerings \
  #     --region us-west-2 \
  #     --filter Name=instance-type,Values=x1.16xlarge \
  #     --location-type=availability-zone
  #
  # 233 pods, 64 cpu, 976 GB, 1,920 GB local SSD storage, (Intel, 10 GBits
  # network)
  - name: user-highmem-a-64
    availabilityZones: &user-highmem-availabilityZones [us-west-2b, us-west-2a]
    instanceType: x1.16xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-highmem-cpu: "64"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-highmem-cpu: "64"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: &user-iam
      withAddonPolicies:
        autoScaler: true
        efs: true

  # GPU Nodes.
  #
  # g4dn was chosen based on input from Shane in this comment
  # https://github.com/pangeo-data/jupyter-earth/issues/77#issuecomment-910864707.
  #
  # For reference of the available choices, see
  # https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing.
  #
  # For reference on the GPU device plugin that needs to be installed, but is
  # installed automatically by eksctl, see:
  # https://eksctl.io/usage/gpu-support/#gpu-support
  #
  # The machine nodes AMI (what is installed when it starts) for GPU nodes may
  # require you to subscribe to the AMI and accept some license. For more info,
  # see:
  # https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-eks-setup.html#deep-learning-containers-eks-setup-licensing
  #
  # Note that we opted for us-west-2b here because g4dn machines were not
  # available in us-west-2d.
  #
  # 57 pods, 4 cpu, 16 GB (Intel, 25 GBits network), 1 T4 Tensor Core GPU
  - name: user-gpu-a-4
    availabilityZones: &user-gpu-availabilityZones [us-west-2b]
    instanceType: g4dn.xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "4"
      2i2c.org/node-gpu: "1"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "4"
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-gpu: "1"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
      k8s.io/cluster-autoscaler/node-template/taint/nvidia.com/gpu: NoSchedule
    iam: &user-iam
      withAddonPolicies:
        autoScaler: true
        efs: true

  # 233 pods, 16 cpu, 64 GB (Intel, 25 GBits network), 1 T4 Tensor Core GPU
  - name: user-gpu-a-16
    availabilityZones: *user-gpu-availabilityZones
    instanceType: g4dn.4xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "16"
      2i2c.org/node-gpu: "1"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "16"
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-gpu: "1"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
      k8s.io/cluster-autoscaler/node-template/taint/nvidia.com/gpu: NoSchedule
    iam: &user-iam
      withAddonPolicies:
        autoScaler: true
        efs: true



  # Worker node pools using cheaper spot instances that are temporary.
  #
  #   References:
  #   - About spotAllocationStrategy: https://aws.amazon.com/blogs/compute/introducing-the-capacity-optimized-allocation-strategy-for-amazon-ec2-spot-instances/
  #   - About instancesDistribution:  https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-autoscaling-autoscalinggroup-instancesdistribution.html
  #
  # Note: instance types with different capacity (CPU/Memory) must have
  #       different node pools for the cluster autoscaler to work properly.
  #
  #   "Due to the Cluster Autoscaler’s limitations (more on that in the next
  #   section) on which Instance type to expand, it’s important to choose
  #   instances of the same size (vCPU and memory) for each InstanceGroup."
  #
  #   ref: https://medium.com/riskified-technology/run-kubernetes-on-aws-ec2-spot-instances-with-zero-downtime-f7327a95dea
  #
  # Note: use of YAML merge below (<<) would be great, but it is not supported
  #       and was just part of YAML 1.1 but not 1.0 or 1.2.
  #
  - name: worker-a-4
    availabilityZones: &worker-availabilityZones [us-west-2d, us-west-2b, us-west-2a]
    minSize: &worker-minSize 0
    maxSize: &worker-maxSize 8
    desiredCapacity: &worker-desiredCapacity 0
    volumeSize: &worker-volumeSize 500
    labels:
      k8s.dask.org/node-purpose: worker
      2i2c.org/node-cpu: "4"
    taints:
      k8s.dask.org_dedicated: worker:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.dask.org/node-purpose: worker
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "4"
      k8s.io/cluster-autoscaler/node-template/taint/k8s.dask.org_dedicated: worker:NoSchedule
    iam: &worker-iam
      withAddonPolicies:
        autoScaler: true
        efs: true
    # Spot instance specific configuration
    instancesDistribution:
      instanceTypes:
        - m5a.xlarge      # 57 pods, 4 cpu, 16 GB (AMD,   10 GBits network,  100% cost)
        - m5.xlarge       # 57 pods, 4 cpu, 16 GB (Intel, 10 GBits network, ~112% cost)
        # - m5n.xlarge    # 57 pods, 4 cpu, 16 GB (Intel, 25 GBits network, ~139% cost)
      onDemandBaseCapacity: &worker-onDemandBaseCapacity 0
      onDemandPercentageAboveBaseCapacity: &worker-onDemandPercentageAboveBaseCapacity 0
      spotAllocationStrategy: &worker-spotAllocationStrategy capacity-optimized

  - name: worker-a-16
    availabilityZones: *worker-availabilityZones
    minSize: *worker-minSize
    maxSize: *worker-maxSize
    desiredCapacity: *worker-desiredCapacity
    volumeSize: *worker-volumeSize
    labels:
      k8s.dask.org/node-purpose: worker
      2i2c.org/node-cpu: "16"
    taints:
      k8s.dask.org_dedicated: worker:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.dask.org/node-purpose: worker
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "16"
      k8s.io/cluster-autoscaler/node-template/taint/k8s.dask.org_dedicated: worker:NoSchedule
    iam: *worker-iam
    instancesDistribution:
      instanceTypes:
        - m5a.4xlarge     # 233 pods, 16 cpu, 64 GB (AMD,   10 GBits network,  100% cost)
        - m5.4xlarge      # 233 pods, 16 cpu, 64 GB (Intel, 10 GBits network, ~112% cost)
        # - m5n.4xlarge   # 233 pods, 16 cpu, 64 GB (Intel, 25 GBits network, ~139% cost)
      onDemandBaseCapacity: *worker-onDemandBaseCapacity
      onDemandPercentageAboveBaseCapacity: *worker-onDemandPercentageAboveBaseCapacity
      spotAllocationStrategy: *worker-spotAllocationStrategy

  - name: worker-a-64
    availabilityZones: *worker-availabilityZones
    minSize: *worker-minSize
    maxSize: *worker-maxSize
    desiredCapacity: *worker-desiredCapacity
    volumeSize: *worker-volumeSize
    labels:
      k8s.dask.org/node-purpose: worker
      2i2c.org/node-cpu: "64"
    taints:
      k8s.dask.org_dedicated: worker:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.dask.org/node-purpose: worker
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "64"
      k8s.io/cluster-autoscaler/node-template/taint/k8s.dask.org_dedicated: worker:NoSchedule
    iam: *worker-iam
    instancesDistribution:
      instanceTypes:
        - m5a.16xlarge     # 736 pods, 64 cpu, 256 GB (AMD,   12 GBits network,  100% cost)
        - m5.16xlarge      # 736 pods, 64 cpu, 256 GB (Intel, 20 GBits network, ~112% cost)
        # - m5n.16xlarge   # 736 pods, 64 cpu, 256 GB (Intel, 75 GBits network, ~139% cost)
      onDemandBaseCapacity: *worker-onDemandBaseCapacity
      onDemandPercentageAboveBaseCapacity: *worker-onDemandPercentageAboveBaseCapacity
      spotAllocationStrategy: *worker-spotAllocationStrategy
