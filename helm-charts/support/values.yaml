prometheusAuth:
  enabled: false
  username: ""
  password: ""

cluster-autoscaler:
  enabled: false
ingress-nginx:
  controller:
    podLabels:
      hub.jupyter.org/network-access-proxy-http: "true"
prometheus:
  networkPolicy:
    enabled: true
  alertmanager:
    enabled: false
  nodeExporter:
    tolerations:
      - effect: NoSchedule
        # Deploy onto user nodes
        key: hub.jupyter.org_dedicated
        value: user
      - effect: NoSchedule
        # Deploy onto user nodes
        key: k8s.dask.org_dedicated
        value: worker
    updateStrategy:
      type: RollingUpdate
  pushgateway:
    enabled: false
  server:
    ingress:
      annotations:
        nginx.ingress.kubernetes.io/auth-type: basic
        nginx.ingress.kubernetes.io/auth-secret: prometheus-ingress-auth
        nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required'
        kubernetes.io/ingress.class: nginx
        cert-manager.io/cluster-issuer: letsencrypt-prod
    strategy:
      # We have a persistent disk attached, so the default (RollingUpdate)
      # can sometimes get 'stuck' and require pods to be manually deleted.
      type: Recreate
    resources:
      # Without this, prometheus can easily starve users
      requests:
        cpu: 0.2
        memory: 512Mi
      limits:
        cpu: 1
        memory: 2Gi
    labels:
      # For HTTP access to the hub, to scrape metrics
      hub.jupyter.org/network-access-hub: "true"
    persistentVolume:
      size: 100Gi
    retention: 90d
    service:
      type: ClusterIP

grafana:
  persistence:
    enabled: true
  deploymentStrategy:
    type: Recreate
  service:
    type: ClusterIP

  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: letsencrypt-prod

  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: prometheus
          orgId: 1
          type: prometheus
          url: http://support-prometheus-server
          access: proxy
          isDefault: true
          editable: false

# Values reference for nfs-server-provisioner
# https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner/blob/master/deploy/helm/values.yaml
nfs-server-provisioner:
  enabled: false
  storageClass:
    # Users will have to opt-in to using NFS-type storage under
    # singleuser.storage.dynamic.storageClass
    name: incluster-nfs
    create: true
    defaultClass: false
    mountOptions:
      - vers=4.2
      - soft
      - noatime
  persistence:
    enabled: true
    accessMode: ReadWriteOnce
    # Ideally, this *must* be set by each cluster's config. But there's no
    # easy way for us to force this.
    # This is the total size of the disk to contain *all* the home directories
    # of *all* the hubs on the cluster. Each hub gets a directory, with quotas
    # enforced by XFS. If the size here is lower than the sum of all the requests
    # for all the home directories PVCs on the cluster, hub deployment will
    # fail with an error. You have to increase the size here, do a deploy-support,
    # and try again.
    size: 100Gi
    # Selects for SSD vs HDD for base storage of the files.
    # On Google cloud, this is `standard-rwo` for HDD and `premium-rwo` for ssds.
    # Standard will do for most use cases.
    storageClass: standard-rwo

# Enable a daemonset to install nvidia device plugin to GPU nodes
nvidiaDevicePlugin:
  # For Azure-specific image, default to false
  azure:
    enabled: false

# A placeholder as global values that can be referenced from the same location
# of any chart should be possible to provide, but aren't necessarily provided or
# used.
global: {}
