basehub:
  nfs:
    enabled: true
    pv:
      enabled: true
      # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
      mountOptions:
        - rsize=1048576
        - wsize=1048576
        - timeo=600
        - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
        - retrans=2
        - noresvport
      serverIP: fs-01707b06.efs.us-west-2.amazonaws.com
      # This is different from rest of our hubs!
      baseShareName: /
  dask-gateway:
    enabled: true
    gateway:
      backend:
        scheduler:
          # IMPORTANT: We have experienced that the scheduler can fail with
          #            1GB memory limit. This was observed "stream closed"
          #            from the python client working against the
          #            Dask-Gateway created DaskCluster.
          #
          #            CommClosedError: in <TLS (closed) ConnectionPool.gather local=tls://192.168.40.210:54296 remote=gateway://traefik-prod-dask-gateway.prod:80/prod.b9600f678bb747c1a5f038b5bef3eb90>: Stream is closed
          #
          cores:
            request: 1
            limit: 64
          memory:
            request: 2G
            limit: 500G
  jupyterhub:
    custom:
      daskhubSetup:
        enabled: true
      2i2c:
        add_staff_user_ids_to_admin_users: true
        add_staff_user_ids_of_type: "github"
      jupyterhubConfigurator:
        enabled: false
      homepage:
        templateVars:
          org:
            name: Jupyter meets the Earth
            logo_url: https://pangeo-data.github.io/jupyter-earth/_static/jupyter-earth.png
            url: https://jupytearth.org
          designed_by:
            name: 2i2c
            url: https://2i2c.org
          operated_by:
            name: 2i2c
            url: https://2i2c.org
          funded_by:
            name: Jupyter meets the Earth
            url: https://jupytearth.org

    singleuser:
      cloudMetadata:
        blockWithIptables: false
      initContainers:
        # Need to explicitly set this up and copy what's in basehub/values.yaml
        # as we have an extra 'shared-public' directory here.
        - name: volume-mount-ownership-fix
          image: busybox:1.36.1
          command:
            - sh
            - -c
            - id && chown 1000:1000 /home/jovyan /home/jovyan/shared /home/jovyan/shared-public && ls -lhd /home/jovyan
          securityContext:
            runAsUser: 0
          volumeMounts:
            - name: home
              mountPath: /home/jovyan
              subPath: "{username}"
            - name: home
              mountPath: /home/jovyan/shared
              subPath: _shared
            - name: home
              mountPath: /home/jovyan/shared-public
              subPath: _shared_public

      # /dev/shm is mounted as a filesystem path, where writing to it means to
      # write to memory.
      #
      # How to: https://stackoverflow.com/questions/46085748/define-size-for-dev-shm-on-container-engine/46434614#46434614
      # Request for this by Ellie: https://fperezgroup.slack.com/archives/C020XCEFPEH/p1658168872788389
      storage:
        extraVolumes:
          - name: dev-shm
            emptyDir:
              medium: Memory
        extraVolumeMounts:
          - name: dev-shm
            mountPath: /dev/shm
          # FIXME: we override the list extraVolumeMounts which is also set in
          #        the the basehub chart, due to that, we need to add this here
          #        as well. An option is to add hub.extraConfig entries that
          #        append the kubespawner configuration to include these extra
          #        volume mounts.
          #
          - name: home
            mountPath: /home/jovyan/shared
            subPath: _shared
            readOnly: true
          - name: home
            mountPath: /home/jovyan/shared-public
            subPath: _shared_public

      # Increased as we have experienced a too slow image pull at least
      # once. Our pods can take ~6-7 minutes to start on a new node it
      # seems, so this gives us some margin.
      startTimeout: 1200

      extraEnv:
        GH_SCOPED_CREDS_APP_URL: https://github.com/apps/hub-jupytearth-org-github-integ
        GH_SCOPED_CREDS_CLIENT_ID: Iv1.a073b1649637af12
      profileList:
        - display_name: "16th of Medium: 0.25-4 CPU, 1-16 GB"
          default: True
          description: "A shared machine, the recommended option until you experience a limitation."
          kubespawner_override:
            cpu_guarantee: 0.225
            mem_guarantee: 0.875G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: m5.xlarge
          profile_options: &profile_options
            image:
              display_name: Image
              choices:
                jmte:
                  display_name: JMTE all-in-one image (deprecated)
                  default: true
                  slug: "jmte"
                  kubespawner_override:
                    image: "286354552638.dkr.ecr.us-west-2.amazonaws.com/jmte/user-env:9baee2d"
                tensorflow:
                  display_name: Pangeo Tensorflow ML Notebook
                  slug: "tensorflow"
                  kubespawner_override:
                    image: "quay.io/pangeo/ml-notebook:2023.05.18"
                pytorch:
                  display_name: Pangeo PyTorch ML Notebook
                  slug: "pytorch"
                  kubespawner_override:
                    image: "quay.io/pangeo/pytorch-notebook:2023.05.18"
                datascience:
                  display_name: Jupyter DockerStacks Julia Notebook
                  slug: "julia"
                  kubespawner_override:
                    # FIXME: use quay.io/ for tags after 2023-10-20
                    image: "jupyter/julia-notebook:2023-07-05"
        - display_name: "4th of Medium: 1-4 CPU, 4-16 GB"
          description: "A shared machine."
          profile_options: *profile_options
          kubespawner_override:
            cpu_guarantee: 0.875
            mem_guarantee: 3.5G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: m5.xlarge
        - display_name: "Medium: 4 CPU, 16 GB"
          description: "A dedicated machine for you."
          profile_options: *profile_options
          kubespawner_override:
            cpu_guarantee: 3.5
            mem_guarantee: 13G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: m5.xlarge
        - display_name: "Large: 16 CPU, 64 GB"
          description: "A dedicated machine for you."
          profile_options: *profile_options
          kubespawner_override:
            mem_guarantee: 56G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: m5.4xlarge
        - display_name: "Massive: 64 CPU, 256 GB"
          description: "A dedicated machine for you."
          profile_options: *profile_options
          kubespawner_override:
            mem_guarantee: 224G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: m5.16xlarge
        - display_name: "Massive high-memory: 64 CPU, 976 GB"
          description: "A dedicated machine for you."
          profile_options: *profile_options
          kubespawner_override:
            mem_guarantee: 900G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: x1.16xlarge
        - display_name: "Medium GPU: 4 CPU, 16 GB, 1 T4 Tensor Core GPU"
          description: "A dedicated machine for you with one GPU attached."
          profile_options: *profile_options
          kubespawner_override:
            cpu_guarantee: 3.5
            mem_guarantee: 13G
            mem_limit: null
            environment:
              NVIDIA_DRIVER_CAPABILITIES: compute,utility
            node_selector:
              node.kubernetes.io/instance-type: g4dn.xlarge
            extra_resource_limits:
              nvidia.com/gpu: "1"
        - display_name: "Large GPU: 16 CPU, 64 GB, 1 T4 Tensor Core GPU"
          description: "A dedicated machine for you with one GPU attached."
          profile_options: *profile_options
          kubespawner_override:
            mem_guarantee: 56G
            mem_limit: null
            environment:
              NVIDIA_DRIVER_CAPABILITIES: compute,utility
            node_selector:
              node.kubernetes.io/instance-type: g4dn.4xlarge
            extra_resource_limits:
              nvidia.com/gpu: "1"
        - display_name: "Massive GPU: 64 CPU, 256 GB, 1 T4 Tensor Core GPU"
          description: "A dedicated machine for you with one GPU attached."
          profile_options: *profile_options
          kubespawner_override:
            mem_guarantee: 200G
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: g4dn.16xlarge
            environment:
              NVIDIA_DRIVER_CAPABILITIES: compute,utility
            extra_resource_limits:
              nvidia.com/gpu: "1"

    hub:
      allowNamedServers: true
      config:
        JupyterHub:
          authenticator_class: cilogon
        CILogonOAuthenticator:
          allowed_idps:
            http://github.com/login/oauth/authorize:
              default: true
              username_derivation:
                username_claim: "preferred_username"
        OAuthenticator:
          # WARNING: Don't use allow_existing_users with config to allow an
          #          externally managed group of users, such as
          #          GitHubOAuthenticator.allowed_organizations, as it breaks a
          #          common expectations for an admin user.
          #
          #          The broken expectation is that removing a user from the
          #          externally managed group implies that the user won't have
          #          access any more. In practice the user will still have
          #          access if it had logged in once before, as it then exists
          #          in JupyterHub's database of users.
          #
          allow_existing_users: True
        Authenticator:
          # WARNING: Removing a user from admin_users or allowed_users doesn't
          #          revoke admin status or access.
          #
          #          OAuthenticator.allow_existing_users allows any user in the
          #          JupyterHub database of users able to login. This includes
          #          any previously logged in user or user previously listed in
          #          allowed_users or admin_users, as such users are added to
          #          JupyterHub's database on startup.
          #
          #          To revoke admin status or access for a user when
          #          allow_existing_users is enabled, first remove the user from
          #          admin_users or allowed_users, then deploy the change, and
          #          finally revoke the admin status or delete the user via the
          #          /hub/admin panel.
          #
          admin_users:
            # This is just listing a few of the users/admins, a lot of
            # users has been added manually, see:
            # https://github.com/pangeo-data/jupyter-earth/issues/53
            - abbyazari # Abby Azari
            - andersy005 # Anderson Banihirwe
            - consideratio # Erik Sundell
            - choldgraf # Chris Holdgraf
            - elliesch # Ellie Abrahams
            - EMscience # Edom Moges
            - espg # Shane Grigsby
            - facusapienza21 # Facundo Sapienza
            - fperez # Fernando PÃ©rez
            - kmpaul # Kevin Paul
            - lrennels # Lisa Rennels
            - mrsiegfried # Matthew Siegfried
            - tsnow03 # Tasha Snow
            - whyjz # Whyjay Zheng
            - yuvipanda # Yuvi Panda
            - jonathan-taylor # Jonathan Taylor

    scheduling:
      userScheduler:
        enabled: true
