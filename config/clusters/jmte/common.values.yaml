basehub:
  # Cloudformation: The EFS filesystem was created by cloudformation.
  #
  nfs:
    # enabled is adjusted by staging/prod values
    # enabled: true
    shareCreator:
      enabled: true
    pv:
      serverIP: fs-01707b06.efs.us-west-2.amazonaws.com
      # mountOptions from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
      mountOptions:
        - rsize=1048576
        - wsize=1048576
        - timeo=600
        - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
        - retrans=2
        - noresvport
      # baseShareName is required to be just "/" so that we can create
      # various sub folders in the filesystem that our PV to access the
      # NFS server can reference successfully as it isn't supported to
      # access a not yet existing folder. This creation is automated by
      # the nfs-share-creator resource part of the basehub Helm chart.
      baseShareName: /

  jupyterhub:
    custom:
      homepage:
        templateVars:
          org:
            name: Jupyter meets the Earth
            logo_url: https://pangeo-data.github.io/jupyter-earth/_static/jupyter-earth.png
            url: https://jupytearth.org
          designed_by:
            name: 2i2c
            url: https://2i2c.org
          operated_by:
            name: 2i2c
            url: https://2i2c.org
          funded_by:
            name: Jupyter meets the Earth
            url: https://jupytearth.org

    scheduling:
      userScheduler:
        # Revert basehubs default that relies on GKE's built in scheduler that
        # is optimized to pack pods into busy nodes. This is a AWS EKS based
        # hub without such default scheduler.
        enabled: true

    singleuser:
      # extraFiles ref: https://zero-to-jupyterhub.readthedocs.io/en/latest/resources/reference.html#singleuser-extrafiles
      extraFiles:
        jupyter_notebook_config.json:
          mountPath: /etc/jupyter/jupyter_notebook_config.json
          data:
            # Allow jupyterlab option to show hidden files in browser
            # https://github.com/berkeley-dsep-infra/datahub/issues/3160
            ContentsManager:
              allow_hidden: true
      # /dev/shm is mounted as a filesystem path, where writing to it means to
      # write to memory.
      #
      # How to: https://stackoverflow.com/questions/46085748/define-size-for-dev-shm-on-container-engine/46434614#46434614
      # Request for this by Ellie: https://fperezgroup.slack.com/archives/C020XCEFPEH/p1658168872788389
      #
      storage:
        extraVolumes:
          - name: dev-shm
            emptyDir:
              medium: Memory
        extraVolumeMounts:
          - name: dev-shm
            mountPath: /dev/shm
          # FIXME: we override the list extraVolumeMounts which is also set in
          #        the the basehub chart, due to that, we need to add this here
          #        as well. An option is to add hub.extraConfig entries that
          #        append the kubespawner configuration to include these extra
          #        volume mounts.
          #
          - name: home
            mountPath: /home/jovyan/shared
            subPath: _shared
            readOnly: true
          - name: home
            mountPath: /home/jovyan/shared-public
            subPath: _shared_public

      # Eksctl: The service account was created by eksctl.
      #
      # serviceAccountName is added to prod values
      # serviceAccountName: &user-sa s3-full-access

      # Increased as we have experienced a too slow image pull at least
      # once. Our pods can take ~6-7 minutes to start on a new node it
      # seems, so this gives us some margin.
      startTimeout: 1200

      extraEnv:
        # github-app-user-auth requires:
        # - Installed python package
        # - GH_SCOPED_CREDS_APP_URL env var set
        # - GITHUB_APP_CLIENT_ID env var set
        #
        # NOTE: an associated GitHub App has been created by Erik Sundell aka.
        #       @consideRatio and can be configured by him at:
        #       https://github.com/settings/apps/hub-jupytearth-org-github-integ
        #
        GH_SCOPED_CREDS_APP_URL: https://github.com/apps/hub-jupytearth-org-github-integ
        GH_SCOPED_CREDS_CLIENT_ID: Iv1.a073b1649637af12
        # NVIDIA_DRIVER_CAPABILITIES is added based on
        # https://github.com/2i2c-org/infrastructure/pull/1314 and
        # https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html#driver-capabilities
        # that indicate this is needed.
        #
        # It was added when `nvidia-smi` didn't report a CUDA driver version,
        # and no /usr/local/cuda folders were found in the container filesystem.
        #
        NVIDIA_DRIVER_CAPABILITIES: compute,utility

      image:
        # NOTE: We use the jupyterhub-configurator so this image/tag is not
        #       relevant. Visit its UI to configure the hub.
        #
        #       staging: https://staging.hub.jupytearth.org/services/configurator/
        #       prod:    https://hub.jupytearth.org/services/configurator/
        pullPolicy: Always
        name: 286354552638.dkr.ecr.us-west-2.amazonaws.com/jmte/user-env
        tag: "latest"

      profileList:
        - display_name: "16th of Medium: 0.25-4 CPU, 1-16 GB"
          default: True
          description: "A shared machine, the recommended option until you experience a limitation."
          kubespawner_override:
            cpu_guarantee: 0.225
            mem_guarantee: 0.875G
            mem_limit: null
            node_selector: { 2i2c.org/node-cpu: "4" }
            extra_resource_limits: {}
        - display_name: "4th of Medium: 1-4 CPU, 4-16 GB"
          description: "A shared machine."
          kubespawner_override:
            cpu_guarantee: 0.875
            mem_guarantee: 3.5G
            mem_limit: null
            node_selector: { 2i2c.org/node-cpu: "4" }
            extra_resource_limits: {}
        - display_name: "Medium: 4 CPU, 16 GB"
          description: "A dedicated machine for you."
          kubespawner_override:
            cpu_guarantee: 3.5
            mem_guarantee: 14G
            mem_limit: null
            node_selector: { 2i2c.org/node-cpu: "4" }
            extra_resource_limits: {}
        - display_name: "Large: 16 CPU, 64 GB"
          description: "A dedicated machine for you."
          kubespawner_override:
            mem_guarantee: 56G
            mem_limit: null
            node_selector: { 2i2c.org/node-cpu: "16" }
            extra_resource_limits: {}
        - display_name: "Massive: 64 CPU, 256 GB"
          description: "A dedicated machine for you."
          kubespawner_override:
            mem_guarantee: 224G
            mem_limit: null
            node_selector: { 2i2c.org/node-cpu: "64" }
            extra_resource_limits: {}
        - display_name: "Massive high-memory: 64 CPU, 976 GB"
          description: "A dedicated machine for you."
          kubespawner_override:
            mem_guarantee: 900G
            mem_limit: null
            node_selector: { 2i2c.org/node-highmem-cpu: "64" }
            extra_resource_limits: {}
        - display_name: "Medium GPU: 4 CPU, 16 GB, 1 T4 Tensor Core GPU"
          description: "A dedicated machine for you with one GPU attached."
          kubespawner_override:
            cpu_guarantee: 3.5
            mem_guarantee: 14G
            mem_limit: null
            node_selector: { 2i2c.org/node-cpu: "4", 2i2c.org/node-gpu: "1" }
            extra_resource_limits:
              nvidia.com/gpu: "1"
        - display_name: "Large GPU: 16 CPU, 64 GB, 1 T4 Tensor Core GPU"
          description: "A dedicated machine for you with one GPU attached."
          kubespawner_override:
            mem_guarantee: 56G
            mem_limit: null
            node_selector: { 2i2c.org/node-cpu: "16", 2i2c.org/node-gpu: "1" }
            extra_resource_limits:
              nvidia.com/gpu: "1"
        - display_name: "Massive GPU: 64 CPU, 256 GB, 1 T4 Tensor Core GPU"
          description: "A dedicated machine for you with one GPU attached."
          kubespawner_override:
            mem_guarantee: 224G
            mem_limit: null
            node_selector: { 2i2c.org/node-cpu: "64", 2i2c.org/node-gpu: "1" }
            extra_resource_limits:
              nvidia.com/gpu: "1"
        - display_name: "16th of Medium: 0.25-4 CPU, 1-16 GB - Test of latest image"
          description: "Helps us test an image before we make it the default"
          kubespawner_override:
            image: 286354552638.dkr.ecr.us-west-2.amazonaws.com/jmte/user-env:latest
            image_pull_policy: Always
            cpu_guarantee: 0.225
            mem_guarantee: 0.875G
            mem_limit: null
            node_selector: { 2i2c.org/node-cpu: "4" }
            extra_resource_limits: {}

    proxy:
      # proxy notes:
      #
      # - Revert basehubs overrides as we don't install ingress-nginx and
      #   cert-manager yet, and therefore should use
      #   service.type=LoadBalancer instead of service.type=ClusterIP.
      #   Along with this, we also make use of the autohttps system that
      #   requires us to configure an letsencrypt email.
      #
      https:
        enabled: true
        type: letsencrypt
        letsencrypt:
          contactEmail: erik@sundellopensource.se

      service:
        # Revert an unwanted basehub default
        type: LoadBalancer
        annotations:
          service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "3600"

        # jupyterhub-ssh/sftp integration part 1/3:
        #
        # We must accept traffic to the k8s Service (proxy-public) receiving traffic
        # from the internet. Port 22 is typically used for both SSH and SFTP, but we
        # can't use the same port for both so we use 2222 for SFTP in this example.
        #
        extraPorts:
          - name: ssh
            port: 22
            targetPort: ssh
          - name: sftp
            port: 2222
            targetPort: sftp
      traefik:
        # jupyterhub-ssh/sftp integration part 2/3:
        #
        # We must accept traffic arriving to the autohttps pod (traefik) from the
        # proxy-public service. Expose a port and update the NetworkPolicy
        # to tolerate incoming (ingress) traffic on the exposed port.
        #
        extraPorts:
          - name: ssh
            containerPort: 8022
          - name: sftp
            containerPort: 2222
        networkPolicy:
          allowedIngressPorts: [http, https, ssh, sftp]
        # jupyterhub-ssh/sftp integration part 3/3:
        #
        # extraStaticConfig is adjusted by staging/prod values
        # extraDynamicConfig is adjusted by staging/prod values

    hub:
      config:
        Authenticator:
          allowed_users: &users
            # This is just listing a few of the users/admins, a lot of
            # users has been added manually, see:
            # https://github.com/pangeo-data/jupyter-earth/issues/53
            - abbyazari # Abby Azari
            - andersy005 # Anderson Banihirwe
            - consideratio # Erik Sundell
            - choldgraf # Chris Holdgraf
            - elliesch # Ellie Abrahams
            - EMscience # Edom Moges
            - espg # Shane Grigsby
            - facusapienza21 # Facundo Sapienza
            - fperez # Fernando Pérez
            - kmpaul # Kevin Paul
            - lrennels # Lisa Rennels
            - mrsiegfried # Matthew Siegfried
            - tsnow03 # Tasha Snow
            - whyjz # Whyjay Zheng
            - yuvipanda # Yuvi Panda
            - jonathan-taylor # Jonathan Taylor
          admin_users: *users
      allowNamedServers: true
      networkPolicy:
        # FIXME: Required for dask gateway 0.9.0. It is fixed but a Helm
        #        chart of newer version is not yet released.
        enabled: false

dask-gateway:
  # dask-gateway notes:
  #
  # - Explicitly unset daskhub's nodeSelectors for all pods except the
  #   worker pods. The tolerations applied in the basehub config to all
  #   non-worker pods in dask-gateway will provide a preferred affinity
  #   towards suitable nodes without needing to have a label on them. Then
  #   we use the node label "k8s.dask.org/node-purpose: worker"
  #   specifically for enforce workers to schedule on such nodes.
  #
  traefik:
    nodeSelector: null
  controller:
    nodeSelector: null
  gateway:
    nodeSelector: null
    backend:
      scheduler:
        # IMPORTANT: We have experienced that the scheduler can fail with
        #            1GB memory limit. This was observed "stream closed"
        #            from the python client working against the
        #            Dask-Gateway created DaskCluster.
        #
        #            CommClosedError: in <TLS (closed) ConnectionPool.gather local=tls://192.168.40.210:54296 remote=gateway://traefik-prod-dask-gateway.prod:80/prod.b9600f678bb747c1a5f038b5bef3eb90>: Stream is closed
        #
        cores:
          request: 1
          limit: 64
        memory:
          request: 2G
          limit: 500G
        extraPodConfig:
          nodeSelector:
            hub.jupyter.org/node-purpose: user
            k8s.dask.org/node-purpose: null
          # serviceAccountName is adjusted by staging/prod values
          # serviceAccountName: *user-sa
      worker:
        extraPodConfig:
          nodeSelector:
            k8s.dask.org/node-purpose: worker
          # serviceAccountName is adjusted by staging/prod values
          # serviceAccountName: *user-sa

    # Note that we are overriding options provided in 2i2c's helm chart that has
    # default values for these config entries.
    #
    extraConfig:
      # This configuration represents options that can be presented to users
      # that want to create a Dask cluster using dask-gateway. For more
      # details, see https://gateway.dask.org/cluster-options.html
      #
      # The goal is to provide a simple configuration that allow the user some
      # flexibility while also fitting well well on AWS nodes that are all
      # having 1:4 ratio between CPU and GB of memory. By providing the
      # username label, we help administrators to track user pods.
      option_handler: |
        from dask_gateway_server.options import Options, Select, String, Mapping
        def cluster_options(user):
            def option_handler(options):
                if ":" not in options.image:
                    raise ValueError("When specifying an image you must also provide a tag")

                extra_labels = {}
                extra_annotations = {
                    "prometheus.io/scrape": "true",
                    "prometheus.io/port": "8787",
                }
                chosen_worker_cpu = int(options.worker_specification.split("CPU")[0])
                chosen_worker_memory = 4 * chosen_worker_cpu

                # We multiply the requests by a fraction to ensure that the
                # worker fit well within a node that need some resources
                # reserved for system pods.
                return {
                    # A default image is suggested via DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE env variable
                    "image": options.image,
                    "scheduler_extra_pod_labels": extra_labels,
                    "scheduler_extra_pod_annotations": extra_annotations,
                    "worker_extra_pod_labels": extra_labels,
                    "worker_extra_pod_annotations": extra_annotations,
                    "worker_cores": 0.85 * chosen_worker_cpu,
                    "worker_cores_limit": chosen_worker_cpu,
                    "worker_memory": "%fG" % (0.85 * chosen_worker_memory),
                    "worker_memory_limit": "%fG" % chosen_worker_memory,
                    "environment": options.environment,
                }
            return Options(
                Select(
                    "worker_specification",
                    [
                        "1CPU, 4GB",
                        "2CPU, 8GB",
                        "4CPU, 16GB",
                        "8CPU, 32GB",
                        "16CPU, 64GB",
                        "32CPU, 128GB",
                        "64CPU, 256GB",
                    ],
                    default="1CPU, 4GB",
                    label="Worker specification",
                ),
                # The default image is set via DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE env variable
                String("image", label="Image"),
                Mapping("environment", {}, label="Environment variables"),
                handler=option_handler,
            )
        c.Backend.cluster_options = cluster_options
      idle: |
        # timeout after 30 minutes of inactivity
        c.KubeClusterConfig.idle_timeout = 1800
      limits: |
        # per Dask cluster limits.
        #
        # Limits removed for JMTE as I think they could hamper Shane Griggsby's
        # work with powerful dask clusters.
        #
        # c.ClusterConfig.cluster_max_cores = 256
        # c.ClusterConfig.cluster_max_memory = "1028G"

# jupyterhub-ssh values.yaml reference:
# https://github.com/yuvipanda/jupyterhub-ssh/blob/main/helm-chart/jupyterhub-ssh/values.yaml
#
jupyterhub-ssh:
  hubUrl: http://proxy-http:8000

  ssh:
    enabled: true

  sftp:
    # enabled is adjusted by staging/prod values
    # enabled: true
    pvc:
      enabled: true
      name: home-nfs
