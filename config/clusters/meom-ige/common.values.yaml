basehub:
  nfs:
    enabled: true
    pv:
      enabled: true
      # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
      mountOptions:
        - rsize=1048576
        - wsize=1048576
        - timeo=600
        - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
        - retrans=2
        - noresvport
      serverIP: nfs-server-01.us-central1-b.c.meom-ige-cnrs.internal
      baseShareName: /export/home-01/homes/
  dask-gateway:
    enabled: true
  jupyterhub:
    custom:
      daskhubSetup:
        enabled: true
      2i2c:
        add_staff_user_ids_to_admin_users: true
        add_staff_user_ids_of_type: "github"
      homepage:
        templateVars:
          org:
            name: "SWOT Ocean Pangeo Team"
            logo_url: https://2i2c.org/media/logo.png
            url: https://meom-group.github.io/
          designed_by:
            name: 2i2c
            url: https://2i2c.org
          operated_by:
            name: 2i2c
            url: https://2i2c.org
          funded_by:
            name: SWOT Ocean Pangeo Team
            url: https://meom-group.github.io/
    singleuser:
      extraEnv:
        DATA_BUCKET: gs://meom-ige-data
        SCRATCH_BUCKET: "gs://meom-ige-scratch/$(JUPYTERHUB_USER)"
      profileList:
        # This profile list option was used for the event outlined in
        # https://github.com/2i2c-org/infrastructure/issues/3126, it was a one
        # day event taking place september 27th 2023. Its retained in a comment
        # to enable a similar event to be prepared for faster in the future.
        #
        # - display_name: Grenoble demo
        #   default: true
        #   allowed_groups:
        #     - 2i2c-org:hub-access-for-2i2c-staff
        #     - meom-group:hub-users # long term users
        #     - demo-dask-grenoble2023:demo # temporary users for event
        #   description: Start a server on a machine with 64 CPUs and 512GB of memory
        #   slug: demo
        #   profile_options:
        #     requests:
        #       display_name: Resource allocation
        #       choices:
        #         # mem_8:
        #         #   display_name: 8 GB RAM, up to 4 CPU
        #         #   kubespawner_override:
        #         #     mem_guarantee: 7.593G
        #         #     mem_limit: 8G
        #         #     cpu_guarantee: 0.984
        #         #     cpu_limit: 4
        #         # mem_16:
        #         #   default: true
        #         #   display_name: 16 GB RAM, up to 8 CPU
        #         #   kubespawner_override:
        #         #     mem_guarantee: 15.186G
        #         #     mem_limit: 16G
        #         #     cpu_guarantee: 1.969
        #         #     cpu_limit: 8
        #         mem_32:
        #           display_name: 32 GB RAM, up to 16 CPU
        #           kubespawner_override:
        #             mem_guarantee: 30.372G
        #             mem_limit: 32G
        #             cpu_guarantee: 3.938
        #             cpu_limit: 16
        #         # mem_64:
        #         #   display_name: 64 GB RAM, up to 32 CPU
        #         #   kubespawner_override:
        #         #     mem_guarantee: 60.744G
        #         #     mem_limit: 64G
        #         #     cpu_guarantee: 7.875
        #         #     cpu_limit: 32
        #   kubespawner_override:
        #     node_selector:
        #       node.kubernetes.io/instance-type: n2-highmem-64

        # The mem-guarantees are here so k8s doesn't schedule other pods
        # on these nodes. They need to be just under total allocatable
        # RAM on a node, not total node capacity
        - display_name: "Small"
          default: true
          allowed_groups: &allowed_groups_normal_use
            - 2i2c-org:hub-access-for-2i2c-staff
            - meom-group:hub-users # long term users
          description: "~2 CPU, ~8G RAM"
          kubespawner_override:
            mem_limit: 8G
            mem_guarantee: 4G
            node_selector:
              node.kubernetes.io/instance-type: n1-standard-2
        - display_name: "Medium"
          allowed_groups: *allowed_groups_normal_use
          description: "~8 CPU, ~32G RAM"
          kubespawner_override:
            mem_limit: 32G
            mem_guarantee: 22G
            node_selector:
              node.kubernetes.io/instance-type: n1-standard-8
        - display_name: "Large"
          allowed_groups: *allowed_groups_normal_use
          description: "~16 CPU, ~64G RAM"
          kubespawner_override:
            mem_limit: 64G
            mem_guarantee: 47G
            node_selector:
              node.kubernetes.io/instance-type: n1-standard-16
        - display_name: "Very Large"
          allowed_groups: *allowed_groups_normal_use
          description: "~32 CPU, ~128G RAM"
          kubespawner_override:
            mem_limit: 128G
            mem_guarantee: 100G
            node_selector:
              node.kubernetes.io/instance-type: n1-standard-32
        - display_name: "Huge"
          allowed_groups: *allowed_groups_normal_use
          description: "~64 CPU, ~256G RAM"
          kubespawner_override:
            mem_limit: 256G
            mem_guarantee: 220G
            node_selector:
              node.kubernetes.io/instance-type: n1-standard-64
      defaultUrl: /lab
      image:
        name: pangeo/pangeo-notebook
        tag: "2022.06.13"
    scheduling:
      userScheduler:
        enabled: false
    hub:
      allowNamedServers: true
      config:
        JupyterHub:
          authenticator_class: github
        GitHubOAuthenticator:
          populate_teams_in_auth_state: true
          allowed_organizations:
            - meom-group:hub-users # long term users
            - demo-dask-grenoble2023:demo # temporary users for event
          scope:
            - read:org
        Authenticator:
          enable_auth_state: true
          admin_users:
            - roxyboy
            - lesommer
            - auraoupa
