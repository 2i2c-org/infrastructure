nfs:
  enabled: true
  pv:
    mountOptions:
      - soft
      - noatime
    # Google FileStore IP
    serverIP: 10.155.184.90
    # Name of Google Filestore share
    baseShareName: /homes/
jupyterhub:
  prePuller:
    continuous:
      enabled: true
    hook:
      enabled: true
  custom:
    2i2c:
      add_staff_user_ids_to_admin_users: true
      add_staff_user_ids_of_type: "github"
    homepage:
      templateVars:
        org:
          name: "QuantifiedCarbon"
          logo_url: https://avatars.githubusercontent.com/u/124042132?s=400&u=b84f1c7dfd1f9699b2adec7c8eb9ca7b9b2b0a6e&v=4
          url: https://quantifiedcarbon.com
        designed_by:
          name: "2i2c"
          url: https://2i2c.org
        operated_by:
          name: "2i2c"
          url: https://2i2c.org
        funded_by:
          name: ""
          url: ""
  hub:
    allowNamedServers: true
    config:
      Authenticator:
        # This hub uses GitHub Teams auth and so we don't set
        # allowed_users in order to not deny access to valid members of
        # the listed teams. These people should have admin access though.
        admin_users:
          - gizmo404
          - jtkmckenna
      JupyterHub:
        authenticator_class: github
      GitHubOAuthenticator:
        allowed_organizations:
          - 2i2c-org:hub-access-for-2i2c-staff
          - QuantifiedCarbon:jupyterhub
        scope:
          - read:org
  singleuser:
    image:
      # pangeo/pangeo-notebook is maintained at: https://github.com/pangeo-data/pangeo-docker-images
      name: pangeo/pangeo-notebook
      # pullPolicy set to "Always" because we use the changing over time tag
      # "latest".
      pullPolicy: Always
      tag: "latest"
    profileList:
      # NOTE: About node sharing
      #
      #       CPU/Memory requests/limits are actively considered still. This
      #       profile list is setup to involve node sharing as considered in
      #       https://github.com/2i2c-org/infrastructure/issues/2121.
      #
      #       - Memory requests are different from the description, based on:
      #         whats found to remain allocate in k8s, subtracting 1GiB
      #         overhead for misc system pods, and transitioning from GB in
      #         description to GiB in mem_guarantee
      #         https://cloud.google.com/kubernetes-engine/docs/concepts/plan-node-sizes.
      #       - CPU requests are lower than the description, with a factor of
      #         10%.
      #
      - display_name: "Small: up to 4 CPU / 32 GB RAM"
        description: &profile_list_description "Start a container with at least a chosen share of capacity on a node of this type"
        slug: small
        default: true
        profile_options:
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_1:
                default: true
                display_name: ~1 GB, ~0.125 CPU
                kubespawner_override:
                  mem_guarantee: 0.836G
                  cpu_guarantee: 0.013
              mem_2:
                display_name: ~2 GB, ~0.25 CPU
                kubespawner_override:
                  mem_guarantee: 1.671G
                  cpu_guarantee: 0.025
              mem_4:
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.342G
                  cpu_guarantee: 0.05
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 6.684G
                  cpu_guarantee: 0.1
              mem_16:
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 13.369G
                  cpu_guarantee: 0.2
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 26.738G
                  cpu_guarantee: 0.4
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
          node_selector:
            node.kubernetes.io/instance-type: n2-highmem-4
      - display_name: "Medium: up to 16 CPU / 128 GB RAM"
        description: *profile_list_description
        slug: medium
        profile_options:
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_1:
                display_name: ~1 GB, ~0.125 CPU
                kubespawner_override:
                  mem_guarantee: 0.903G
                  cpu_guarantee: 0.013
              mem_2:
                display_name: ~2 GB, ~0.25 CPU
                kubespawner_override:
                  mem_guarantee: 1.805G
                  cpu_guarantee: 0.025
              mem_4:
                default: true
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.611G
                  cpu_guarantee: 0.05
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 7.222G
                  cpu_guarantee: 0.1
              mem_16:
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 14.444G
                  cpu_guarantee: 0.2
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 28.887G
                  cpu_guarantee: 0.4
              mem_64:
                display_name: ~64 GB, ~8.0 CPU
                kubespawner_override:
                  mem_guarantee: 57.775G
                  cpu_guarantee: 0.8
              mem_128:
                display_name: ~128 GB, ~16.0 CPU
                kubespawner_override:
                  mem_guarantee: 115.549G
                  cpu_guarantee: 1.6
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
          node_selector:
            node.kubernetes.io/instance-type: n2-highmem-16
      - display_name: "n2-highcpu-32: 32 CPU / 32 GB RAM"
        description: "Start a container on a dedicated node"
        slug: "n2_highcpu_32"
        kubespawner_override:
          node_selector:
            node.kubernetes.io/instance-type: n2-highcpu-32
          mem_guarantee: 27G
          cpu_guarantee: 3.2
          cpu_limit: null
          mem_limit: null
      - display_name: "n2-highcpu-96: 96 CPU / 96 GB RAM"
        description: "Start a container on a dedicated node"
        slug: "n2_highcpu_96"
        kubespawner_override:
          node_selector:
            node.kubernetes.io/instance-type: n2-highcpu-96
          cpu_limit: null
          mem_limit: null
          mem_guarantee: 86G
          cpu_guarantee: 9.6
      - display_name: "n2-standard-48: 48 CPU / 192 GB RAM"
        description: "Start a container on a dedicated node"
        slug: "n2_standard_48"
        kubespawner_override:
          node_selector:
            node.kubernetes.io/instance-type: n2-standard-48
          mem_guarantee: 160G
          cpu_guarantee: 4.8
          cpu_limit: null
          mem_limit: null
      - display_name: "n2-standard-96: 96 CPU / 384 GB RAM"
        description: "Start a container on a dedicated node"
        slug: "n2_standard_96"
        kubespawner_override:
          node_selector:
            node.kubernetes.io/instance-type: n2-standard-96
          mem_guarantee: 320G
          cpu_guarantee: 9.6
          cpu_limit: null
          mem_limit: null
    # shared-public for collaboration
    # See https://2i2c.freshdesk.com/a/tickets/814
    storage:
      extraVolumeMounts:
        - name: home
          mountPath: /home/jovyan/shared-public
          subPath: _shared-public
          readOnly: false
        - name: home
          mountPath: /home/jovyan/shared
          subPath: _shared
          readOnly: true
    initContainers:
      - name: volume-mount-ownership-fix
        image: busybox
        command:
          [
            "sh",
            "-c",
            "id && chown 1000:1000 /home/jovyan && chown 1000:1000 /home/jovyan/shared && chown 1000:1000 /home/jovyan/shared-public && ls -lhd /home/jovyan ",
          ]
        securityContext:
          runAsUser: 0
        volumeMounts:
          - name: home
            mountPath: /home/jovyan
            subPath: "{username}"
          # Mounted without readonly attribute here,
          # so we can chown it appropriately
          - name: home
            mountPath: /home/jovyan/shared
            subPath: _shared
          - name: home
            mountPath: /home/jovyan/shared-public
            subPath: _shared-public
