name: jmte
provider: none
# kubeconfig:
#   file: secrets/jmte.yaml
hubs:
  - name: prod
    domain: hub.jupytearth.org
    template: daskhub
    auth0:
      connection: github
    config: &config

      basehub:
        # Cloudformation: The EFS filesystem was created by cloudformation.
        #
        nfsPVC:
          enabled: true
          nfs:
            # mountOptions from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
            mountOptions:
              - rsize=1048576
              - wsize=1048576
              - timeo=600
              - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
              - retrans=2
              - noresvport
            serverIP: fs-01707b06.efs.us-west-2.amazonaws.com
            # baseShareName is required to be just "/" so that we can create
            # various sub folders in the filesystem that our PV to access the
            # NFS server can reference successfully as it isn't supported to
            # access a not yet existing folder. This creation is automated by
            # the nfs-share-creator resource part of the basehub Helm chart.
            baseShareName: /



        jupyterhub:
          custom:
            homepage:
              templateVars:
                org:
                  name: Jupyter meets the Earth
                  logo_url: https://pangeo-data.github.io/jupyter-earth/_static/jupyter-earth.png
                  url: https://jupytearth.org
                designed_by:
                  name: 2i2c
                  url: https://2i2c.org
                operated_by:
                  name: 2i2c
                  url: https://2i2c.org
                funded_by:
                  name: Jupyter meets the Earth
                  url: https://jupytearth.org

          singleuser:
            # Eksctl: The service account was created by eksctl.
            #
            serviceAccountName: &user-sa s3-full-access

            # cmd: I've experimented with these settings to get a JupyterLab RTC
            #      setup functioning. It currently is, but is this what makes
            #      sense to get it to function?
            #
            #      ref: https://github.com/jupyterlab-contrib/jupyterlab-link-share/issues/10#issuecomment-851899758
            #      ref: https://github.com/jupyterlab/jupyterlab/blob/1c8ff104a99e294265e6cf476dcb46279b0c3593/binder/jupyter_notebook_config.py#L39
            #
            #      Note the default in z2jh is jupyterhub-singleuser.
            cmd:
              - jupyterhub-singleuser
              - --LabApp.collaborative=True
              - --ServerApp.allow_remote_access=True

            extraEnv:
              # SCRATCH_BUCKET / PANGEO_SCRATCH are environment variables that
              # help users write notebooks and such referencing this environment
              # variable in a way that will work between users.
              #
              # $(ENV_VAR) will by evaluated by k8s automatically
              #
              # Cloudformation: The s3 bucket was created by cloudformation.
              #
              SCRATCH_BUCKET: s3://jmte-scratch/$(JUPYTERHUB_USER)
              PANGEO_SCRATCH: s3://jmte-scratch/$(JUPYTERHUB_USER)

            initContainers:
              # Need to explicitly fix ownership here, since EFS doesn't do anonuid
              - name: volume-mount-ownership-fix
                image: busybox
                command: ["sh", "-c", "id && chown 1000:1000 /home/jovyan /home/jovyan/shared && ls -lhd /home/jovyan"]
                securityContext:
                  runAsUser: 0
                volumeMounts:
                  - name: home
                    mountPath: /home/jovyan
                    subPath: "{username}"
                  - name: home
                    mountPath: /home/jovyan/shared
                    subPath: _shared

            image:
              name: pangeo/pangeo-notebook
              tag: "2021.05.15" # https://hub.docker.com/r/pangeo/pangeo-notebook/tags

            profileList:
              - display_name: "16th of Medium: 0.25-4 CPU, 1-16 GB"
                kubespawner_override:
                  cpu_guarantee: 0.225
                  mem_guarantee: 0.875G
                  mem_limit: null
                  node_selector: { 2i2c.org/node-cpu: "4" }
              - display_name: "4th of Medium: 1-4 CPU, 4-16 GB"
                kubespawner_override:
                  cpu_guarantee: 0.875
                  mem_guarantee: 3.5G
                  mem_limit: null
                  node_selector: { 2i2c.org/node-cpu: "4" }
              - display_name: "Medium: 4 CPU, 16 GB"
                kubespawner_override:
                  cpu_guarantee: 3.5
                  mem_guarantee: 14G
                  mem_limit: null
                  node_selector: { 2i2c.org/node-cpu: "4" }
              - display_name: "Large: 16 CPU, 64 GB"
                kubespawner_override:
                  mem_guarantee: 56G
                  mem_limit: null
                  node_selector: { 2i2c.org/node-cpu: "16" }
              - display_name: "Massive: 64 CPU, 256 GB"
                kubespawner_override:
                  mem_guarantee: 224G
                  mem_limit: null
                  node_selector: { 2i2c.org/node-cpu: "64" }

          proxy:
            # proxy notes:
            #
            # - Revert basehubs overrides as we don't install ingress-nginx and
            #   cert-manager yet, and therefore should use
            #   service.type=LoadBalancer instead of service.type=ClusterIP.
            #   Along with this, we also make use of the autohttps system that
            #   requires us to configure an letsencrypt email.
            #
            service:
              type: LoadBalancer
            https:
              enabled: true
              type: letsencrypt
              letsencrypt:
                contactEmail: erik@sundellopensource.se

          hub:
            config:
              Authenticator:
                allowed_users: &users
                  - abbyazari       # Abby Azari
                  - andersy005      # Anderson Banihirwe
                  - consideratio    # Erik Sundell
                  - choldgraf       # Chris Holdgraf
                  - elliesch        # Ellie Abrahams
                  - EMscience       # Edom Moges
                  - espg            # Shane Grigsby
                  - facusapienza21  # Facundo Sapienza
                  - fperez          # Fernando PÃ©rez
                  - kmpaul          # Kevin Paul
                  - lrennels        # Lisa Rennels
                  - mrsiegfried     # Matthew Siegfried
                  - tsnow03         # Tasha Snow
                  - whyjz           # Whyjay Zheng
                  - yuvipanda       # Yuvi Panda
                admin_users: *users
            allowNamedServers: true
            networkPolicy:
              # FIXME: Required for dask gateway 0.9.0. It is fixed but a Helm
              #        chart of newer version is not yet released.
              enabled: false



      dask-gateway:
        # dask-gateway notes:
        #
        # - Explicitly unset daskhub's nodeSelectors for all pods except the
        #   worker pods. The tolerations applied in the basehub config to all
        #   non-worker pods in dask-gateway will provide a preferred affinity
        #   towards suitable nodes without needing to have a label on them. Then
        #   we use the node label "k8s.dask.org/node-purpose: worker"
        #   specifically for enforce workers to schedule on such nodes.
        #
        traefik:
          nodeSelector: null
        controller:
          nodeSelector: null
        gateway:
          nodeSelector: null
          backend:
            scheduler:
              extraPodConfig:
                nodeSelector:
                  hub.jupyter.org/node-purpose: user
                  k8s.dask.org/node-purpose: null
                serviceAccountName: *user-sa
            worker:
              extraPodConfig:
                nodeSelector:
                  k8s.dask.org/node-purpose: worker
                serviceAccountName: *user-sa

          extraConfig:
            idle: |
              # timeout after 30 minutes of inactivity
              c.KubeClusterConfig.idle_timeout = 1800
            limits: |
              # per Dask cluster limits.
              c.ClusterConfig.cluster_max_cores = 256
              c.ClusterConfig.cluster_max_memory = "1028G"
