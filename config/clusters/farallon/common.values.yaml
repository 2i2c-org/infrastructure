basehub:
  nfs:
    pv:
      # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
      mountOptions:
        - rsize=1048576
        - wsize=1048576
        - timeo=600
        - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
        - retrans=2
        - noresvport
      serverIP: fs-cb9c65b0.efs.us-east-2.amazonaws.com
      baseShareName: /
    shareCreator:
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: "Exists"
          effect: "NoSchedule"
  jupyterhub:
    custom:
      2i2c:
        add_staff_user_ids_to_admin_users: true
        add_staff_user_ids_of_type: "github"
      homepage:
        templateVars:
          org:
            name: Farallon Institute
            logo_url: https://2i2c.org/media/logo.png
            url: http://www.faralloninstitute.org/
          designed_by:
            name: 2i2c
            url: https://2i2c.org
          operated_by:
            name: 2i2c
            url: https://2i2c.org
          funded_by:
            name: Farallon Institute
            url: http://www.faralloninstitute.org/
    singleuser:
      initContainers:
        # Need to explicitly fix ownership here, since EFS doesn't do anonuid
        - name: volume-mount-ownership-fix
          image: busybox
          command:
            [
              "sh",
              "-c",
              "id && chown 1000:1000 /home/jovyan && ls -lhd /home/jovyan",
            ]
          securityContext:
            runAsUser: 0
          volumeMounts:
            - name: home
              mountPath: /home/jovyan
              subPath: "{username}"
      image:
        name: 677861182063.dkr.ecr.us-east-2.amazonaws.com/2i2c-hub/user-image
        tag: "5115e56"
      profileList:
        # The mem-guarantees are here so k8s doesn't schedule other pods
        # on these nodes.
        - display_name: "Default: m5.xlarge"
          description: "~4CPUs & ~15GB RAM"
          kubespawner_override:
            # Expllicitly unset mem_limit, so it overrides the default memory limit we set in
            # basehub/values.yaml
            mem_limit: null
            mem_guarantee: 14G
            cpu_guarantee: 3
            node_selector:
              node.kubernetes.io/instance-type: m5.xlarge
        - display_name: "Default: m5.2xlarge"
          description: "~8CPUs & ~30GB RAM"
          kubespawner_override:
            # Expllicitly unset mem_limit, so it overrides the default memory limit we set in
            # basehub/values.yaml
            mem_limit: null
            mem_guarantee: 28G
            cpu_guarantee: 7
            node_selector:
              node.kubernetes.io/instance-type: m5.2xlarge
    scheduling:
      userPlaceholder:
        enabled: false
        replicas: 0
      userScheduler:
        enabled: false
    proxy:
      service:
        type: LoadBalancer
      https:
        enabled: true
      chp:
        nodeSelector: {}
        tolerations:
          - key: "node-role.kubernetes.io/master"
            effect: "NoSchedule"
      traefik:
        nodeSelector: {}
        tolerations:
          - key: "node-role.kubernetes.io/master"
            effect: "NoSchedule"
    hub:
      allowNamedServers: true
      networkPolicy:
        # FIXME: For dask gateway
        enabled: false
      readinessProbe:
        enabled: false
      nodeSelector: {}
      config:
        Authenticator:
          allowed_users: &users
            - caitlinkroeger
            - cgentemann
            - DaisyShi19
            - jeffdorman
            - marisolgr
            - trondkr
            - zbird21
          admin_users: *users
      tolerations:
        - key: "node-role.kubernetes.io/master"
          effect: "NoSchedule"
dask-gateway:
  traefik:
    tolerations:
      - key: "node-role.kubernetes.io/master"
        effect: "NoSchedule"
  controller:
    tolerations:
      - key: "node-role.kubernetes.io/master"
        effect: "NoSchedule"
  gateway:
    tolerations:
      - key: "node-role.kubernetes.io/master"
        effect: "NoSchedule"
      # TODO: figure out a replacement for userLimits.
    extraConfig:
      idle: |
        # timeout after 30 minutes of inactivity
        c.KubeClusterConfig.idle_timeout = 1800
