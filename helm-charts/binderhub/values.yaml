userServiceAccount:
  enabled: true
binderhub:
  config:
    BinderHub:
      use_registry: true
      per_repo_quota: 150
      build_node_selector:
        # Build user images only on user nodes, not dask or core nodes
        hub.jupyter.org/node-purpose: user
  service:
    type: ClusterIP
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 256m
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: letsencrypt-prod
  imageBuilderType: dind
  imageCleaner:
    enabled: true
    # when 80% of inodes are used,
    # cull images until only 40% are used.
    imageGCThresholdHigh: 80
    imageGCThresholdLow: 40
  jupyterhub:
    ingress:
      enabled: true
      annotations:
        nginx.ingress.kubernetes.io/proxy-body-size: 256m
        kubernetes.io/ingress.class: nginx
        cert-manager.io/cluster-issuer: letsencrypt-prod
    proxy:
      service:
        type: ClusterIP
      chp:
        nodeSelector:
          hub.jupyter.org/node-purpose: core
        resources:
          requests:
            # FIXME: We want no guarantees here!!!
            # This is lowest possible value
            cpu: 0.01
            memory: 64Mi
          limits:
            memory: 1Gi
      traefik:
        nodeSelector:
          hub.jupyter.org/node-purpose: core
        resources:
          requests:
            memory: 64Mi
          limits:
            memory: 1Gi
    singleuser:
      serviceAccountName: user-sa
      # Almost everyone using dask by default wants JupyterLab
      defaultUrl: /lab
      extraLabels:
        hub.jupyter.org/network-access-proxy-http: "true"
      cloudMetadata:
        # Don't block access to AWS cloud metadata
        # If we don't, our users can't access S3 buckets / other AWS services
        # without an explicit identity
        # FIXME: Provide an explicit identity for users instead
        blockWithIptables: false
      extraEnv:
        # About DASK_ prefixed variables we set:
        #
        # 1. k8s native variable expansion is applied with $(MY_ENV) syntax. The
        #    order variables are defined matters though and we are under the
        #    mercy of how KubeSpawner renders our passed dictionaries.
        #
        # 2. Dask loads local YAML config.
        #
        # 3. Dask loads environment variables prefixed DASK_.
        #    - DASK_ is stripped
        #    - Capitalization is ignored
        #    - Double underscore means a nested configuration
        #    - `ast.literal_eval` is used to parse values
        #
        # 4. dask-gateway and dask-distributed looks at its config and expands
        #    expressions in {} again, sometimes only with the environment
        #    variables as context but sometimes also with additional variables.
        #
        # References:
        # - K8s expansion:     https://kubernetes.io/docs/tasks/inject-data-application/define-interdependent-environment-variables/
        # - KubeSpawner issue: https://github.com/jupyterhub/kubespawner/issues/491
        # - Dask config:       https://docs.dask.org/en/latest/configuration.html
        # - Exploration issue: https://github.com/2i2c-org/infrastructure/issues/442
        #
        # DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE makes the default worker image
        # match the singleuser image.
        DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: "{JUPYTER_IMAGE_SPEC}"
        # DASK_GATEWAY__CLUSTER__OPTIONS__ENVIRONMENT makes some environment
        # variables be copied over to the worker nodes from the user nodes.
        DASK_GATEWAY__CLUSTER__OPTIONS__ENVIRONMENT:
          '{"SCRATCH_BUCKET": "$(SCRATCH_BUCKET)",
          "PANGEO_SCRATCH": "$(PANGEO_SCRATCH)"}'
        # DASK_DISTRIBUTED__DASHBOARD__LINK makes the suggested link to the
        # dashboard account for the /user/<username> prefix in the path. Note
        # that this still misbehave if you have a named server but now its at
        # least functional for non-named servers.
        DASK_DISTRIBUTED__DASHBOARD__LINK: "/user/{JUPYTERHUB_USER}/proxy/{port}/status"
      extraFiles:
        jupyter_notebook_config.json:
          mountPath: /usr/local/etc/jupyter/jupyter_notebook_config.json
          # if a user leaves a notebook with a running kernel,
          # the effective idle timeout will typically be cull idle timeout
          # of the server + the cull idle timeout of the kernel,
          # as culling the kernel will register activity,
          # resetting the no_activity timer for the server as a whole
          data:
            MappingKernelManager:
              # shutdown kernels after no activity
              cull_idle_timeout: 3600
              # check for idle kernels this often
              cull_interval: 300
              # a kernel with open connections but no activity still counts as idle
              # this is what allows us to shutdown servers
              # when people leave a notebook open and wander off
              cull_connected: true
      nodeSelector:
        hub.jupyter.org/node-purpose: user
      networkPolicy:
        # Allow unrestricted access to the internet but not local cluster network
        enabled: true
        egress:
          - to:
              - ipBlock:
                  cidr: 0.0.0.0/0
                  except:
                    # Don't allow network access to private IP ranges
                    # Listed in https://datatracker.ietf.org/doc/html/rfc1918
                    - 10.0.0.0/8
                    - 172.16.0.0/12
                    - 192.168.0.0/16
                    # Don't allow network access to the metadata IP
                    - 169.254.169.254/32
          # Allow code in hubs to talk to ingress provider, so they can talk to
          # the hub via its public URL
          - to:
              - namespaceSelector:
                  matchLabels:
                    name: support
                podSelector:
                  matchLabels:
                    app.kubernetes.io/name: ingress-nginx
          # If a hub is using autohttps instead of ingress-nginx, allow traffic
          # to the autohttps pod as well
          - to:
              - podSelector:
                  matchLabels:
                    app: jupyterhub
                    component: autohttps
          # Allow traffic to the proxy pod from user pods
          # This is particularly important for daskhubs that utilise the proxy
          # in order to create clusters (schedulers and workers)
          - to:
              - podSelector:
                  matchLabels:
                    app: jupyterhub
                    component: proxy
          # Allow traffic to the traefik pod from user pods. Needed for daskhubs.
          - to:
              - podSelector:
                  matchLabels:
                    app.kubernetes.io/component: traefik
    hub:
      config:
        JupyterHub:
          # Allow unauthenticated prometheus requests
          # Otherwise our prometheus server can't get hub metrics
          authenticate_prometheus: false
        KubeSpawner:
          # Make sure working directory is ${HOME}
          working_dir: /home/jovyan
          extra_container_config:
            securityContext:
              # Explicitly disallow setuid binaries from working inside the container
              allowPrivilegeEscalation: false
      services:
        dask-gateway:
          # Don't display a dask-gateway entry under 'services',
          # as dask-gateway has no UI
          display: false
        # hub-health service helps us run health checks from the deployer script.
        # The JupyterHub Helm chart will automatically generate an API token for
        # services and expose it in a k8s Secret named `hub`. When we run health
        # tests against a hub, we read this token from the k8s Secret to acquire
        # the credentials needed to interacting with the JupyterHub API.
        #
        hub-health:
          # FIXME: With JupyterHub 2 we can define a role for this service with
          #        more tightly scoped permissions based on our needs.
          #
          admin: true
      nodeSelector:
        hub.jupyter.org/node-purpose: core
      extraConfig:
        daskhub-01-add-dask-gateway-values: |
          # 1. Sets `DASK_GATEWAY__PROXY_ADDRESS` in the singleuser environment.
          # 2. Adds the URL for the Dask Gateway JupyterHub service.
          import os
          # These are set by jupyterhub.
          release_name = os.environ["HELM_RELEASE_NAME"]
          release_namespace = os.environ["POD_NAMESPACE"]
          if "PROXY_HTTP_SERVICE_HOST" in os.environ:
              # https is enabled, we want to use the internal http service.
              gateway_address = "http://{}:{}/services/dask-gateway/".format(
                  os.environ["PROXY_HTTP_SERVICE_HOST"],
                  os.environ["PROXY_HTTP_SERVICE_PORT"],
              )
              print("Setting DASK_GATEWAY__ADDRESS {} from HTTP service".format(gateway_address))
          else:
              gateway_address = "http://proxy-public/services/dask-gateway"
              print("Setting DASK_GATEWAY__ADDRESS {}".format(gateway_address))
          # Internal address to connect to the Dask Gateway.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__ADDRESS", gateway_address)
          # Internal address for the Dask Gateway proxy.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__PROXY_ADDRESS", "gateway://traefik-{}-dask-gateway.{}:80".format(release_name, release_namespace))
          # Relative address for the dashboard link.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__PUBLIC_ADDRESS", "/services/dask-gateway/")
          # Use JupyterHub to authenticate with Dask Gateway.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__AUTH__TYPE", "jupyterhub")
          # Adds Dask Gateway as a JupyterHub service to make the gateway available at
          # {HUB_URL}/services/dask-gateway
          service_url = "http://traefik-{}-dask-gateway.{}".format(release_name, release_namespace)
          for service in c.JupyterHub.services:
              if service["name"] == "dask-gateway":
                  if not service.get("url", None):
                      print("Adding dask-gateway service URL")
                      service.setdefault("url", service_url)
                  break
          else:
              print("dask-gateway service not found, this should not happen!")
dask-gateway:
  #=== VALUES BELOW HERE ARE COPIED FROM DASKHUB VALUES AND SHOULD BE UPDATED ===#
  #=== IF DASKHUB CHANGES ===#
  enabled: true # Enabling dask-gateway will install Dask Gateway as a dependency.
  controller:
    nodeSelector:
      k8s.dask.org/node-purpose: core
  gateway:
    nodeSelector:
      k8s.dask.org/node-purpose: core
    backend:
      scheduler:
        extraPodConfig:
          serviceAccountName: user-sa
          tolerations:
            # Let's put schedulers on notebook nodes, since they aren't ephemeral
            # dask can recover from dead workers, but not dead schedulers
            - key: "hub.jupyter.org/dedicated"
              operator: "Equal"
              value: "user"
              effect: "NoSchedule"
            - key: "hub.jupyter.org_dedicated"
              operator: "Equal"
              value: "user"
              effect: "NoSchedule"
          nodeSelector:
            k8s.dask.org/node-purpose: scheduler
        cores:
          request: 0.01
          limit: 1
        memory:
          request: 128M
          limit: 1G
      worker:
        extraContainerConfig:
          securityContext:
            runAsGroup: 1000
            runAsUser: 1000
        extraPodConfig:
          serviceAccountName: user-sa
          securityContext:
            fsGroup: 1000
          tolerations:
            - key: "k8s.dask.org/dedicated"
              operator: "Equal"
              value: "worker"
              effect: "NoSchedule"
            - key: "k8s.dask.org_dedicated"
              operator: "Equal"
              value: "worker"
              effect: "NoSchedule"
          nodeSelector:
            # Dask workers get their own pre-emptible pool
            k8s.dask.org/node-purpose: worker
    # TODO: figure out a replacement for userLimits.
    extraConfig:
      optionHandler: |
        from dask_gateway_server.options import Options, Integer, Float, String, Mapping
        import string

        # Escape a string to be dns-safe in the same way that KubeSpawner does it.
        # Reference https://github.com/jupyterhub/kubespawner/blob/616f72c4aee26c3d2127c6af6086ec50d6cda383/kubespawner/spawner.py#L1828-L1835
        # Adapted from https://github.com/minrk/escapism to avoid installing the package
        # in the dask-gateway api pod which would have been problematic.
        def escape_string_label_safe(to_escape):
            safe_chars = set(string.ascii_lowercase + string.digits)
            escape_char = "-"
            chars = []
            for c in to_escape:
                if c in safe_chars:
                    chars.append(c)
                else:
                    # escape one character
                    buf = []
                    # UTF-8 uses 1 to 4 bytes per character, depending on the Unicode symbol
                    # so we need to transform each byte to its hex value
                    for byte in c.encode("utf8"):
                        buf.append(escape_char)
                        # %X is the hex value of the byte
                        buf.append('%X' % byte)
                    escaped_hex_char = "".join(buf)
                    chars.append(escaped_hex_char)
            return u''.join(chars)

        def cluster_options(user):
            safe_username = escape_string_label_safe(user.name)
            def option_handler(options):
                if ":" not in options.image:
                    raise ValueError("When specifying an image you must also provide a tag")
                extra_annotations = {
                    "hub.jupyter.org/username": safe_username,
                    "prometheus.io/scrape": "true",
                    "prometheus.io/port": "8787",
                }
                extra_labels = {
                    "hub.jupyter.org/username": safe_username,
                }
                return {
                    "worker_cores_limit": options.worker_cores,
                    "worker_cores": options.worker_cores,
                    "worker_memory": "%fG" % options.worker_memory,
                    "image": options.image,
                    "scheduler_extra_pod_annotations": extra_annotations,
                    "worker_extra_pod_annotations": extra_annotations,
                    "scheduler_extra_pod_labels": extra_labels,
                    "worker_extra_pod_labels": extra_labels,
                    "environment": options.environment,
                }
            return Options(
                Integer("worker_cores", 2, min=1, label="Worker Cores"),
                Float("worker_memory", 4, min=1, label="Worker Memory (GiB)"),
                # The default image is set via DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE env variable
                String("image", label="Image"),
                Mapping("environment", {}, label="Environment Variables"),
                handler=option_handler,
            )
        c.Backend.cluster_options = cluster_options
      idle: |
        # timeout after 30 minutes of inactivity
        c.KubeClusterConfig.idle_timeout = 1800
    prefix: "/services/dask-gateway" # Users connect to the Gateway through the JupyterHub service.
    auth:
      type: jupyterhub # Use JupyterHub to authenticate with Dask Gateway
  traefik:
    nodeSelector:
      k8s.dask.org/node-purpose: core
    service:
      type: ClusterIP # Access Dask Gateway through JupyterHub. To access the Gateway from outside JupyterHub, this must be changed to a `LoadBalancer`.
# A placeholder as global values that can be referenced from the same location
# of any chart should be possible to provide, but aren't necessarily provided or
# used.
global: {}
