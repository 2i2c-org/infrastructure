# This file contains generated information about cpu/memory requests made by
# DaemonSets with running pods in our clusters. This information is relevant
# when planning cpu/memory requests for other pods as the daemonsets requests
# reduces the available allocatable capacity.
#
# The requests vary between cloud providers, clusters, and k8s versions for
# reasons like:
#
# - Cloud providers' managed k8s provides different DaemonSets by default
# - DaemonSets may be coupled to managed k8s features (calico-node)
# - DaemonSets' requests may be coupled to managed k8s version (netd)
# - DaemonSets may have a vertical autoscaler changing requests dynamically over
#   time if needed (calico-node-vertical-autoscaler)
# - We may deploy or change a DaemonSet's requests over time (support-cryptnono,
#   support-prometheus-node-exporter)
#
# This file isn't updated by automation, but can easily be updated by manually
# running a command once for each cluster:
#
#     deployer config get-clusters | xargs -I {} deployer generate resource-allocation daemonset-requests {}
#
gke:
  # Current overhead is 454m and 656Mi (May 15 2024 with EKS GKE 1.29.1).
  #
  # About kube-proxy:
  #
  #   GKE's node controller creates a standalone kube-proxy pod per node not
  #   observed by inspecting the daemonsets' requests, currently requesting 100m
  #   CPU (no memory requests). These needs to be accounted for as well in the
  #   end, but isn't tracked by in this file.
  #
  # About daemonsets varying between GKE clusters:
  #
  #   - gke-metrics-agent requests 3+3m CPU and 60+40Mi memory, its not clear
  #     why some clusters but not all have it.
  #   - calico-node requests 100m CPU and 0Mi memory, its around if network
  #     policy enforcement is enabled.
  #   - ip-masq-agent requests 10m CPU and 16Mi memory, its around if network
  #     policy enforcement is enabled.
  #
  2i2c:
    requesting_daemon_sets: calico-node,fluentbit-gke,gke-metadata-server,gke-metrics-agent,ip-masq-agent,netd,pdcsi-node,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: docker-api,docker-api
    cpu_requests: 354m
    memory_requests: 656Mi
    k8s_version: v1.29.1-gke.1589020
  2i2c-uk:
    requesting_daemon_sets: calico-node,fluentbit-gke,gke-metadata-server,gke-metrics-agent,ip-masq-agent,netd,pdcsi-node,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 354m
    memory_requests: 656Mi
    k8s_version: v1.29.1-gke.1589020
  awi-ciroh:
    requesting_daemon_sets: calico-node,fluentbit-gke,gke-metadata-server,ip-masq-agent,netd,pdcsi-node,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 348m
    memory_requests: 556Mi
    k8s_version: v1.29.4-gke.1043002
  catalystproject-latam:
    # missing: gke-metrics-agent
    requesting_daemon_sets: calico-node,fluentbit-gke,gke-metadata-server,ip-masq-agent,netd,pdcsi-node,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 348m
    memory_requests: 556Mi
    k8s_version: v1.29.1-gke.1589020
  cloudbank:
    requesting_daemon_sets: calico-node,fluentbit-gke,gke-metadata-server,gke-metrics-agent,ip-masq-agent,netd,pdcsi-node,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 354m
    memory_requests: 656Mi
    k8s_version: v1.29.1-gke.1589020
  hhmi:
    # missing: calico-node, ip-masq-agent, gke-metrics-agent
    requesting_daemon_sets: calico-node,fluentbit-gke,gke-metadata-server,ip-masq-agent,netd,pdcsi-node,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 348m
    memory_requests: 556Mi
    k8s_version: v1.29.6-gke.1254000
  leap:
    requesting_daemon_sets: calico-node,fluentbit-gke,gke-metadata-server,gke-metrics-agent,ip-masq-agent,netd,pdcsi-node,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: maintenance-handler
    cpu_requests: 354m
    memory_requests: 656Mi
    k8s_version: v1.29.1-gke.1589020
  pangeo-hubs:
    requesting_daemon_sets: calico-node,fluentbit-gke,gke-metadata-server,gke-metrics-agent,ip-masq-agent,netd,pdcsi-node,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 354m
    memory_requests: 656Mi
    k8s_version: v1.29.1-gke.1589020
eks:
  # Current overhead is 195m and 250Mi (May 15 2024 with EKS 1.29.4).
  #
  # About daemonsets varying between GKE clusters:
  #
  # - aws-node requests 25m or 50m depending on if it has the extra container
  #   running aws-network-policy-agent or not.
  #
  #   Based on docs, we should be able to use the latest aws-node version for
  #   all clusters, but eksctl has hardcoded a very old version. This bug is
  #   tracked in https://github.com/eksctl-io/eksctl/issues/7755. The latest
  #   aws-node version, or at least 1.16+, will provide us with network policy
  #   enforcement, which is tracked in
  #   https://github.com/2i2c-org/infrastructure/issues/1794.
  #
  2i2c-aws-us:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  catalystproject-africa:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  earthscope:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 195m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  gridsst:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  jupyter-health:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 195m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  jupyter-meets-the-earth:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  kitware:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 195m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  nasa-cryo:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  nasa-ghg:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  nasa-veda:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  nmfs-openscapes:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 195m
    memory_requests: 250Mi
    k8s_version: v1.30.4-eks-a737599
  openscapes:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  opensci:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  projectpythia:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 195m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  smithsonian:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  ubc-eoas:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
  victor:
    requesting_daemon_sets: aws-node,ebs-csi-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: ""
    cpu_requests: 170m
    memory_requests: 250Mi
    k8s_version: v1.29.7-eks-2f46c53
aks:
  pchub:
    requesting_daemon_sets: cloud-node-manager,csi-azuredisk-node,csi-azurefile-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: calico-node
    cpu_requests: 226m
    memory_requests: 300Mi
    k8s_version: v1.29.4
  utoronto:
    requesting_daemon_sets: cloud-node-manager,csi-azuredisk-node,csi-azurefile-node,kube-proxy,support-cryptnono,support-prometheus-node-exporter
    other_daemon_sets: calico-node
    cpu_requests: 226m
    memory_requests: 300Mi
    k8s_version: v1.28.3
