basehub:
  nfs:
    enabled: true

  jupyterhub:
    # Reverts changes in basehub configuration to the z2jh defaults and ensures
    # 1 pod is used as a placeholder pod, sized as the smallest node in the JMTE
    # cluster. We also update singleuser.nodeSelector to ensure we default to
    # have a placeholder for the smallest nodes only.
    #
    prePuller:
      continuous:
        enabled: true
    scheduling:
      userPlaceholder:
        enabled: true
        replicas: 1
        resources:
          requests:
            cpu: 2.5
            memory: 14G

    singleuser:
      # This default value will be relevant for the userPlaceholder
      # configuration, but irrelevant for the defaults we override in our
      # profileList configuration.
      #
      nodeSelector:
        2i2c.org/node-cpu: "4"

      # Eksctl: The service account was created by eksctl.
      #
      serviceAccountName: &user-sa s3-full-access

      extraEnv:
        # SCRATCH_BUCKET / PANGEO_SCRATCH are environment variables that
        # help users write notebooks and such referencing this environment
        # variable in a way that will work between users.
        #
        # $(ENV_VAR) will by evaluated by k8s automatically
        #
        # Cloudformation: The s3 bucket was created by cloudformation.
        #
        SCRATCH_BUCKET: s3://jmte-scratch/$(JUPYTERHUB_USER)
        PANGEO_SCRATCH: s3://jmte-scratch/$(JUPYTERHUB_USER)
        # An Amazon RDS postgresql 14 database server has been setup on a
        # machine with 4 cores and 32 GB memory. See
        # https://us-west-2.console.aws.amazon.com/rds/home?region=us-west-2#modify-instance:id=jmte-db.https://us-west-2.console.aws.amazon.com/rds/home?region=us-west-2#modify-instance:id=jmte-db
        #
        # I created a postgresql user and database for use by some like this:
        #
        # CREATE USER proj WITH ENCRYPTED PASSWORD '***';
        # CREATE DATABASE proj;
        # GRANT ALL PRIVILEGES ON DATABASE proj TO proj;
        #
        JMTE_DB_HOST: jmte-db.cqf1ngjal8bq.us-west-2.rds.amazonaws.com

      initContainers:
        # Need to explicitly fix ownership here, since EFS doesn't do anonuid
        - name: volume-mount-ownership-fix
          image: busybox
          command:
            [
              "sh",
              "-c",
              "id && chown 1000:1000 /home/jovyan /home/jovyan/shared && ls -lhd /home/jovyan",
            ]
          securityContext:
            runAsUser: 0
          volumeMounts:
            - name: home
              mountPath: /home/jovyan
              subPath: "{username}"
            - name: home
              mountPath: /home/jovyan/shared
              subPath: _shared

    proxy:
      traefik:
        # jupyterhub-ssh/sftp integration part 3/3:
        #
        # We must let traefik know it should listen for traffic (traefik entrypoint)
        # and route it (traefik router) onwards to the jupyterhub-ssh k8s Service
        # (traefik service).
        #
        extraStaticConfig:
          entryPoints:
            ssh-entrypoint:
              address: :8022
            sftp-entrypoint:
              address: :2222
        extraDynamicConfig:
          tcp:
            services:
              ssh-service:
                loadBalancer:
                  servers:
                    - address: jupyterhub-ssh:22
              sftp-service:
                loadBalancer:
                  servers:
                    - address: jupyterhub-sftp:22
            routers:
              ssh-router:
                entrypoints: [ssh-entrypoint]
                rule: HostSNI(`*`)
                service: ssh-service
              sftp-router:
                entrypoints: [sftp-entrypoint]
                rule: HostSNI(`*`)
                service: sftp-service

dask-gateway:
  gateway:
    backend:
      scheduler:
        extraPodConfig:
          serviceAccountName: *user-sa
      worker:
        extraPodConfig:
          serviceAccountName: *user-sa

jupyterhub-ssh:
  sftp:
    enabled: true
