jupyterhub:
  ingress:
    hosts: [jupyter.utoronto.ca]
    tls:
      - hosts: [jupyter.utoronto.ca]
        secretName: https-auto-tls
  scheduling:
    userPlaceholder:
      # Keep at least one spare node around
      replicas: 2
      resources:
        requests:
          # Each node on the UToronto cluster has 59350076Ki of RAM
          # You can find this out by looking at the output of `kubectl get node <node-name> -o yaml`
          # Look under `allocatable`, not `capacity`. Unfortunately then you have to fiddle with it to
          # find the right number that's big enough that no user pods will schedule here, but small enough
          # that pods in `kube-system` will still schedule.
          # So even though this is under `userPlaceholder`, it really is operating as a `nodePlaceholder`
          memory: 57350076Ki
  custom:
    homepage:
      gitRepoBranch: "utoronto-prod"
  hub:
    db:
      pvc:
        # prod stores logs, so let's make it big
        storage: 60Gi
    config:
      CILogonOAuthenticator:
        oauth_callback_url: https://jupyter.utoronto.ca/hub/oauth_callback
