# This eksctl configuration file represents the cluster and node groups for use
# by the cluster.
# ref: https://eksctl.io/usage/schema/
#
# Get cluster credentials:
#
#   eksctl utils write-kubeconfig --cluster=jmte
#
# Cluster operations:
# ref: https://eksctl.io/usage/cluster-upgrade/
#
#   create:   eksctl create cluster --config-file=eksctl-cluster-config.yaml --set-kubeconfig-context
#   upgrade:  eksctl upgrade cluster --config-file=eksctl-cluster-config.yaml
#   delete:   eksctl delete cluster --config-file=eksctl-cluster-config.yaml
#
# Node group operations:
# ref: https://eksctl.io/usage/managing-nodegroups/
#
#   eksctl get nodegroups --cluster jmte
#
#   eksctl delete nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*" --approve
#   eksctl create nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*" --install-nvidia-plugin=false
#   eksctl delete nodegroup --config-file=eksctl-cluster-config.yaml --include "user-gpu-a-*" --approve
#   eksctl create nodegroup --config-file=eksctl-cluster-config.yaml --include "user-gpu-a-*" --install-nvidia-plugin=false
#   eksctl delete nodegroup --cluster jmte --name core-a --approve
#   eksctl create nodegroup --cluster jmte --name core-a --install-nvidia-plugin=false
#
#   eksctl delete nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*" --approve && eksctl create nodegroup --config-file=eksctl-cluster-config.yaml --include "user-a-*,worker-a-*"
#
# Attribution: this was based on @yuvipanda's work in 2i2c! <3
# ref: https://github.com/2i2c-org/pangeo-hubs/blob/8e552bc198d8339efe8c003cb847849255e8f8ed/aws/eksctl-config.yaml
#

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: jmte
  # region:
  #   The region was chosen to to us-west-2 (Oregon) to be close to a CMIP-6
  #   dataset.
  #
  region: us-west-2
  # version:
  #   The k8s control plane version, to upgrade this, see
  #   https://eksctl.io/usage/cluster-upgrade/.
  #
  # For reference, this is the steps I took when upgrading from k8s 1.19 to k8s
  # 1.22, April 29th 2022.
  #
  # 1. Updated the version field in this config from 1.19 to 1.20
  #
  #    - It is not allowed to upgrade the control plane more than one minor at the time
  #
  # 2. Upgraded the control plane (takes ~10 minutes)
  #
  #    eksctl upgrade cluster --config-file eksctl-cluster-config.yaml --approve
  #
  # 3. Deleted all non-core nodegroups
  #
  #    eksctl delete nodegroup --config-file=eksctl-cluster-config.yaml --include "user-*,worker-*" --approve
  #
  # 4. Updated the version field in this config from 1.20 to 1.22
  #
  #    - It is allowed to have a nodegroup +-2 minors away from the control plan version
  #
  # 5. Created a new core nodepool (core-b)
  #
  #    eksctl create nodegroup --config-file=eksctl-cluster-config.yaml --include "core-b" --install-nvidia-plugin=false
  #
  # 6. Deleted the old core nodepool (core-a)
  #
  #    eksctl delete nodegroup --config-file=eksctl-cluster-config.yaml --include "core-b" --approve
  #
  # 7. Upgraded add-ons (takes ~3*5s)
  #
  #    eksctl utils update-kube-proxy --cluster=jmte --approve
  #    eksctl utils update-aws-node --cluster=jmte --approve
  #    eksctl utils update-coredns --cluster=jmte --approve
  #
  # 8. Update the version field in this config from 1.22 to 1.21
  #
  # 9. Upgraded the control plane, as in step 2.
  #
  # A. Upgraded add-ons, as in step 7.
  #
  # B. Update the version field in this config from 1.21 to 1.22
  #
  # C. Upgraded the control plane, as in step 2.
  #
  # D. Upgraded add-ons, as in step 7.
  #
  # E. Recreated all nodegroups
  #
  #    eksctl create nodegroup --config-file=eksctl-cluster-config.yaml --include "*" --install-nvidia-plugin=false
  #
  # For reference, this is the steps I took when upgrading from k8s 1.22 to k8s
  # 1.24, Dec 18th 2022.
  #
  # 1. Performed step 1-7 from above to, but migrated control plane from 1.22 to
  #    1.23 and node groups from 1.22 to 1.24.
  #
  # 2. When performing step 7:
  #
  #    - the aws-node daemonset's pods failed to start because of a too
  #      restrictive container securityContext not running as root.
  #    - the kube-proxy deamonset's pods failed to pull the image, it was not
  #      found.
  #
  #    I patched the aws-node thing now, but went ahead with the upgrade to k8s
  #    1.24 in the control plane, hoping another `eksctl utils update-aws-node`
  #    and `eksctl utils update-kube-proxy` would resolve the issues.
  #
  #    Later I concluded the following:
  #
  #    - aws-node issue: https://github.com/weaveworks/eksctl/issues/6048.
  #      Resolved by removing `runAsNonRoot: true` and
  #      `allowPrivilegeEscalation: false`.
  #    - kube-proxy issue: it went away when upgrading the plugin in 1.24
  #    - the cluster-autoscaler failed to start initially, but made it in the
  #      end when other pods got running.
  #
  # 3. I upgraded the control plan to 1.24 (step 2 above) and re-upgraded add-ons
  #    (step 7 above).
  #
  # 4. I recreated all node groups as in step E above.
  #
  # 5. My hub pod entered a pending state because
  #
  #    - 1 node(s) had no available volume zone
  #    - I think this is the issue:
  #      https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html, I
  #      upgraded from v1.22 to v1.23+ without manually activating the plugin
  #      mentioned there.
  #    - Looking at
  #      https://docs.aws.amazon.com/eks/latest/userguide/managing-ebs-csi.html
  #      and running the command below, I conclude it was not active in my
  #      cluster.
  #
  # 6. (what I should have done) Getting ebs-csi-driver setup:
  #
  #    What I think should have been done is to:
  #
  #    1. Ensure a service account was setup via this config:
  #       https://eksctl.io/usage/schema/#iam-serviceAccounts-wellKnownPolicies-ebsCSIController
  #    2. Ensure that the addon was setup via this config:
  #       https://eksctl.io/usage/schema/#addons-wellKnownPolicies-ebsCSIController
  #    3. Ensure that the node pools using ebs storage (core) was configured to use this:
  #       https://eksctl.io/usage/schema/#nodeGroups-iam-withAddonPolicies-ebs
  #
  # 6. (what I actually did) Getting ebs-csi-driver setup:
  #
  #   I read the following instructions: https://docs.aws.amazon.com/eks/latest/userguide/managing-ebs-csi.html#adding-ebs-csi-eks-add-on
  #
  #   I did pre-requisites to setup permissions via: https://docs.aws.amazon.com/eks/latest/userguide/csi-iam-role.html
  #
  #     UPDATE: I think this pre-requites step could be done via this config instead:
  #             https://eksctl.io/usage/schema/#iam-serviceAccounts-wellKnownPolicies-ebsCSIController
  #
  #     eksctl get addon --name aws-ebs-csi-driver --cluster=jmte
  #
  #     eksctl create iamserviceaccount \
  #         --name=ebs-csi-controller-sa \
  #         --namespace=kube-system \
  #         --cluster=jmte \
  #         --attach-policy-arn=arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
  #         --approve \
  #         --role-only \
  #         --role-name=AmazonEKS_EBS_CSI_DriverRole
  #
  #   I verified I didn't have a EBS driver installed already:
  #
  #     eksctl get addon --name=aws-ebs-csi-driver --cluster=jmte
  #
  #   I added the ebs driver addon:
  #
  #     UPDATE: I think this main step could be done via this config instead:
  #             https://eksctl.io/usage/schema/#addons-wellKnownPolicies-ebsCSIController
  #
  #     eksctl create addon --name=aws-ebs-csi-driver --cluster=jmte --service-account-role-arn=arn:aws:iam::286354552638:role/AmazonEKS_EBS_CSI_DriverRole --force
  #
  #   The hub pod that mounted a PVC with ebs storage and got "1 node(s) had no
  #   available volume zone" was suddenly scheduled successfully!
  #
  #   I think maybe we could manage to setup eksctl clusters to directly have
  #   this plugin via this config. For now, this was done with manual patches
  #   though.
  #
  # 7. I realized the ingress -> service coupling didn't work, so
  #    https://hub.jupytearth.org got stuck.
  #
  #    Resolution attempt failing: eksctl utils update-legacy-subnet-settings --cluster=jmte
  #
  #    Resolution attempt succeeded: I had also upgraded the deployer and ended
  #    up without getting proxy.https.hosts set following this:
  #    https://github.com/2i2c-org/infrastructure/pull/1404/commits/ec6f0aee616cb16d8b8e2e99252bb4110716b5d2#diff-eedaf02b81cd907a3feb5e4389e9825226bf7dc82a0fb582f9ad367c00ba6651L37,
  #    by adding proxy.https.hosts things started working again.
  #
  version: "1.24"
  tags:
    2i2c.org/project: jmte

# availabilityZones:
#   For the EKS control plane, arbitrary chosen but made explicit to ensure we
#   can locate the node pool on an AZ where the EKS control plane exist as
#   required.
#
availabilityZones: [us-west-2d, us-west-2b, us-west-2a]

# This section will create additional k8s ServiceAccount's that are coupled with
# AWS Role's. By declaring pods to use them, you can grant these pods the
# associated permissions. For this deployment, we create a k8s ServiceAccount
# with Full S3 credentials which we then also declare user pods and dask worker
# pods will make use of.
#
iam:
  withOIDC: true # https://eksctl.io/usage/security/#withoidc
  # serviceAccounts like nodeGroups etc can be managed directly with eksctl, for
  # more information, see: https://eksctl.io/usage/iamserviceaccounts/
  #
  #   eksctl create iamserviceaccount --config-file=eksctl-cluster-config.yaml
  #
  serviceAccounts:
    - metadata:
        name: s3-full-access
        namespace: prod
        labels:
          aws-usage: application
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
    - metadata:
        name: s3-full-access
        namespace: staging
        labels:
          aws-usage: application
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonS3FullAccess

# Choose the type of node group?
# - nodeGroups cannot be updated but must be recreated on changes:
#   https://eksctl.io/usage/managing-nodegroups/#nodegroup-immutability
# - managedNodeGroups cannot scale to zero:
#   https://github.com/aws/containers-roadmap/issues/724
#
# Choosing instance type?
# - Maximum pods: https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt
# - Node specs:   https://aws.amazon.com/ec2/instance-types/
# - Cost:         https://ec2pricing.net/
# - Instance availability in zone:
#   - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-discovery.html
#   - aws ec2 describe-instance-type-offerings --location-type "availability-zone" --filters Name=location,Values=us-west-2d --region us-west-2 | grep g4dn
#
# Management advice:
# - Always use a suffix for node group names that you can replace with something
#   to create a new node group and delete the old. You will run into issues if
#   you name it "core" and "core-a" instead of "core-a" and "core-b", such as
#   when deleting "core" you end up draining both node groups.
#
# Common gotcha:
# - AWS quotas may stop you from scaling up. The symptoms for this will be that
#   you observe that a scale up request has been made by the cluster-autoscaler
#   but no new node ever comes online. If that happens, you should visit
#   https://<your-region-here>.console.aws.amazon.com/ec2autoscaling/home, click
#   on the auto scaling group (ASG), then go to the activity tab and verify that
#   you have run into a quota issue. Following that, you make a request to AWS using provided link: https://aws.amazon.com/contact-us/ec2-request
#
nodeGroups:
  - name: core-a
    availabilityZones: [us-west-2d] # aws ec2 describe-availability-zones --region <region-name>
    instanceType: m5.large # 28 pods, 2 cpu, 8 GB
    minSize: 0
    maxSize: 2
    desiredCapacity: 1
    volumeSize: 250
    labels:
      hub.jupyter.org/node-purpose: core
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: core
    iam:
      withAddonPolicies:
        autoScaler: true
        # ebs: I'm not sure if this was needed because I added it before adding
        #      the ebs csi driver which was absolutely needed. Maybe this and
        #      the driver was needed.
        ebs: true
        efs: true

  # 57 pods, 4 cpu, 16 GB (Intel, 10 GBits network)
  - name: user-a-4
    availabilityZones: &user-availabilityZones [us-west-2d]
    instanceType: &user-instanceType m5.xlarge
    minSize: &user-minSize 0
    maxSize: &user-maxSize 4
    desiredCapacity: &user-desiredCapacity 0
    volumeSize: &user-volumeSize 500
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "4"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "4"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: &user-iam
      withAddonPolicies:
        autoScaler: true
        efs: true

  # 233 pods, 16 cpu, 64 GB (Intel, 10 GBits network)
  - name: user-a-16
    availabilityZones: *user-availabilityZones
    instanceType: m5.4xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "16"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "16"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: *user-iam

  # 736 pods, 64 cpu, 256 GB (Intel, 20 GBits network)
  - name: user-a-64
    availabilityZones: *user-availabilityZones
    instanceType: m5.16xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "64"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "64"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: *user-iam

  # High memory nodes.
  #
  # The local SSD storage available on these high memory nodes is not exposed by
  # default in some easy way but is rather quite tricky to make use of in k8s.
  # To make that happen, one needs to have a daemonset installed to prepare the
  # nodes that has local storage to make it exposed.
  #
  # A discussion on how this is done is made in
  # https://github.com/pangeo-data/jupyter-earth/issues/88.
  #
  # To figure out what availability zones we could use, I used the command below
  # and took the union of that output with the zones of the EKS control plane
  # configured in the root level of this config. I'm not sure if I could use
  # nodes in other availability zones.
  #
  #   aws ec2 describe-instance-type-offerings \
  #     --region us-west-2 \
  #     --filter Name=instance-type,Values=x1.16xlarge \
  #     --location-type=availability-zone
  #
  # 233 pods, 64 cpu, 976 GB, 1,920 GB local SSD storage, (Intel, 10 GBits
  # network)
  - name: user-highmem-a-64
    availabilityZones: &user-highmem-availabilityZones [us-west-2b, us-west-2a]
    instanceType: x1.16xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-highmem-cpu: "64"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-highmem-cpu: "64"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
    iam: &user-iam
      withAddonPolicies:
        autoScaler: true
        efs: true

  # GPU Nodes.
  #
  # g4dn was chosen based on input from Shane in this comment
  # https://github.com/pangeo-data/jupyter-earth/issues/77#issuecomment-910864707.
  #
  # For reference of the available choices, see
  # https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing.
  #
  # For reference on the GPU device plugin that needs to be installed, but is
  # installed automatically by eksctl, see:
  # https://eksctl.io/usage/gpu-support/#gpu-support. With that said, the
  # daemonset must still have a toleration set manually on it.
  #
  # Do a `kubectl edit ds -n kube-system nvidia-device-plugin-daemonset` and add
  # the following entries under tolerations:
  #
  # - effect: NoSchedule
  #   key: hub.jupyter.org/dedicated
  #   operator: Equal
  #   value: user
  # - effect: NoSchedule
  #   key: hub.jupyter.org_dedicated
  #   operator: Equal
  #   value: user
  #
  # WARNING: If you create any nodegroup without --install-nvidia-plugin=false,
  #          the daemonset will reset and this change will be lost.
  #
  # It seems I may need to specify additional tags also, with associated value
  # for the GPU of choice:
  # https://github.com/kubernetes/autoscaler/blob/e80ab518340f88f364fe3ef063f8303755125971/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider.go#L40-L47
  #
  # The machine nodes AMI (what is installed when it starts) for GPU nodes may
  # require you to subscribe to the AMI and accept some license. For more info,
  # see:
  # https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-eks-setup.html#deep-learning-containers-eks-setup-licensing
  #
  # Note that we opted for us-west-2b here because g4dn machines were not
  # available in us-west-2d.
  #
  #   aws ec2 describe-instance-type-offerings \
  #     --region us-west-2 \
  #     --filter Name=instance-type,Values=g4dn.xlarge \
  #     --location-type=availability-zone
  #
  # 28 pods, 4 cpu, 16 GB (Intel, 25 GBits network), 1 T4 Tensor Core GPU
  - name: user-gpu-a-4
    availabilityZones: &user-gpu-availabilityZones [us-west-2a, us-west-2b]
    instanceType: g4dn.xlarge
    minSize: *user-minSize
    # maxSize increased to accommodate request by Facu that a workshop is to
    # support 8 simultaneous users with GPU servers.
    maxSize: 10
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "4"
      2i2c.org/node-gpu: "1"
      k8s.amazonaws.com/accelerator: "nvidia-tesla-t4"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
      nvidia.com/gpu: NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.amazonaws.com/accelerator: "nvidia-tesla-t4"
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "4"
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-gpu: "1"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
      k8s.io/cluster-autoscaler/node-template/taint/nvidia.com/gpu: NoSchedule
    iam: &user-iam
      withAddonPolicies:
        autoScaler: true
        efs: true

  # 28 pods, 16 cpu, 64 GB (Intel, 25 GBits network), 1 T4 Tensor Core GPU
  - name: user-gpu-a-16
    availabilityZones: *user-gpu-availabilityZones
    instanceType: g4dn.4xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "16"
      2i2c.org/node-gpu: "1"
      k8s.amazonaws.com/accelerator: "nvidia-tesla-t4"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
      nvidia.com/gpu: NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.amazonaws.com/accelerator: "nvidia-tesla-t4"
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "16"
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-gpu: "1"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
      k8s.io/cluster-autoscaler/node-template/taint/nvidia.com/gpu: NoSchedule
    iam: &user-iam
      withAddonPolicies:
        autoScaler: true
        efs: true

  # 57 pods, 64 cpu, 256 GB (Intel, 50 GBits network), 1 T4 Tensor Core GPU
  - name: user-gpu-a-64
    availabilityZones: *user-gpu-availabilityZones
    instanceType: g4dn.16xlarge
    minSize: *user-minSize
    maxSize: *user-maxSize
    desiredCapacity: *user-desiredCapacity
    volumeSize: *user-volumeSize
    labels:
      hub.jupyter.org/node-purpose: user
      2i2c.org/node-cpu: "64"
      2i2c.org/node-gpu: "1"
      k8s.amazonaws.com/accelerator: "nvidia-tesla-t4"
    taints:
      hub.jupyter.org_dedicated: user:NoSchedule
      nvidia.com/gpu: NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.amazonaws.com/accelerator: "nvidia-tesla-t4"
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "64"
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-gpu: "1"
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org_dedicated: user:NoSchedule
      k8s.io/cluster-autoscaler/node-template/taint/nvidia.com/gpu: NoSchedule
    iam: &user-iam
      withAddonPolicies:
        autoScaler: true
        efs: true

  # Worker node pools using cheaper spot instances that are temporary.
  #
  #   References:
  #   - About spotAllocationStrategy: https://aws.amazon.com/blogs/compute/introducing-the-capacity-optimized-allocation-strategy-for-amazon-ec2-spot-instances/
  #   - About instancesDistribution:  https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-autoscaling-autoscalinggroup-instancesdistribution.html
  #
  # Note: instance types with different capacity (CPU/Memory) must have
  #       different node pools for the cluster autoscaler to work properly.
  #
  #   "Due to the Cluster Autoscaler’s limitations (more on that in the next
  #   section) on which Instance type to expand, it’s important to choose
  #   instances of the same size (vCPU and memory) for each InstanceGroup."
  #
  #   ref: https://medium.com/riskified-technology/run-kubernetes-on-aws-ec2-spot-instances-with-zero-downtime-f7327a95dea
  #
  # Note: use of YAML merge below (<<) would be great, but it is not supported
  #       and was just part of YAML 1.1 but not 1.0 or 1.2.
  #
  - name: worker-a-4
    availabilityZones:
      &worker-availabilityZones [us-west-2d, us-west-2b, us-west-2a]
    minSize: &worker-minSize 0
    maxSize: &worker-maxSize 8
    desiredCapacity: &worker-desiredCapacity 0
    volumeSize: &worker-volumeSize 500
    labels:
      k8s.dask.org/node-purpose: worker
      2i2c.org/node-cpu: "4"
    taints:
      k8s.dask.org_dedicated: worker:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.dask.org/node-purpose: worker
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "4"
      k8s.io/cluster-autoscaler/node-template/taint/k8s.dask.org_dedicated: worker:NoSchedule
    iam: &worker-iam
      withAddonPolicies:
        autoScaler: true
        efs: true
    # Spot instance specific configuration
    instancesDistribution:
      instanceTypes:
        - m5a.xlarge # 57 pods, 4 cpu, 16 GB (AMD,   10 GBits network,  100% cost)
        - m5.xlarge # 57 pods, 4 cpu, 16 GB (Intel, 10 GBits network, ~112% cost)
        # - m5n.xlarge    # 57 pods, 4 cpu, 16 GB (Intel, 25 GBits network, ~139% cost)
      onDemandBaseCapacity: &worker-onDemandBaseCapacity 0
      onDemandPercentageAboveBaseCapacity: &worker-onDemandPercentageAboveBaseCapacity 0
      spotAllocationStrategy: &worker-spotAllocationStrategy capacity-optimized

  - name: worker-a-16
    availabilityZones: *worker-availabilityZones
    minSize: *worker-minSize
    maxSize: *worker-maxSize
    desiredCapacity: *worker-desiredCapacity
    volumeSize: *worker-volumeSize
    labels:
      k8s.dask.org/node-purpose: worker
      2i2c.org/node-cpu: "16"
    taints:
      k8s.dask.org_dedicated: worker:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.dask.org/node-purpose: worker
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "16"
      k8s.io/cluster-autoscaler/node-template/taint/k8s.dask.org_dedicated: worker:NoSchedule
    iam: *worker-iam
    instancesDistribution:
      instanceTypes:
        - m5a.4xlarge # 233 pods, 16 cpu, 64 GB (AMD,   10 GBits network,  100% cost)
        - m5.4xlarge # 233 pods, 16 cpu, 64 GB (Intel, 10 GBits network, ~112% cost)
        # - m5n.4xlarge   # 233 pods, 16 cpu, 64 GB (Intel, 25 GBits network, ~139% cost)
      onDemandBaseCapacity: *worker-onDemandBaseCapacity
      onDemandPercentageAboveBaseCapacity: *worker-onDemandPercentageAboveBaseCapacity
      spotAllocationStrategy: *worker-spotAllocationStrategy

  - name: worker-a-64
    availabilityZones: *worker-availabilityZones
    minSize: *worker-minSize
    maxSize: *worker-maxSize
    desiredCapacity: *worker-desiredCapacity
    volumeSize: *worker-volumeSize
    labels:
      k8s.dask.org/node-purpose: worker
      2i2c.org/node-cpu: "64"
    taints:
      k8s.dask.org_dedicated: worker:NoSchedule
    tags:
      k8s.io/cluster-autoscaler/node-template/label/k8s.dask.org/node-purpose: worker
      k8s.io/cluster-autoscaler/node-template/label/2i2c.org/node-cpu: "64"
      k8s.io/cluster-autoscaler/node-template/taint/k8s.dask.org_dedicated: worker:NoSchedule
    iam: *worker-iam
    instancesDistribution:
      instanceTypes:
        - m5a.16xlarge # 736 pods, 64 cpu, 256 GB (AMD,   12 GBits network,  100% cost)
        - m5.16xlarge # 736 pods, 64 cpu, 256 GB (Intel, 20 GBits network, ~112% cost)
        # - m5n.16xlarge   # 736 pods, 64 cpu, 256 GB (Intel, 75 GBits network, ~139% cost)
      onDemandBaseCapacity: *worker-onDemandBaseCapacity
      onDemandPercentageAboveBaseCapacity: *worker-onDemandPercentageAboveBaseCapacity
      spotAllocationStrategy: *worker-spotAllocationStrategy
