basehub:
  nfs:
    enabled: true
    pv:
      mountOptions:
        - soft
        - noatime
      # Google FileStore IP
      serverIP: 10.155.150.2
      # Name of Google Filestore share
      baseShareName: /homes/
  jupyterhub:
    proxy:
      https:
        enabled: false
    custom:
      2i2c:
        add_staff_user_ids_to_admin_users: true
        add_staff_user_ids_of_type: "github"
      homepage:
        templateVars:
          org:
            name: LinkedEarth
            url: https://linked.earth/
            logo_url: https://linked.earth/_images/LinkedEarth_small.png
          designed_by:
            name: 2i2c
            url: https://2i2c.org
          operated_by:
            name: 2i2c
            url: https://2i2c.org
          funded_by:
            name: ""
            url: ""
    hub:
      config:
        JupyterHub:
          authenticator_class: github
        Authenticator:
          # This hub uses GitHub Orgs auth and so we don't set
          # allowed_users in order to not deny access to valid members of
          # the listed orgs. These people should have admin access though.
          admin_users:
            - khider
        GitHubOAuthenticator:
          allowed_organizations:
            - 2i2c-org
            - LinkedEarth
          scope:
            - read:org
    singleuser:
      image:
        # User image repo: https://quay.io/repository/linkedearth/pyleoclim
        name: quay.io/linkedearth/pyleoclim
        tag: "latest"

      # FIXME: extraFiles and extraEnv has been updated since the combination of
      #        jupyter_server 2 and jupyterlab doesn't work with a classical
      #        notebook server.
      #
      #        This can probably be removed with this PR that is meant to transition
      #        the default to jupyter_server from notebook:
      #        https://github.com/2i2c-org/infrastructure/pull/2160
      #
      extraFiles:
        jupyter_server_config.json:
          mountPath: /usr/local/etc/jupyter/jupyter_server_config.json
          data:
            MappingKernelManager:
              cull_idle_timeout: 3600
              cull_interval: 300
              cull_connected: true
            ServerApp:
              extra_template_paths:
                - /usr/local/share/jupyter/custom_template
      extraEnv:
        JUPYTERHUB_SINGLEUSER_APP: "jupyter_server.serverapp.ServerApp"

      profileList:
        - display_name: "Small: up to 4 CPU / 32 GB RAM"
          description: &profile_list_description "Start a container with at least a chosen share of capacity on a node of this type"
          slug: small
          profile_options:
            requests:
              display_name: Node share
              choices:
                mem_1:
                  default: true
                  display_name: ~1 GB, ~0.125 CPU
                  kubespawner_override:
                    mem_guarantee: 0.875
                    cpu_guarantee: 0.013
                mem_2:
                  display_name: ~2 GB, ~0.25 CPU
                  kubespawner_override:
                    mem_guarantee: 1.75
                    cpu_guarantee: 0.025
                mem_4:
                  display_name: ~4 GB, ~0.5 CPU
                  kubespawner_override:
                    mem_guarantee: 3.5
                    cpu_guarantee: 0.05
                mem_8:
                  display_name: ~8 GB, ~1.0 CPU
                  kubespawner_override:
                    mem_guarantee: 7.0
                    cpu_guarantee: 0.1
                mem_16:
                  display_name: ~16 GB, ~2.0 CPU
                  kubespawner_override:
                    mem_guarantee: 14.0
                    cpu_guarantee: 0.2
                mem_32:
                  display_name: ~32 GB, ~4.0 CPU
                  kubespawner_override:
                    mem_guarantee: 28.0
                    cpu_guarantee: 0.4
          kubespawner_override:
            cpu_limit: null
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: e2-highmem-4
        - display_name: "Medium: up to 16 CPU / 128 GB RAM"
          description: *profile_list_description
          slug: medium
          default: true
          profile_options:
            requests:
              display_name: Node share
              choices:
                mem_1:
                  display_name: ~1 GB, ~0.125 CPU
                  kubespawner_override:
                    mem_guarantee: 0.969
                    cpu_guarantee: 0.013
                mem_2:
                  display_name: ~2 GB, ~0.25 CPU
                  kubespawner_override:
                    mem_guarantee: 1.938
                    cpu_guarantee: 0.025
                mem_4:
                  default: true
                  display_name: ~4 GB, ~0.5 CPU
                  kubespawner_override:
                    mem_guarantee: 3.875
                    cpu_guarantee: 0.05
                mem_8:
                  display_name: ~8 GB, ~1.0 CPU
                  kubespawner_override:
                    mem_guarantee: 7.75
                    cpu_guarantee: 0.1
                mem_16:
                  display_name: ~16 GB, ~2.0 CPU
                  kubespawner_override:
                    mem_guarantee: 15.5
                    cpu_guarantee: 0.2
                mem_32:
                  display_name: ~32 GB, ~4.0 CPU
                  kubespawner_override:
                    mem_guarantee: 31.0
                    cpu_guarantee: 0.4
                mem_64:
                  display_name: ~64 GB, ~8.0 CPU
                  kubespawner_override:
                    mem_guarantee: 62.0
                    cpu_guarantee: 0.8
                mem_128:
                  display_name: ~128 GB, ~16.0 CPU
                  kubespawner_override:
                    mem_guarantee: 124.0
                    cpu_guarantee: 1.6
          kubespawner_override:
            cpu_limit: null
            mem_limit: null
            node_selector:
              node.kubernetes.io/instance-type: e2-highmem-16
dask-gateway:
  gateway:
    backend:
      scheduler:
        cores:
          request: 0.8
          limit: 1
        memory:
          request: 1G
          limit: 2G
