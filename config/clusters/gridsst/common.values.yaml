basehub:
  nfs:
    pv:
      # from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html
      mountOptions:
        - rsize=1048576
        - wsize=1048576
        - timeo=600
        - soft # We pick soft over hard, so NFS lockups don't lead to hung processes
        - retrans=2
        - noresvport
      serverIP: fs-05f68d7e096d7cf16.efs.us-west-2.amazonaws.com
      baseShareName: /
  jupyterhub:
    prePuller:
      continuous:
        enabled: true
    custom:
      2i2c:
        add_staff_user_ids_to_admin_users: true
        add_staff_user_ids_of_type: "github"
      homepage:
        templateVars:
          org:
            name: gridSST Hack-A-Thon
            logo_url: https://gridsst-hackathon.github.io/_static/logo.png
            url: https://gridsst-hackathon.github.io/ # todo: find the correct link here
          designed_by:
            name: 2i2c
            url: https://2i2c.org
          operated_by:
            name: 2i2c
            url: https://2i2c.org
          funded_by:
            name: "NASA Physical Oceanography Program"
            url: https://science.nasa.gov/earth-science/focus-areas/climate-variability-and-change/ocean-physics
    hub:
      config:
        Authenticator:
          allowed_users: &gridsst_users
            - alisonrgray
            - nikki-t
            - dgumustel
          admin_users: *gridsst_users
        JupyterHub:
          authenticator_class: github
    singleuser:
      extraEnv:
        # Temporarily set for *all* pods, including pods without any GPUs,
        # to work around https://github.com/2i2c-org/infrastructure/issues/1530
        NVIDIA_DRIVER_CAPABILITIES: compute,utility
      profileList:
        # The mem-guarantees are here so k8s doesn't schedule other pods
        # on these nodes.
        - display_name: "Small: m5.large"
          description: "~2 CPU, ~8G RAM"
          kubespawner_override:
            # Expllicitly unset mem_limit, so it overrides the default memory limit we set in
            # basehub/values.yaml
            mem_limit: null
            mem_guarantee: 6.5G
            node_selector:
              node.kubernetes.io/instance-type: m5.large
        - display_name: "Medium: m5.xlarge"
          description: "~4 CPU, ~15G RAM"
          default: true
          kubespawner_override:
            mem_limit: null
            mem_guarantee: 12G
            node_selector:
              node.kubernetes.io/instance-type: m5.xlarge
        - display_name: "Large: m5.2xlarge"
          description: "~8 CPU, ~30G RAM"
          kubespawner_override:
            mem_limit: null
            mem_guarantee: 26G
            node_selector:
              node.kubernetes.io/instance-type: m5.2xlarge
        - display_name: "Huge: m5.8xlarge"
          description: "~32 CPU, ~128G RAM"
          kubespawner_override:
            mem_limit: null
            mem_guarantee: 115G
            node_selector:
              node.kubernetes.io/instance-type: m5.8xlarge
        - display_name: "GPU"
          # P2.xlarge has 64G of RAM per GPU while g4dn has 16?!
          description: |
            ~4CPUs, Nvidia T4 GPU, 14G of RAM.
          profile_options:
            image:
              display_name: Image
              choices:
                tensorflow:
                  display_name: Pangeo Tensorflow ML Notebook
                  slug: "tensorflow"
                  kubespawner_override:
                    image: "pangeo/ml-notebook:2022.10.18"
                pytorch:
                  display_name: Pangeo PyTorch ML Notebook
                  default: true
                  slug: "pytorch"
                  kubespawner_override:
                    image: "pangeo/pytorch-notebook:2022.10.18"
          kubespawner_override:
            mem_limit: null
            extra_resource_limits:
              nvidia.com/gpu: "1"
            mem_guarantee: 14G
            node_selector:
              node.kubernetes.io/instance-type: g4dn.xlarge
      nodeSelector:
        node.kubernetes.io/instance-type: m5.8xlarge
      defaultUrl: /lab
      # User image: https://quay.io/repository/uwhackweek/snowex?tab=tags
      image:
        name: quay.io/uwhackweek/snowex
        tag: "2022.07.07"
    scheduling:
      userScheduler:
        enabled: true
