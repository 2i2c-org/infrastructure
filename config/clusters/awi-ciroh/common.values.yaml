basehub:
  nfs:
    enabled: true
    pv:
      enabled: true
      mountOptions:
        - soft
        - noatime
      # Google FileStore IP
      serverIP: 10.11.233.234
      # Name of Google Filestore share
      baseShareName: /homes/
  jupyterhub:
    custom:
      2i2c:
        add_staff_user_ids_to_admin_users: true
        add_staff_user_ids_of_type: "github"
      homepage:
        templateVars:
          org:
            name: Cooperative Institute for Research to Operations in Hydrology
            url: http://ovpred.ua.edu/alabama-water-institute/
            logo_url: https://user-images.githubusercontent.com/1879041/174884338-ec97bec1-1cc9-4ad8-8f63-06de666b5012.png
          designed_by:
            name: 2i2c
            url: https://2i2c.org
          operated_by:
            name: 2i2c
            url: https://2i2c.org
          funded_by:
            name: National Oceanic and Atmospheric Administration
            url: https://www.noaa.gov/
    hub:
      config:
        JupyterHub:
          authenticator_class: github
        GitHubOAuthenticator:
          allowed_organizations:
            - alabamawaterinstitute
            - NOAA-OWP
          scope:
            - read:org
        Authenticator:
          admin_users:
            - jameshalgren
            - arpita0911patel
            - karnesh
      profileList:
        # The mem-guarantees are here so k8s doesn't schedule other pods
        # on these nodes. They need to be just under total allocatable
        # RAM on a node, not total node capacity. Values calculated using
        # https://learnk8s.io/kubernetes-instance-calculator
        #
        # FIXME: These are changed to a temporary node sharing setup based on
        #        the legacy choices to help us pre-warm capacity independent on
        #        the choices users end up making and avoiding running into
        #        persistent disk quotas.
        #
        #        Change PR: https://github.com/2i2c-org/infrastructure/pull/2539
        #        Related event: https://github.com/2i2c-org/infrastructure/issues/2520
        #
        #        This is an interim setup, trying to balance the experience of
        #        the previous 1:1 user:node setup with a node sharing setup. It
        #        is not meant to be retained long term!
        #
        #        -[ ] Make this cluster have a node sharing setup like in the
        #             basehub/daskhub template.
        #
        - display_name: "Small"
          description: 5GB RAM, 2 CPUs
          default: true
          profile_options: &profile_options
            image:
              display_name: Image
              choices:
                old:
                  display_name: Original Pangeo Notebook base image 2023.01.13
                  slug: "tensorflow"
                  kubespawner_override:
                    image: "quay.io/2i2c/awi-ciroh-image:f7222fce8b16 "
                new:
                  display_name: New Pangeo Notebook base image 2023.09.11
                  default: true
                  slug: "pytorch"
                  kubespawner_override:
                    image: "quay.io/2i2c/awi-ciroh-image:88ea9a74a66e"
          kubespawner_override:
            mem_limit: 7G
            mem_guarantee: 5G
            cpu_limit: 2
            cpu_guarantee: 0.938
            node_selector:
              node.kubernetes.io/instance-type: n2-highmem-16
        - display_name: Medium
          description: 11GB RAM, 4 CPUs
          profile_options: *profile_options
          kubespawner_override:
            mem_limit: 15G
            mem_guarantee: 11G
            cpu_limit: 4
            cpu_guarantee: 1.875
            node_selector:
              node.kubernetes.io/instance-type: n2-highmem-16
        - display_name: Large
          description: 24GB RAM, 8 CPUs
          profile_options: *profile_options
          kubespawner_override:
            mem_limit: 30G
            mem_guarantee: 24G
            cpu_limit: 8
            cpu_guarantee: 3.75
            node_selector:
              node.kubernetes.io/instance-type: n2-highmem-16
        - display_name: Huge
          description: 52GB RAM, 16 CPUs
          profile_options: *profile_options
          kubespawner_override:
            mem_limit: 60G
            mem_guarantee: 52G
            cpu_limit: 16
            cpu_guarantee: 7.5
            node_selector:
              node.kubernetes.io/instance-type: n2-highmem-16
dask-gateway:
  gateway:
    backend:
      scheduler:
        cores:
          request: 0.8
          limit: 1
        memory:
          request: 1G
          limit: 2G
